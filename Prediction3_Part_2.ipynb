{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Prediction3 Part 2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOiok72+achZmuREek/17ql",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YazCodes/Deep-Learning-projects/blob/main/Prediction3_Part_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CUKWUcft3Hg"
      },
      "source": [
        "DQN for cartpole"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQNVaN0XtuNB",
        "outputId": "df4fbf4c-3070-49f3-a45c-1becdafe397a"
      },
      "source": [
        "# install keras rl2 (we need to install keras-rl2 so it works with the tensorflow 2 version that comes pre-installed with colab)\n",
        "!pip install keras-rl2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-rl2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/34/94ffeab44eef43e22a01d82aa0ca062a97392c2c2415ba8b210e72053285/keras_rl2-1.0.4-py3-none-any.whl (53kB)\n",
            "\r\u001b[K     |██████▏                         | 10kB 12.8MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 20kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 30kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 40kB 6.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 51kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 3.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from keras-rl2) (2.4.1)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.32.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.1.2)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (0.3.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (2.4.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (3.12.4)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (2.10.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (0.10.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.12)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (0.36.2)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.12.1)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (2.4.1)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (3.7.4.3)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.9.2->tensorflow>=2.1.0->keras-rl2) (54.1.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (3.3.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (0.4.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (1.8.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (1.27.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (3.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (1.3.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (2020.12.5)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (4.2.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (3.4.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (0.4.8)\n",
            "Installing collected packages: keras-rl2\n",
            "Successfully installed keras-rl2-1.0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zr6MBOzWuFJY",
        "outputId": "196e1d33-faf3-415d-9abe-54ca63056308"
      },
      "source": [
        "!pip install gym"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.19.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCP8mBGnuJ1E"
      },
      "source": [
        "# load the gym module\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "# import the usual Keras modules for creating deep neural networks\n",
        "from keras import Sequential\n",
        "from keras.layers import Input, Flatten, Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "ENV_NAME = 'CartPole-v0'\n",
        "env = gym.make(ENV_NAME)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhQ1EbZLwhWD",
        "outputId": "36b2cf90-80ea-4cda-8073-d688fcc5cac6"
      },
      "source": [
        "print(env.observation_space.shape) #gives us a tupel. 0 is the first element of the tupel"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUIQR1wtxnwL",
        "outputId": "2a7a65d4-44cf-4a08-faa8-beeaefa45504"
      },
      "source": [
        "#number of actions \n",
        "print(env.action_space.n)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dEJwofrluTC2",
        "outputId": "32e8831f-e6c9-44d2-fa25-13031a638eeb"
      },
      "source": [
        "import rl\n",
        "from rl.memory import SequentialMemory  # import the exerience replay buffer module\n",
        "from rl.policy import BoltzmannQPolicy, LinearAnnealedPolicy, EpsGreedyQPolicy  # import the policy\n",
        "from rl.agents.dqn import DQNAgent      # import the DQN agent\n",
        "\n",
        "memory = SequentialMemory(limit=10000, window_length=1) #setting up the experince replay buffer\n",
        "#limit = the numer of steps of episodes stored in the replay buffer\n",
        "\n",
        "# define the policy (how we select the actions)\n",
        "# setup the Linear annealed policy with the EpsGreedyQPolicy as the inner policy\n",
        "policy =  LinearAnnealedPolicy(inner_policy=EpsGreedyQPolicy(),   # policy used to select actions\n",
        "                               attr='eps',                        # attribute in the inner policy to vary             \n",
        "                               value_max=1.,                       # maximum value of attribute that is varying\n",
        "                               value_min=.1,                      # minimum value of attribute that is varying\n",
        "                               value_test=.09,  #0.9??                  # test if the value selected is < 0.05\n",
        "                               nb_steps=8000)                    # the number of steps between value_max and value_min\n",
        "\n",
        "#need to change the value_max and value_min. \n",
        "# Q-Network\n",
        "model = Sequential() #sequnetial model \n",
        "model.add(Input(shape=(1,env.observation_space.shape[0]))) # 1 = one observation and env.observation_space.shape is the number of states within our observation. 0 = the first element of the tupel\n",
        "model.add(Flatten())\n",
        "# extra layers here\n",
        "model.add(Dense(16, activation='relu')) #layer 1\n",
        "model.add(Dense(32, activation='relu')) #layer 2\n",
        "#model.add(Dense(64, activation='relu')) #layer 2\n",
        "\n",
        "model.add(Dense(env.action_space.n, activation='linear'))   # the output is the number of actions in the action space. Activation has to be linear due to how the q value does its calculation\n",
        "print(model.summary())\n",
        "\n",
        "# define the agent using the DQNAgent class\n",
        "dqn = DQNAgent(model=model,                     # Q-Network model created above ^\n",
        "               nb_actions=env.action_space.n,   # number of actions used above - the data from the enviroment\n",
        "               memory=memory,                   # experience replay memory\n",
        "               nb_steps_warmup=10,              # how many steps are waited before starting experience replay\n",
        "               target_model_update=1e-2,        # how often the target network is updated\n",
        "               policy=policy)                   # the action selection policy\n",
        "\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "\n",
        "history = dqn.fit(env, nb_steps=8000, visualize=False, verbose=2) #visualize false to save time\n",
        "\n",
        "\n",
        "# summarize the history for number  of episode steps\n",
        "plt.plot(history.history['nb_episode_steps'])\n",
        "plt.ylabel('nb_episode_steps')\n",
        "plt.xlabel('episodes')\n",
        "plt.show()\n",
        "dqn.test(env, nb_episodes=20, visualize=False) #testing for 20 episodes, reward should all be 200- evaluating my algortithm \n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_9 (Flatten)          (None, 4)                 0         \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 16)                80        \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, 32)                544       \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 2)                 66        \n",
            "=================================================================\n",
            "Total params: 690\n",
            "Trainable params: 690\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Training for 8000 steps ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n",
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:40: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "   11/8000: episode: 1, duration: 3.102s, episode steps:  11, steps per second:   4, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
            "   31/8000: episode: 2, duration: 0.173s, episode steps:  20, steps per second: 115, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.434517, mae: 0.571529, mean_q: 0.483668, mean_eps: 0.997694\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:40: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "   42/8000: episode: 3, duration: 0.116s, episode steps:  11, steps per second:  95, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.337874, mae: 0.580384, mean_q: 0.527409, mean_eps: 0.995950\n",
            "   73/8000: episode: 4, duration: 0.303s, episode steps:  31, steps per second: 102, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 0.222962, mae: 0.597442, mean_q: 0.639426, mean_eps: 0.993588\n",
            "   88/8000: episode: 5, duration: 0.164s, episode steps:  15, steps per second:  91, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.136964, mae: 0.629038, mean_q: 0.856374, mean_eps: 0.991000\n",
            "  104/8000: episode: 6, duration: 0.166s, episode steps:  16, steps per second:  96, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 0.075219, mae: 0.667891, mean_q: 1.052083, mean_eps: 0.989256\n",
            "  143/8000: episode: 7, duration: 0.373s, episode steps:  39, steps per second: 105, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.436 [0.000, 1.000],  loss: 0.056542, mae: 0.775465, mean_q: 1.352922, mean_eps: 0.986163\n",
            "  155/8000: episode: 8, duration: 0.121s, episode steps:  12, steps per second:  99, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 0.051379, mae: 0.887117, mean_q: 1.627143, mean_eps: 0.983294\n",
            "  176/8000: episode: 9, duration: 0.207s, episode steps:  21, steps per second: 102, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.067997, mae: 0.929842, mean_q: 1.707639, mean_eps: 0.981438\n",
            "  225/8000: episode: 10, duration: 0.471s, episode steps:  49, steps per second: 104, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.347 [0.000, 1.000],  loss: 0.058837, mae: 1.035407, mean_q: 1.921327, mean_eps: 0.977500\n",
            "  271/8000: episode: 11, duration: 0.428s, episode steps:  46, steps per second: 108, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.413 [0.000, 1.000],  loss: 0.079547, mae: 1.245568, mean_q: 2.331990, mean_eps: 0.972156\n",
            "  283/8000: episode: 12, duration: 0.125s, episode steps:  12, steps per second:  96, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.124226, mae: 1.378831, mean_q: 2.641056, mean_eps: 0.968894\n",
            "  298/8000: episode: 13, duration: 0.147s, episode steps:  15, steps per second: 102, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.106427, mae: 1.426056, mean_q: 2.701057, mean_eps: 0.967375\n",
            "  324/8000: episode: 14, duration: 0.258s, episode steps:  26, steps per second: 101, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.423 [0.000, 1.000],  loss: 0.121811, mae: 1.507862, mean_q: 2.878811, mean_eps: 0.965069\n",
            "  338/8000: episode: 15, duration: 0.142s, episode steps:  14, steps per second:  99, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 0.127218, mae: 1.609092, mean_q: 3.078541, mean_eps: 0.962819\n",
            "  349/8000: episode: 16, duration: 0.118s, episode steps:  11, steps per second:  94, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.136971, mae: 1.654668, mean_q: 3.160456, mean_eps: 0.961413\n",
            "  386/8000: episode: 17, duration: 0.365s, episode steps:  37, steps per second: 101, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 0.153135, mae: 1.752903, mean_q: 3.364265, mean_eps: 0.958713\n",
            "  399/8000: episode: 18, duration: 0.136s, episode steps:  13, steps per second:  95, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 0.171180, mae: 1.848565, mean_q: 3.583211, mean_eps: 0.955900\n",
            "  421/8000: episode: 19, duration: 0.222s, episode steps:  22, steps per second:  99, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.409 [0.000, 1.000],  loss: 0.156665, mae: 1.913302, mean_q: 3.667189, mean_eps: 0.953931\n",
            "  447/8000: episode: 20, duration: 0.255s, episode steps:  26, steps per second: 102, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 0.132047, mae: 1.995521, mean_q: 3.877535, mean_eps: 0.951231\n",
            "  463/8000: episode: 21, duration: 0.160s, episode steps:  16, steps per second: 100, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.265410, mae: 2.112881, mean_q: 3.956053, mean_eps: 0.948869\n",
            "  530/8000: episode: 22, duration: 0.654s, episode steps:  67, steps per second: 102, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  loss: 0.205064, mae: 2.272186, mean_q: 4.376528, mean_eps: 0.944200\n",
            "  544/8000: episode: 23, duration: 0.145s, episode steps:  14, steps per second:  97, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 0.299340, mae: 2.490337, mean_q: 4.739111, mean_eps: 0.939644\n",
            "  566/8000: episode: 24, duration: 0.220s, episode steps:  22, steps per second: 100, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.214144, mae: 2.522913, mean_q: 4.898521, mean_eps: 0.937619\n",
            "  582/8000: episode: 25, duration: 0.166s, episode steps:  16, steps per second:  97, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 0.174837, mae: 2.594592, mean_q: 5.000606, mean_eps: 0.935481\n",
            "  605/8000: episode: 26, duration: 0.232s, episode steps:  23, steps per second:  99, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 0.201608, mae: 2.695535, mean_q: 5.275339, mean_eps: 0.933288\n",
            "  620/8000: episode: 27, duration: 0.151s, episode steps:  15, steps per second:  99, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 0.264629, mae: 2.751377, mean_q: 5.389604, mean_eps: 0.931150\n",
            "  637/8000: episode: 28, duration: 0.176s, episode steps:  17, steps per second:  97, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 0.304116, mae: 2.852246, mean_q: 5.526157, mean_eps: 0.929350\n",
            "  665/8000: episode: 29, duration: 0.291s, episode steps:  28, steps per second:  96, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 0.366279, mae: 2.952641, mean_q: 5.614620, mean_eps: 0.926819\n",
            "  688/8000: episode: 30, duration: 0.222s, episode steps:  23, steps per second: 103, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.652 [0.000, 1.000],  loss: 0.483496, mae: 3.041933, mean_q: 5.810038, mean_eps: 0.923950\n",
            "  701/8000: episode: 31, duration: 0.119s, episode steps:  13, steps per second: 109, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.386623, mae: 3.131792, mean_q: 6.079571, mean_eps: 0.921925\n",
            "  717/8000: episode: 32, duration: 0.150s, episode steps:  16, steps per second: 106, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 0.448276, mae: 3.156876, mean_q: 5.955587, mean_eps: 0.920294\n",
            "  730/8000: episode: 33, duration: 0.117s, episode steps:  13, steps per second: 111, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.231 [0.000, 1.000],  loss: 0.452129, mae: 3.269853, mean_q: 6.268003, mean_eps: 0.918663\n",
            "  740/8000: episode: 34, duration: 0.100s, episode steps:  10, steps per second: 100, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.333604, mae: 3.273775, mean_q: 6.386884, mean_eps: 0.917369\n",
            "  762/8000: episode: 35, duration: 0.209s, episode steps:  22, steps per second: 105, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.557077, mae: 3.354314, mean_q: 6.409392, mean_eps: 0.915569\n",
            "  801/8000: episode: 36, duration: 0.408s, episode steps:  39, steps per second:  96, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.564 [0.000, 1.000],  loss: 0.443933, mae: 3.489158, mean_q: 6.716453, mean_eps: 0.912137\n",
            "  828/8000: episode: 37, duration: 0.276s, episode steps:  27, steps per second:  98, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 0.408686, mae: 3.602457, mean_q: 6.996679, mean_eps: 0.908425\n",
            "  853/8000: episode: 38, duration: 0.264s, episode steps:  25, steps per second:  95, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 0.582963, mae: 3.712148, mean_q: 7.097726, mean_eps: 0.905500\n",
            "  876/8000: episode: 39, duration: 0.282s, episode steps:  23, steps per second:  82, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 0.422092, mae: 3.818677, mean_q: 7.430469, mean_eps: 0.902800\n",
            "  905/8000: episode: 40, duration: 0.302s, episode steps:  29, steps per second:  96, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 0.548613, mae: 3.935629, mean_q: 7.606305, mean_eps: 0.899875\n",
            "  930/8000: episode: 41, duration: 0.264s, episode steps:  25, steps per second:  95, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  loss: 0.474811, mae: 4.058877, mean_q: 7.864360, mean_eps: 0.896838\n",
            "  973/8000: episode: 42, duration: 0.429s, episode steps:  43, steps per second: 100, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 0.556423, mae: 4.173997, mean_q: 8.043162, mean_eps: 0.893012\n",
            " 1029/8000: episode: 43, duration: 0.564s, episode steps:  56, steps per second:  99, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 0.665024, mae: 4.377263, mean_q: 8.415149, mean_eps: 0.887444\n",
            " 1046/8000: episode: 44, duration: 0.198s, episode steps:  17, steps per second:  86, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.706 [0.000, 1.000],  loss: 0.599956, mae: 4.510221, mean_q: 8.716076, mean_eps: 0.883337\n",
            " 1099/8000: episode: 45, duration: 0.480s, episode steps:  53, steps per second: 111, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 0.577540, mae: 4.665388, mean_q: 9.087457, mean_eps: 0.879400\n",
            " 1146/8000: episode: 46, duration: 0.403s, episode steps:  47, steps per second: 116, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 0.558276, mae: 4.882493, mean_q: 9.587169, mean_eps: 0.873775\n",
            " 1175/8000: episode: 47, duration: 0.269s, episode steps:  29, steps per second: 108, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 0.702082, mae: 5.044002, mean_q: 9.834958, mean_eps: 0.869500\n",
            " 1232/8000: episode: 48, duration: 0.544s, episode steps:  57, steps per second: 105, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.439 [0.000, 1.000],  loss: 0.745090, mae: 5.202752, mean_q: 10.142703, mean_eps: 0.864663\n",
            " 1287/8000: episode: 49, duration: 0.557s, episode steps:  55, steps per second:  99, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 0.720379, mae: 5.442566, mean_q: 10.644803, mean_eps: 0.858363\n",
            " 1298/8000: episode: 50, duration: 0.117s, episode steps:  11, steps per second:  94, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.821864, mae: 5.619297, mean_q: 11.030509, mean_eps: 0.854650\n",
            " 1314/8000: episode: 51, duration: 0.156s, episode steps:  16, steps per second: 103, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.075417, mae: 5.567163, mean_q: 10.783633, mean_eps: 0.853131\n",
            " 1344/8000: episode: 52, duration: 0.315s, episode steps:  30, steps per second:  95, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 1.113562, mae: 5.626418, mean_q: 10.867220, mean_eps: 0.850544\n",
            " 1366/8000: episode: 53, duration: 0.232s, episode steps:  22, steps per second:  95, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.660388, mae: 5.853074, mean_q: 11.493679, mean_eps: 0.847619\n",
            " 1381/8000: episode: 54, duration: 0.153s, episode steps:  15, steps per second:  98, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 0.713856, mae: 5.834209, mean_q: 11.415968, mean_eps: 0.845537\n",
            " 1442/8000: episode: 55, duration: 0.610s, episode steps:  61, steps per second: 100, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 0.976599, mae: 6.026060, mean_q: 11.777478, mean_eps: 0.841263\n",
            " 1466/8000: episode: 56, duration: 0.256s, episode steps:  24, steps per second:  94, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 0.837937, mae: 6.226474, mean_q: 12.201661, mean_eps: 0.836481\n",
            " 1487/8000: episode: 57, duration: 0.221s, episode steps:  21, steps per second:  95, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.381 [0.000, 1.000],  loss: 0.738259, mae: 6.314791, mean_q: 12.422003, mean_eps: 0.833950\n",
            " 1511/8000: episode: 58, duration: 0.245s, episode steps:  24, steps per second:  98, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 0.825211, mae: 6.391437, mean_q: 12.550675, mean_eps: 0.831419\n",
            " 1543/8000: episode: 59, duration: 0.354s, episode steps:  32, steps per second:  91, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.063377, mae: 6.420433, mean_q: 12.605326, mean_eps: 0.828269\n",
            " 1572/8000: episode: 60, duration: 0.288s, episode steps:  29, steps per second: 101, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  loss: 1.143761, mae: 6.500560, mean_q: 12.739647, mean_eps: 0.824837\n",
            " 1600/8000: episode: 61, duration: 0.268s, episode steps:  28, steps per second: 104, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 1.113648, mae: 6.691565, mean_q: 13.129051, mean_eps: 0.821631\n",
            " 1619/8000: episode: 62, duration: 0.167s, episode steps:  19, steps per second: 114, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 1.064931, mae: 6.797844, mean_q: 13.414023, mean_eps: 0.818988\n",
            " 1642/8000: episode: 63, duration: 0.212s, episode steps:  23, steps per second: 108, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  loss: 1.124576, mae: 6.804957, mean_q: 13.345929, mean_eps: 0.816625\n",
            " 1686/8000: episode: 64, duration: 0.426s, episode steps:  44, steps per second: 103, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 0.879785, mae: 6.932907, mean_q: 13.696423, mean_eps: 0.812856\n",
            " 1846/8000: episode: 65, duration: 1.586s, episode steps: 160, steps per second: 101, episode reward: 160.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 0.938448, mae: 7.385362, mean_q: 14.643068, mean_eps: 0.801381\n",
            " 1888/8000: episode: 66, duration: 0.429s, episode steps:  42, steps per second:  98, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.405 [0.000, 1.000],  loss: 1.292341, mae: 7.824261, mean_q: 15.536338, mean_eps: 0.790019\n",
            " 1911/8000: episode: 67, duration: 0.265s, episode steps:  23, steps per second:  87, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.391 [0.000, 1.000],  loss: 1.117278, mae: 7.896162, mean_q: 15.722115, mean_eps: 0.786363\n",
            " 1943/8000: episode: 68, duration: 0.312s, episode steps:  32, steps per second: 103, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 0.998236, mae: 8.091005, mean_q: 16.108620, mean_eps: 0.783269\n",
            " 1969/8000: episode: 69, duration: 0.256s, episode steps:  26, steps per second: 101, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.806516, mae: 8.174466, mean_q: 16.341834, mean_eps: 0.780006\n",
            " 1992/8000: episode: 70, duration: 0.221s, episode steps:  23, steps per second: 104, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  loss: 0.927484, mae: 8.282278, mean_q: 16.643739, mean_eps: 0.777250\n",
            " 2036/8000: episode: 71, duration: 0.402s, episode steps:  44, steps per second: 109, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.166903, mae: 8.419189, mean_q: 16.924619, mean_eps: 0.773481\n",
            " 2051/8000: episode: 72, duration: 0.149s, episode steps:  15, steps per second: 101, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 1.217595, mae: 8.594583, mean_q: 17.295481, mean_eps: 0.770163\n",
            " 2078/8000: episode: 73, duration: 0.253s, episode steps:  27, steps per second: 107, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.407 [0.000, 1.000],  loss: 1.238357, mae: 8.528440, mean_q: 17.020526, mean_eps: 0.767800\n",
            " 2118/8000: episode: 74, duration: 0.361s, episode steps:  40, steps per second: 111, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 1.441948, mae: 8.694648, mean_q: 17.364377, mean_eps: 0.764031\n",
            " 2209/8000: episode: 75, duration: 0.826s, episode steps:  91, steps per second: 110, episode reward: 91.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 1.509815, mae: 9.001746, mean_q: 17.994917, mean_eps: 0.756662\n",
            " 2222/8000: episode: 76, duration: 0.116s, episode steps:  13, steps per second: 112, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 1.362744, mae: 9.290763, mean_q: 18.669675, mean_eps: 0.750812\n",
            " 2246/8000: episode: 77, duration: 0.224s, episode steps:  24, steps per second: 107, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 1.555701, mae: 9.339020, mean_q: 18.663558, mean_eps: 0.748731\n",
            " 2265/8000: episode: 78, duration: 0.183s, episode steps:  19, steps per second: 104, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 1.739025, mae: 9.429493, mean_q: 18.829563, mean_eps: 0.746313\n",
            " 2294/8000: episode: 79, duration: 0.280s, episode steps:  29, steps per second: 104, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.586 [0.000, 1.000],  loss: 1.483391, mae: 9.489194, mean_q: 19.026785, mean_eps: 0.743612\n",
            " 2494/8000: episode: 80, duration: 1.767s, episode steps: 200, steps per second: 113, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 1.466741, mae: 9.916747, mean_q: 19.953851, mean_eps: 0.730731\n",
            " 2534/8000: episode: 81, duration: 0.366s, episode steps:  40, steps per second: 109, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 1.301865, mae: 10.350436, mean_q: 20.892917, mean_eps: 0.717231\n",
            " 2554/8000: episode: 82, duration: 0.191s, episode steps:  20, steps per second: 105, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 1.669110, mae: 10.425879, mean_q: 21.011552, mean_eps: 0.713856\n",
            " 2585/8000: episode: 83, duration: 0.318s, episode steps:  31, steps per second:  97, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.613 [0.000, 1.000],  loss: 2.055394, mae: 10.546019, mean_q: 21.241967, mean_eps: 0.710987\n",
            " 2649/8000: episode: 84, duration: 0.636s, episode steps:  64, steps per second: 101, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.824572, mae: 10.627477, mean_q: 21.373766, mean_eps: 0.705644\n",
            " 2665/8000: episode: 85, duration: 0.153s, episode steps:  16, steps per second: 104, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 2.140633, mae: 10.778549, mean_q: 21.606408, mean_eps: 0.701144\n",
            " 2748/8000: episode: 86, duration: 0.735s, episode steps:  83, steps per second: 113, episode reward: 83.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 1.601835, mae: 11.077040, mean_q: 22.340428, mean_eps: 0.695575\n",
            " 2783/8000: episode: 87, duration: 0.343s, episode steps:  35, steps per second: 102, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 1.589152, mae: 11.299711, mean_q: 22.766119, mean_eps: 0.688937\n",
            " 2802/8000: episode: 88, duration: 0.194s, episode steps:  19, steps per second:  98, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 1.850980, mae: 11.381231, mean_q: 22.937723, mean_eps: 0.685900\n",
            " 2961/8000: episode: 89, duration: 1.582s, episode steps: 159, steps per second: 101, episode reward: 159.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 1.852447, mae: 11.832544, mean_q: 23.941413, mean_eps: 0.675888\n",
            " 3038/8000: episode: 90, duration: 0.711s, episode steps:  77, steps per second: 108, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 1.883370, mae: 12.357137, mean_q: 25.021706, mean_eps: 0.662613\n",
            " 3158/8000: episode: 91, duration: 1.148s, episode steps: 120, steps per second: 105, episode reward: 120.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 2.036887, mae: 12.814506, mean_q: 25.955455, mean_eps: 0.651531\n",
            " 3177/8000: episode: 92, duration: 0.179s, episode steps:  19, steps per second: 106, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.368 [0.000, 1.000],  loss: 2.552908, mae: 13.102915, mean_q: 26.446427, mean_eps: 0.643713\n",
            " 3199/8000: episode: 93, duration: 0.210s, episode steps:  22, steps per second: 105, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 2.270283, mae: 13.414399, mean_q: 27.152093, mean_eps: 0.641406\n",
            " 3262/8000: episode: 94, duration: 0.550s, episode steps:  63, steps per second: 115, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 2.231678, mae: 13.346014, mean_q: 26.947189, mean_eps: 0.636625\n",
            " 3286/8000: episode: 95, duration: 0.214s, episode steps:  24, steps per second: 112, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.579368, mae: 13.738346, mean_q: 27.921307, mean_eps: 0.631731\n",
            " 3384/8000: episode: 96, duration: 0.872s, episode steps:  98, steps per second: 112, episode reward: 98.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.449 [0.000, 1.000],  loss: 2.275659, mae: 13.867825, mean_q: 28.167920, mean_eps: 0.624869\n",
            " 3441/8000: episode: 97, duration: 0.506s, episode steps:  57, steps per second: 113, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 2.182263, mae: 14.163709, mean_q: 28.765798, mean_eps: 0.616150\n",
            " 3601/8000: episode: 98, duration: 1.444s, episode steps: 160, steps per second: 111, episode reward: 160.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 2.489272, mae: 14.656347, mean_q: 29.851692, mean_eps: 0.603944\n",
            " 3622/8000: episode: 99, duration: 0.180s, episode steps:  21, steps per second: 117, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 1.811671, mae: 15.114878, mean_q: 30.735363, mean_eps: 0.593762\n",
            " 3749/8000: episode: 100, duration: 1.180s, episode steps: 127, steps per second: 108, episode reward: 127.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.496 [0.000, 1.000],  loss: 2.164474, mae: 15.508238, mean_q: 31.547763, mean_eps: 0.585438\n",
            " 3859/8000: episode: 101, duration: 1.068s, episode steps: 110, steps per second: 103, episode reward: 110.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 3.249977, mae: 15.977514, mean_q: 32.404964, mean_eps: 0.572106\n",
            " 3905/8000: episode: 102, duration: 0.466s, episode steps:  46, steps per second:  99, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  loss: 2.977142, mae: 16.314494, mean_q: 33.267500, mean_eps: 0.563331\n",
            " 4033/8000: episode: 103, duration: 1.319s, episode steps: 128, steps per second:  97, episode reward: 128.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 2.897414, mae: 16.691143, mean_q: 33.931021, mean_eps: 0.553544\n",
            " 4061/8000: episode: 104, duration: 0.297s, episode steps:  28, steps per second:  94, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 3.107537, mae: 17.184416, mean_q: 35.021736, mean_eps: 0.544769\n",
            " 4255/8000: episode: 105, duration: 1.816s, episode steps: 194, steps per second: 107, episode reward: 194.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 3.206333, mae: 17.591420, mean_q: 35.853366, mean_eps: 0.532281\n",
            " 4377/8000: episode: 106, duration: 1.050s, episode steps: 122, steps per second: 116, episode reward: 122.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 4.333756, mae: 18.230403, mean_q: 37.085753, mean_eps: 0.514506\n",
            " 4577/8000: episode: 107, duration: 1.737s, episode steps: 200, steps per second: 115, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.857135, mae: 19.015273, mean_q: 38.893709, mean_eps: 0.496394\n",
            " 4695/8000: episode: 108, duration: 1.019s, episode steps: 118, steps per second: 116, episode reward: 118.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 4.759966, mae: 19.920997, mean_q: 40.517342, mean_eps: 0.478506\n",
            " 4799/8000: episode: 109, duration: 0.939s, episode steps: 104, steps per second: 111, episode reward: 104.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 3.983870, mae: 20.205807, mean_q: 41.366592, mean_eps: 0.466019\n",
            " 4948/8000: episode: 110, duration: 1.442s, episode steps: 149, steps per second: 103, episode reward: 149.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  loss: 4.006076, mae: 20.947784, mean_q: 42.758909, mean_eps: 0.451788\n",
            " 5148/8000: episode: 111, duration: 1.821s, episode steps: 200, steps per second: 110, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.435976, mae: 21.566022, mean_q: 44.092768, mean_eps: 0.432156\n",
            " 5327/8000: episode: 112, duration: 1.658s, episode steps: 179, steps per second: 108, episode reward: 179.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 5.037743, mae: 22.597974, mean_q: 46.030469, mean_eps: 0.410838\n",
            " 5501/8000: episode: 113, duration: 1.725s, episode steps: 174, steps per second: 101, episode reward: 174.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.916608, mae: 23.509364, mean_q: 47.735540, mean_eps: 0.390981\n",
            " 5701/8000: episode: 114, duration: 1.886s, episode steps: 200, steps per second: 106, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 4.027896, mae: 24.184009, mean_q: 49.445084, mean_eps: 0.369944\n",
            " 5901/8000: episode: 115, duration: 1.940s, episode steps: 200, steps per second: 103, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 6.966163, mae: 25.097690, mean_q: 51.099916, mean_eps: 0.347444\n",
            " 6101/8000: episode: 116, duration: 1.777s, episode steps: 200, steps per second: 113, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.479147, mae: 25.962014, mean_q: 52.931012, mean_eps: 0.324944\n",
            " 6301/8000: episode: 117, duration: 1.832s, episode steps: 200, steps per second: 109, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 6.286860, mae: 26.823629, mean_q: 54.399312, mean_eps: 0.302444\n",
            " 6501/8000: episode: 118, duration: 1.937s, episode steps: 200, steps per second: 103, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 8.179188, mae: 27.608780, mean_q: 55.914656, mean_eps: 0.279944\n",
            " 6701/8000: episode: 119, duration: 1.950s, episode steps: 200, steps per second: 103, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 6.434458, mae: 28.312710, mean_q: 57.528302, mean_eps: 0.257444\n",
            " 6901/8000: episode: 120, duration: 1.831s, episode steps: 200, steps per second: 109, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.974558, mae: 29.117890, mean_q: 59.173043, mean_eps: 0.234944\n",
            " 7101/8000: episode: 121, duration: 1.924s, episode steps: 200, steps per second: 104, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 7.096574, mae: 29.776462, mean_q: 60.445373, mean_eps: 0.212444\n",
            " 7301/8000: episode: 122, duration: 1.946s, episode steps: 200, steps per second: 103, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.398424, mae: 30.723119, mean_q: 62.155516, mean_eps: 0.189944\n",
            " 7501/8000: episode: 123, duration: 1.783s, episode steps: 200, steps per second: 112, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 8.105515, mae: 31.111630, mean_q: 63.031520, mean_eps: 0.167444\n",
            " 7701/8000: episode: 124, duration: 1.776s, episode steps: 200, steps per second: 113, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 7.755374, mae: 31.888933, mean_q: 64.551284, mean_eps: 0.144944\n",
            " 7901/8000: episode: 125, duration: 1.903s, episode steps: 200, steps per second: 105, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 8.304494, mae: 32.433575, mean_q: 65.750656, mean_eps: 0.122444\n",
            "done, took 78.967 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEGCAYAAABhMDI9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOy9eZgcV3nv/32rqtdZNaPRLlmyLVm2sS1j2YAxJjZLiHNjCJc1QJyExPALEAi5D0nIxu/+SMIlIQvc4GCCAyRgDDH7FsDYGG/YkiVvkqzN2kezamZ6eqvt/P6oOlWnqqu7q2eqPUufz/PMM9PVXd1nSqPz1vt+34UYY5BIJBKJhKMs9AIkEolEsriQhkEikUgkAaRhkEgkEkkAaRgkEolEEkAaBolEIpEE0BZ6AfNl5cqVbPPmzQu9DIlEIllS7N69e5wxNhT13JI3DJs3b8auXbsWehkSiUSypCCi4/Wek6EkiUQikQSQhkEikUgkAaRhkEgkEkkAaRgkEolEEkAaBolEIpEEaKthIKKNRHQvEe0jomeI6P3u8QEi+jERHXK/r3CPExF9kogOE9GTRPTCdq5PIpFIJLW022MwAfwRY+wSAC8G8B4iugTAnwC4hzG2FcA97mMA+BUAW92vWwHc1ub1SSQSiSREW+sYGGPDAIbdnwtEtB/AegCvBfBL7su+AOA+AH/sHv8ic3qBP0JE/US01n0fiUSSMDMVA/ceGMVrd6xf6KUsON/ccxqvvGQ1ujP1t8WqaeFbe87gDVdtgKJQzfPff2oYB4Zn2rnMAFduWoEbtq9K/H2ftwI3ItoM4EoAvwCwWtjszwJY7f68HsBJ4bRT7rGAYSCiW+F4FNi0aVPb1iyRLHd++NRZfOjuJ/GS8wexqje70MtZME5PlfGBu/bi42+4HG/aubHu6+4/OI4P3f0kLljVhavOGwg8xxjDB7+6FxXDBtXajLbw29duWbqGgYi6AdwN4AOMsRkSrhpjjBFRS9OCGGO3A7gdAHbu3CknDUkkc6Rq2c53017glSwsZd0EAFQMq+HrChUDAHB6qoKrzgs9VzVRMWx8+KbtuPX6C9qyzueLtmclEVEKjlH4EmPs6+7hESJa6z6/FsCoe/w0ANFcb3CPSSSSNmC5hsGyO/v+ihvGqtHYQBZ1x3AMT5VrnhsvVAEAK7szCa/u+afdWUkE4HMA9jPG/kF46tsAbnF/vgXAt4Tjv+lmJ70YwLTUFySS9mG59sCUhgEAoFuNDUOp6ngWw9OVmufGZ3UAy8MwtDuU9FIA7wDwFBHtdY99GMDHAHyViN4J4DiAN7nPfR/ATQAOAygB+O02r08i6WgsW3oMgO8pNAuplVyP4UyUxzDreAxDPdIwNIQx9gCAejLMKyJezwC8p51rkkgkPvwGudMMw+mpMoa6M0hrTtBE97SWxhpDSa/vMYzJUJJEIlkOdKLHYFg2Xv0PP8Ndj53wjlVd0Vlv4jF4GkNkKKkKhYCBrnSCq10YpGGQSDoY7jGYdudkJRmWjaJuYczVBABBY2gWSnI1hvHZao13MT5bxUBXGmpEfcNSQxoGiaSD6USPgQvtVSE11ctKiukxAMDIdDXw3FhBXxZhJEAaBomko7GYs0l2UlaS5aZiiUZAj+sxuBoDAJyZDgrQY7PVZSE8A9IwSCQdDTcIdicZBtcYVgIegxX4Xo+SbmF9fw4AcDakM4wXqtJjkEgkSx9+99xRHoNd6zHE1xgsnD/UBSDoMTDGMD5bxcrupS88A9IwSCQdDb977kiNQfAOeB1DswK3om5iqDuDvlwKw1O+xzBbNVE1bRlKkkgkSx9uEDrJY+Bhs4rQ/kK33FBSk5YYJd1CPqNibV8Ww4LHsJxqGABpGCSSjoYbBukxxPQYqia60pprGHyPYTm1wwCkYZBIOppONAw8RVf0GOJoDJbNUDVt5NMa1vbnQoZBegwSiWSZ4IeSOqfAjTsFAY/By0qqfx14qmpXRsW6viwmi7qX2bSc+iQB0jBIJB1NJ3oM3AiKekKcOgbeQC+XVrG2z0lZ5V7DWGH5tMMApGGQSDqaTixw485RxWyt8rnotsPgGgPgz2VYTu0wAGkYJJKOphML3KI8Bt8w1C9w4x5DPq1ibX/YY1g+7TAAaRgkko7G7sB0VcuuX/ncKJTkeQwZwWOY9j2G5aIvANIwSCQdjdmBGkNU5bMuhJIYi74WoseQTakY6Erj9JSvMUiPQSKRLAs62WMQjYBoJAyrsWHoyjjzzS5b34ef7B9BxbCWVTsMoP0zn+8golEielo4dhcR7XW/jvGRn0S0mYjKwnP/2s61SSSSTtUY/N/V0xYCVdDR4aSim66aT6sAgFuvPx9jhSr+4+HjqJr2svIY2j3z+fMA/i+AL/IDjLE385+J6BMApoXXH2GM7WjzmiQSiYvdgVlJFgsahmxKDVVBW+jO1G6NfEhPPu08d+0Fg7h8Qx8++dNDAJZPDQPQZo+BMXY/gMmo54iIALwJwJ3tXINEIqmPaXGNoYMK3CzRMNSKzvU9Bl9jAAAiwu//0gUoVByDsZw8hoXUGF4GYIQxdkg4toWI9hDRz4joZfVOJKJbiWgXEe0aGxtr/0olkmVKJ9YxBEJJhi868w2/XmZSSTehKoSM5m+br75kjdeGWxqGZHgrgt7CMIBNjLErAXwQwJeJqDfqRMbY7YyxnYyxnUNDQ8/DUiWS5UknVj7brNZjqJo2erMp7+coilUL+bQKJ9jhoCiEP3rVRRjsSmPjQK6Nq35+abfGEAkRaQBeD+AqfowxVgVQdX/eTURHAGwDsGsh1iiRdAKdmK4qegwVwy9sW9uXxdmZ+h5DWbfQla7dMn/18rW46bI1AYOx1Fkoj+GVAA4wxk7xA0Q0RESq+/P5ALYCOLpA65NIOgK7Aw2DbQc9BttmMCyG7qzmHquflZTPqJHPLSejALQ/XfVOAA8DuIiIThHRO92n3oJa0fl6AE+66av/BeDdjLFI4VoikSSD2YF1DGGNgYvNfigpui1GqY7HsBxp62/JGHtrneO/FXHsbgB3t3M9EokkSCd6DGIGVsW0PAG6x/UY6oWSilUTuXS0x7DckJXPEkkHwxvKdZZh8H+uGjaq7ljPnibis+MxSMMgkUiWOdwedFIoqZ7H0NvMY9BN5CMK35Yj0jBIJB2M7zF0ToFbWGPgHkKzUFJZegwSiaQT4PagszyGYEsMLjb35prVMZheO4zljjQMEkkHwz2GTmqiZwXqGCzPQ/A9htqsJMaYozHUSVddbkjDIJF0MFYHegzh7qrcQ+jOOB5DVK8k3bJh2kx6DBKJZPljdWBWEveOiByPgRsGLj6LLbg5pWqwgd5yRxoGiaSDsTq4wC2XUh2PwfAH8BBFewx8FkOnFLhJwyCRdDCd2kRPIW4YLM8QZDQFaVWJzEoq85bbUmOQSCRJcq6o4w23PYTTU+WFXooHb7u9VAxDEtfQtBk0RUFGU1AxbC90lNFUZDQlMiuJz2KQHoNEIkmUo+Oz2HX8HPafmVnopXi0y2M4M1XGOz73C8xUjETfN4lraNkMigJkeCjJNQSZlIK0pkYaBn96m/QYJBJJgugmj+cvnmIyv4lesmt66vQ0fn5oHEdGZxN9X/8azt2QWQGPwfLqGDKa4noMtemqnscgK58lEkmS8M1XtxZH2Ma2GfjMmqQ9Bj4ytF6x2FwxrPlnUVm2ozFwj4FrCmnXMERpDCVdegwSiaQN8M3SrDNT+PnGEiaZJW4YuBFsk2GYj4dj2jY0VUFWU1AV0lXTqoJ0HcNQ9NJVpccgkUgSxNvUFonHIBqDpA0Df7+2GYZ5XEPLhjO7OaWi4rbE0BSCpip1xWfPY5BZSRKJJEl4XNxYJBqDaAySrmNoVyiJh+HmF0qyoRI5RsBwuqtmNGcrrOcxlHi6akoaBolEkiCLzmNoayjJ9Ris6Gloc8VwN+35GFfTZlAVQpZrDJaNtGgY6hS4ZTQFmtoZW2a7R3veQUSjRPS0cOwjRHSaiPa6XzcJz/0pER0momeJ6JfbuTaJ5PnGcA2CsVg0Bqt9HgNvtRHVXmI+JDFYyLYZNDXsMTieQEZTI7OSSlWrYzKSgPZ7DJ8H8JqI4//IGNvhfn0fAIjoEjizoC91z/k0EXWG3ybpCExPOF18HkPS3VW5EYy6+54Puifgz329ps38UJKrMWRSrsdQp/K5qJsdk5EEtNkwMMbuBzAZ8+WvBfAVxliVMfYcgMMArmnb4iSS5xnDXmRZSW3UGNomPpvzz0qymR9K4k30RI0husDN6piqZ2DhNIb3EtGTbqhphXtsPYCTwmtOucdqIKJbiWgXEe0aGxtr91olkkTgBsFYLBpDG7OSuKFpVx3DfAyZaTmGgXsMuulrDPXqGIq6iZz0GNrKbQAuALADwDCAT7T6Boyx2xljOxljO4eGhpJen0TSFgzPMCwuj0FTKPHKZ09jaFeB27zSVX2PwbSZKyw7m36jrKROGdIDLIBhYIyNMMYsxpgN4LPww0WnAWwUXrrBPSaRLAu4p7BoNAZ3HWlNQdK2ytMY2pSuaswnXZUxaK7HAAAzZbNpKKlYNdEtxef2QURrhYe/DoBnLH0bwFuIKENEWwBsBfDo870+iaRdmIssK4kbqIymeHf4SWF5oaSE01W9lhhzXy/3GDzDUDG8nzOaGmnMChWzo7KS2vqbEtGdAH4JwEoiOgXgrwD8EhHtAMAAHAPwLgBgjD1DRF8FsA+ACeA9jLFk/6okkgWEh2sWSx2DzXyPgRdwJYXZdvF5/hpD1i1WmykbNXUMjDEQkXdOUTfRIw1DMjDG3hpx+HMNXv/XAP66fSuSSBYOP5S0SDwGyzcMhYqZ8Hu3p1eS1w12PhqDm5XEU1QLVVOoY3COVU3bMxyMMcx2mMcQO5RERO8nol5y+BwRPU5Er27n4iSS5YSxSLOS0qqyZLKS9IS6qzqhJL7xQwglKYHPAZzfwbSZNAx1+B3G2AyAVwNYAeAdAD7WllVJJMsQr8BtkWgMvMAto6lLp4meOf/MLqclhoJsyt/+vAI3bhiEdRfdIT09WWkYouABt5sA/Adj7BnhmEQiaQLPpJlPRk2ScAE3rSkwbQbGkluX6aWrtkt8nmdLDMFjAIC0WhtK4sy6hkEWuEWzm4h+BMcw/DcR9QBYHLc+EskSwCtwS/gueq7wm25+l5ykvTLb1BIjiZRf02ZQiGJ7DNwwdHeQx9DKb/pOOEVpRxljJSIaBPDb7VmWRLL8MBdZHQO/q+d3yaZtQ1WSKeJqVyhJTyAcZ9l2jcfg1TG4noPo6cy6wnwn1THE/k0ZYzYRbQbwdiJiAB5gjH2jXQuTSJYb+iKrfObJUXxTTDJZymiT+JxESwzLZlBVv44BQE1WUkBjcIf0SPE5AiL6NIB3A3gKTlHau4joX9q1MIlkuWEm0Bk0SXyPQQ08TgKrTaM9zUQG9TjdVbPC0B2xjgEIrrsgPYaG3AjgYuYqVET0BTjFaBKJJAZegdsiqWMQC9yAZBvpmW1riTH/lN9wSwwAgZYYQNDT4fOeO8kwtCI+HwawSXi8EcChZJcjkSxf/EE9i8Rj4AVuKtcYksxKam8oaV4tMaxgHQMQUccQka4qxedoegDsJ6JH4bSzuAbALiL6NgAwxm5uw/okkmXDYvcYkhzWs6jbbtvBymcAyKT87qpAcN0F1zB0yrxnoDXD8JdtW4VE0gEY5mLTGPwmeuLjJPA1hqRnPs//GvJBPWIoiXtNfh2Dv27eWVVROqdsq5WspJ8R0XkAtjLGfkJEOQAaY6zQvuVJJMsHPsB+sWQliW23xcdJwMNl7Qslzc9j0BQCCeM9uffAw0uBOoaK2VGzGIDWspJ+D8B/AfiMe2gDgG+2Y1ESyXLEXGQaQ9gwJOsx+AVuSVZUe3UM89QY+N1/WFtIR/RKmtU7q4Ee0Jr4/B4ALwUwAwCMsUMAVrVjURLJcsRYbL2SajyG5NbFjQxjyRrCJIoEeVYS4GsLNd1VjaDH0Ektt4HWDEOVMabzB0SkwRGhJRJJDPhmtnh6JflN9JzHSb63/2ZJtsXwjes8W2K4hiGbau4xFKvSY2jEz4jowwByRPQqAF8D8J32LEsiWX4sOo+BhUNJCXoMwsadZC1DUm23PY8h5ClwETrgMXTYWE+gNcPwJwDG4FQ+vwvA9xljf9aWVUkky5DFVvnseQxqGwrchPdKssOqN9OiBSP2jT2ncHDEyZFhjLnzGIKeAjcQmqpAVQi6JfRKkoahIe9jjH2WMfZGxtgbGGOfJaL3NzqBiO4golEielo49ndEdICIniSibxBRv3t8MxGViWiv+/Wvc/ydJJJFyVw2tXYiTnAD2iM+A8l6DEZES4yybuGOB56ra9j+4pvP4Mu/OAHA7yCrEg8luR5DKpi6Gi5wk6Gk+twScey3mpzzeQCvCR37MYAXMMYuB3AQwJ8Kzx1hjO1wv97dwtokkkVPEmMpk8T2BvUkX+BmWHZk36H5YNnM2/zFa/jg4XH87+/uw2PHJiPPqxhWTQhKU4NZSTyEBDiGMjyPoZOqnoEYdQxE9FYAvwFgC69ydukFEP0v4cIYu9/tyCoe+5Hw8BEAb4i7WIlkKSNW7YaHzS8EXoFbqj0eQ1dahW7aidUyiPUfoh7C3//4RBEvPn+wZh2mzbwZGNwwqEp9jyGj+R5D1bRgWKzjQklxftuHAAwDWAngE8LxAoAn5/n5vwPgLuHxFiLaAycl9s8ZYz+POomIbgVwKwBs2rQp6iUSyaJD3NgMiyGtLaxh8Gc+q4HHSWDaDPm0hnMloy2GQVwrP358olRzjh4aBcoNCg8l1fMY+Hmd2EAPiGEYGGPHARwnolcCKLtzGbYB2A5HiJ4TRPRnAEwAX3IPDQPYxBibIKKrAHyTiC5150yH13Q7gNsBYOfOnYvDL5dImmC6zducu1gb6ZYiucnTzgI307LRn0sBSC6UJIaPxNoIPZZhcF7PHQ1VKHDTFIJWJ5TEh/RIjaE+9wPIEtF6AD8C8A44GkLLENFvAfgfAN7G23gzxqqMsQn3590AjgDYNpf3l0gWG4w5IY2cG7pYDNXP3DCk3Hh70k308u5mmlRWEr/rz6aUgMfADcaxiWLNOVU3uyhcMc01hmxK9QwjJ6OpvmGodt4sBqA1w0CMsRKA1wP4NGPsjQAubfUDieg1AD4E4Gb3/fjxISJS3Z/PB7AVwNFW318iWYzwu/Fc2h2KswhqGSy3y2iqDW23LZt53UiT8hj45p5LqQGNgRuMExOlmvYbvB4h3GNJcUNJGwfy2LAiFzgnrSneZ0nD0BwiopcAeBuA77nHGnaWIqI7ATwM4CIiOkVE7wTwf+G08P5xKC31egBPEtFeOD2Z3s0YayhuSyRLBb4x5blhWATVzxZzJpnxTTLRlhgW8xrPiRlBz43X3tXHhXtZuZQaCis571+ompgs6oFzwuNUeVEfL3B798svwHfed13gnIymoGo4ngafxdBpTfRaMYPvh5Na+g3G2DPuXf29jU5gjL014vDn6rz2bgB3t7AeiWTJIG5qzuPF4zHwsEqyg3ps5NNuKMm9a//eU8P44F178difvRIrutItvye/Zrm0GsjsEsNyxydLGOzOeI89jSHUrptrDKpCUJXgpp/RFM9T4LMYejosXTW2x8AYu58xdjNj7P+4j48yxv6AP09En2rHAiWS5YDpxcd5KGkReAyuYeCbZFJZSbbNYDPfO+J37WenyzBt5s1QbhW+yfNwHF+uaGSPh3QGrhWE6xjUBrMV0qriGTPfY5CGYa68NMH3kkiWFZ7GwA3DIqh+9jyGhA0D/119j8EJy/AMn7k21TMEjUF8zL8T1WYmhdNVeSipkWHIplWUdGetRakxSCSSdhHWGBZLVpLoMSQVSuIGJqwx8LDMXMNovoCvBT7HqQlRsK4vV2MYeEZUWHzWlPpb3+bBPE6eK6NqWp5305WWhkEikSQMNwTZ9OLRGMw2hZK4N8RDPnqoJmCuvzuvXs6FKrUNy0ZKIWwayNekrIbrGPxQUv3P2ba6xxPKi1UT+bTaUWM9gWQNQ2ddOYmkBcyaMMjCewy27WQltctjyGoqiFBTEzBXw6CHrqEphJJSmoLNK/M4USeUpNe0xKi/9W1b3QMAODgy25GdVYE5GAYiytd56p/nuRaJZNkSzkpaDHUMpqcxJNtEj/+uKZUCnUq5YdDNuX2Odw3TwRYehsWQUhVsGujCRFFHoWJ451RrWmI09xjOH+qCqhAOjRSkYWgGEV1LRPsAHHAfX0FEn+bPM8Y+n/zyJJLlAQ+vLKo6Bttuq8agqUqgvURhvqGkcGaXEEpKqwo2Dzr3rKLOUCM+x/AYMpqK8wbzOMgNQ4elqgKteQz/COCXAfC2FU/AKUqTSCRNCN/tLgaNwWJOoZevMSTU04g3qlMosr3EfA2DZ1wt3zBoKmFThGGoWtEag9ZEM9i2qgeHRmadWQwdJjwDLYaSGGMnQ4eSG80kkSxjwqmWi6OOwQ6kqyblMfDfTVMo0MJ6vuKzV8cQSvk1LBspVcF5g10AgOOTvgDNU2XDvZKUJi3Pt63uxrGJIiaKesfVMACtGYaTRHQtAEZEKSL6XwD2t2ldEsmywgx5DIupjsHzGBIyVqYQSspE9B3S5/g5/H15KCmsMXRnNKzsTuP4uBBKsnzxmTHmdVfl1d712Lq6BzYDnhsvdlzVM9CaYXg3gPcAWA/gNIAd7mOJRNIEPs4zu4iykjzDwHslsYQ1BoUcjcGwYNvMDyXNsameH0rS3MdCuqq70Q90pTFd9sVnsYGf6bY7BxoXuAF+ZhJjndcnCWihVxJjbBxOAz2JRNIi3GPILyaNwTUMikJQKLk6Bv67qa5h0C0bRd2seb5V+CafdesYLLGOwU0zyqVUVIQ23+KQIMOyffG5SShpy8ouaArBtBm6M6k5rXcpE2e056cA1P2LEfslSSSSaMJ1DItBY+DpqoCziSedlZRSye1UanveAjAf8TnUiND2hWXuMWRSKsq6bxhEj8EwWaxeSYDTenvLyi4cGp1Fdwd6DHFCSbsA7AaQBfBCAIfcrx0AWm+RKJF0IOHiLGMRaAy223YbgDdZLglMISWUewyzQuO8uWoMYndVoJHHIM6DFoyE4DE00xgAP5zUiXUMcUZ7fgEAiOj/AXAdY8x0H/8rgMiZzBKJJIgZaomxKDwGy/cYNEVJzjC4G7imOAVuM2XT65MkPt8qhmWDSBhFatUahmxKQWW6jsdg2X4TvSahJADYurobeKrzOqsCrYnPKwD0Co+73WMSiaQJ4QK3xaAx2CwYSkrKMIjis1PHYAU8hvm0xEipijBxzk1BFUJJYY2hxjDEDCUB0mOIy8cA7CGie+H0RboewEfasSiJZLnB4+P5lPNfbjFUPps2QzYlagxJFbj54Zq0W8cgagxzDiWZDGlVqam70AMeQ1BjCIvPfo1F83viF20ZwIu2DODyjf1zWu9SppVBPf8O4EUAvgFn0tpLeJipHkR0BxGNEtHTwrEBIvoxER1yv69wjxMRfZKIDhPRk0T0wrn9ShLJ4sMb1JNWAo8XEttuj8fgp4QqvmFIwGMwbSctlW/qVmQoSUXFiPYYdJN5oaQYdgGD3Rnc9a6XYH1/rvmLlxmtNtG7BsDL4HgLV8d4/ecBvCZ07E8A3MMY2wrgHvcxAPwKgK3u160AbmtxbRLJosVru72I6hhMm3l335pCieke4crnqmkHNIb51DFoqiL0dqoNJWVD4rM4FEgMJcXxGDqZVprofQzO3Od97tcfENHfNDqHMXY/gMnQ4dcC4J7GFwC8Tjj+RebwCIB+Ilobd30SyWKGZyGlFMXNj194j8GymdcaQlUo+QI3IZTEO572ZLR51DE4oaRUaEZ1WHzWTd8A8BGd/HX8HGkXGtOKxnATgB2MMRsAiOgLAPYA+HCLn7maMTbs/nwWwGr35/UAxF5Mp9xjwwhBRLfC8SqwadOmFj9eInn+4XfRKZWgqbQoPAbLZl7appZgKMkIi89uumo+rSKTUueVrppSawcL6WYwXRVw0lTzaQ1Vy/a8Ft2yvdbi0mNoTKtXR1Rh+ub74YwxhgbFcw3Ou50xtpMxtnNoaGi+y5BI2g7XFFSFkFKURZGVZDHfY1ASLXCr1RgKFWeuQVqlln73R45OBMZzplTF29T9lhjBUBIAT4DWTdvrdWRYTKixkHPFGtGKYfhbOFlJn3e9hd0A/noOnznCQ0Tu91H3+GkAG4XXbXCPSSRLHsN2Ni8iQkpTFkUdgxXSGBJrohfSGABgsqSjO6shpcU3iqMzFbzl9kfwvSedoIFnGNRgm3BHlA56DFxnqJqWV4dgmLZgtKRhaEQrWUl3AngxgK/Dz0q6aw6f+W0At7g/3wLgW8Lx33Szk14MYFoIOUkkSxrDtL073cWiMTgFbs6aVEVpi8bADcPEbBU9GQ0pNb5hmHEzmSaLOgAnzTWlBdNVGWMwLAbNNQwZt4+S6DF0eU33bPCPbjaPodNpRXx+KYAZxti34RS6fYiIzmtyzp0AHgZwERGdIqJ3wqmHeBURHQLwSvcxAHwfwFEAhwF8FsDvt/rLSCSLFVOI5zub48J7DE6Bm/NzOzQG3kQPcDb37qxjGOKO9uQhJG4gTMtGWtAYTIt51zEtFLgB8FJWq6btTWBzWmJIjyEOrYjPtwG4goiuAPBBAJ8D8EUAL693AmPsrXWeekXEaxlkG2/JMoWPnwScO+nFUMfgNNFz1pSoxmD5GVj8d54o6rhoTQ9mK2Zsj8EfCepkNPmhJF757LfRFusYAN8w6KbtVS4HNIYYLTE6mVY0BtPdvF8L4F8YY/8CoKc9y5JIlhemFcwAMhZB5bNT4Ob87HgMyVY+qyp5oR1HfE61FErimzsvjtPdkJGm+BqDYfpDgQDRMPgDejyNwc1KInIMoaQ+rXgMBSL6UwBvB3A9ESkAOq9RuUQyBwzb1xhSqrJoPAbN0xiS767qNNHzW1b3ZFvTGHyPwR/wI4aSDIt5BWz1Q0mW4DHYgaI+SX1a8RjeDKAK4J2MsbNwsob+ri2rkkiWGWJKpRNKao/H4IyvjPfetlDglqTGIFYXc40BcJrRpTQlds/o1h8AACAASURBVB0DL04rVIOhJB42siJDSa74bFgwLRs2gzdPQTftQIqupD6tZCWdZYz9A2Ps5+7jE4yxL7ZvaRLJ8sEUqnM1RQm0akiSj35vP27590fjrUkQxJMc1BOVrgoA3Vm3jiFmSwwuPnseg3sN+Q2/afmhpCiNgV9jPoHNsBgsS3oMcWhqGIjoAfd7gYhmwt/bv0SJZOkjplSm2ugxHB2bxYnJUqzXinfPSTfR43F80WOYayhp1jMMDClVcWpBVMeQ8c1fCxW4VQzL8zj4zGYeSpIZSc2JM6jnOve7FJolkjnCO4MCaGlzbJWibgX6AzUiXOCWWBM94X1rQklzMAwzgseQ1oKGjIeSePYTn+5WMWx/al5ahUKu+MykYYhDSxMo3FbY18FpY/EAY2xPW1YlkSwzTCGEoakKSsLMgCQp6WZgnGU9GHPmH/PsHFUh2AkWuHFROxPpMcTVGHgoKagxAE44zrBYbShJ8zUG3nI7o6lu/YQdSNGV1KeVAre/hNMNdRDASgCfJ6I/b9fCJJLlhO62jAaAVBsrn0tVKzCcph48auR7DEpiGoNh2d77BjSGTAppjWLrK1WvrYUN3bSdcByvHled9NpwKElzu69WDMszkGnNqafQLRuW5afoSurTisfwNgBXMMYqgNeGey+Aj7ZjYRLJcsK0bOTd1gztzEoq6mZgOE09wiMulYSzklSVGwY/XbXlUJIwcGe2ajqT2jQh9GUzL+03Lez2WU1FxbA9w5JWFa9Hk8V84yKpTytX6AyArPA4A9nkTiKJhZgBpLVRYyjpVmDDrEfYMCTZv0msj4gUn2NnJfmvK1SMQPW46moiPCyVEj4nk1JRNnzPKZNyvAjDdMJnUmNoTisewzSAZ4jox3A0hlcBeJSIPgkAjLE/aMP6JJJlgRgGSSWYGirCGPO0CzF0FQVvmKeJGkNCtsoUQkninbzvMcTtleQv6FzJAGMIaAymzTwDK6ag5tIKqqLG4NY+yKyk+LRiGL7hfnHuS3YpEsnyhQ+ZAVyPYY7jLRtRDU0uy6frv5a32BYL3Fr1GL7zxBmcmSrjXS+/IHBc3Hx5SwwA6HLnMeiWDcYYqEmhmSiin3M7rHqGIaQxpEKhJFF8FjUGxmQDvTjENgyMsS8QUQ7AJsbYs21ck0Sy7DBF8VltT68kMdOpmcDLjQAPb81FY/j2E2fw7NlCjWGwbL/Km3sMGc2pguYbuCm8ph4VIe120jMMvodj2MzTasSQVS6tuuKzn5WUFuZAyAK35rSSlfRrcMTmH7qPdxDRt9u1MIlkOSG2xGhXr6Ri1fR+blbLwENJQY+hNcNQMazAZ3KcOQ++d6QQvClqXAuIo7FUTSvQthvwDUBKUWBZfiipmcfAQ1iWDdkSIwatiM8fAXANgCkAYIztBXB+G9YkkSw7TNtGShHi423IShI9hma1DH4/I6FgrMU1VQ0bRT3CMAgNAwHnjp03suMbuBFjJkPVsDHUnQHgtO0Wz+ctPPQILyCbVt0CNz9dNeWOFLVs2/OSJPVpxTAYjLHp0LGFbxEpkSwBxLbbTigp+f86JWGTblbLwA2DImQltTrBrWJaqBh2jfdjCRlYgLMx82E5vAtqnFqGqmljsNsRSrjG4BcJOppIVCgpqymBlhgZ12PwC9ykYWhGK4bhGSL6DQAqEW0lok8BeKhN65JIlhW6WLXbpjqGoMcQzzD4HkPrBW68tXXJCHon4dbWaU2p9RhihpK6MxoymuJ5DNwA8G6wkaGklBpoopd29Q2vJYYMJTWlFcPwPgCXwmm9/WU46asfmMuHEtFFRLRX+Johog8Q0UeI6LRw/Ka5vL9EstgItMRwN2GWUAsKTkBjiBlKUj3DgJbFZy4Oh3UGUWMAnDt23uG0NcNgI6Mp6MmmMFmsBs7n4ThPUBY8lFzKDSWZQY/BsFjN2iTRtJKVVALwZ+5XDUT0KcbY+2K+17MAdrjnqXAK5b4B4LcB/CNj7O/jrksiWQqYtu0Jr1yEjpOZ0wpz8RhUwWOwXGPVLI2Uwz2GYjXsMQRrKK69YBBbVzk9OFsSnw0bGU1Fb1YTspJEjcEWZj6LHoMSKHALagxKIIVWEk1LTfSa8NI5nvcKAEcYY8fj/kFKJEsJxpwK3ZSQqQM4d9YptdGZrSEKwc3aYnA9QRWykgCnh1JcW+UbhqDH4KSr+pvvx99whfezpzHEEJ8rpoVMytEnjo0XAQDisKOKWSeUFEpXTbsFbrolB/XEZTGYzrcAuFN4/F4iepKI7iCiFVEnENGtRLSLiHaNjY09P6uUSOaIF89X/fg4EE+AbYVSNb7HwDUOVchKAtBSkVvFjA4lGQ3CNS2FkgweStK81ttp4RryUBJRsGgtq6momjaqhpPuSkRIu5XPli0H9cRhQQ0DEaUB3Azga+6h2wBcACfMNAzgE1HnMcZuZ4ztZIztHBoael7WKpHMFR7u0IQ6BgCJ1zKIHkPVaKwx8BbbYcMQV2ewbeZ5JcVQC/FGm2+r4nM2paIn44+W56Eo1WuJEfROAH8mw0zFQMYrKlRgmFJjiEuShmEuV/tXADzOGBsBAMbYCGPMYozZAD4Lp25CIlnSGKFhMmL1b5KUW9AYzJDGoCkUON4M8f1rxGeb1e3TxH/3uOmq3GPg8HWm3JYYYmM9Dp/JMF02/II4jeSgnhZo2TAQUS8RRU1z++c5fP5bIYSRiGit8NyvA3h6Du8pkSwqxBnIgO85JN1htaibfpiqiWGwa8RnChxvRkXwSMJFbmITvTB8AlucRnqOYVC9GgggJD67oaRwwRr3GKbLhjcPgmsMYudXSX1ii89EdDWAOwD0OA9pCsDvMMZ2AwBj7POtfDARdcHp0Pou4fDHiWgHnO6tx0LPSSRLEtNLqQxlJSVcy1CqWujPpzE+W227x1AR0mGjxOemGkNTDcTRAzKaAkXxQ0liHQPvrhoOJfG5zzNl03s91xhsYWqdpD6tZCV9DsDvM8Z+DgBEdB2Afwdw+Vw+mDFWhDMNTjz2jrm8l0SymOEN87yMGoWHkpL3GFbkU65haKIxeAVufsweiK8xiA3uZmvSVWvj/py4GkNFmKWQE7KI/CJBxS1wYzWhJD4caLpsIJsSNAaL1RTfSaJpxaeyuFEAAMbYAwBqG6VIJJIA/O7Ym8egxg+ntEJJt9Cd1ZBSqQWPAYHvsT0GIZRUqilws5t6DM00Bi6ei32WnPN9D8ew7EA7c04wlKR6n8srpaXG0JymHgMRvdD98WdE9Bk4mgAD8GbImQyLhnC4QrJ44J5ByguD+HUMSVKsmuhKa8hoavw6hrDHEHNNDTWGBnflac9jaPw53LBlU37VtHi+KrTECP/Nc/F5phIUn/m6ZUuM5sQJJYVTRv/S/U5wDIRkEfCBu/ZCVQj//JYrF3opkhDe+MmQ+Jx4HYNuYWV3BhlNad4Sg9cxhArc4jbSE0NJ4crncBM9kZQWT3gXZyn0RIjPKbV5uipjviHh38uG5c2jltSnqWFgjN0AAESUBfA/AWwWzpOGYZFwfKIkRbVFipeVFE5XbYNhyKdVxzA0mccQFp/9OoZ4a2okPhuWXTfzJ67GwA1bOF3Vr2MgmBZPVw3+3WeFcvJMKnjNK0b9jCmJTyvi8zfhzGJ4HEDFPSYNwyKhqJvSRV6kGKFpae2qYyjpJvIZDWlNaaox1Ctwi13H4IaSerJaTSgpTlZSs1CX1zI7FV3HIGYlhUNJOcEwhGtHADmoJw6tGIYNjLHXtG0lknlRqlpSVFukcPHZG9TTrjqGqoWutBpLY6jnMdTTPf7j4WP4wdNn8eXfezEAP5S0sjsT0USvfigprsZQEcTnnqxQ+RyY+cxDScHPEpvkZVJcfPZfIz2G5rSiVD5ERJe1bSWSeVHUTZSbtEGQLAxmKF011Qbx2bIZyoaFfFpDJtVcYwgXuPlN9KLXtOfEFHYfP+c95hv3QFc6uvK5rscQbRQtm+F1//IgfrJvBICoMfgeg6pQoBus6bblqFfHAAgagzDIR2oMzWnFMFwHYDcRPes2uXuKiJ5s18Ik8WGMoaRHz9+VLDxGKGNM89puJ+cx8JuCroyrMcT0GLSYoaSpsoGq6U9r44ZhsCsdCCUxxtxQUvTWoioEolrDMFM2sPfkFPacdIyPn5WkIpdSoSoUuOvnQn7FsGrFZ9EwaEHxGYAMucaglVDSr7RtFZJ5UTWdKlH+JUNKc+OT9xyCqhDec8OFib4v9ww8j6ENdQy8liCfdjSGShPxmXsM/mjPxgVuUyVnHkJRt9CXU7wCtMHudCCUFJ4MF4aIvPYUIgW3eyr/LorPRITujBbwZvhdf9WsrWNIqYqXziq2xODIUFJzYnsMjLHjUV/tXJwkHuKAllLEcHZJPH6yfwQ/ckMZScI9A775tqPymf8NOB5DfI2Bb5L8Br9eeGuqZADwM5ACoSTd9KbRee/bIFyTdjudisxUnPefKTvf/XnNzt1/T1YL3PXzdZd0M7LKmnsNnmEQQ0myV1JT5BVaBojGoKxLnWGuzFZNFNyNKUn0kMegtcFj4OGcXEqLV8fgbuQKxfQY3OvC/9YqhtPVtCebAmN+KCtscKLg09REZqthj8HPSgKA7owWuut36xJ0q6YlBgCvFUY6NDUP8Ku8JfWRl2gZEPQYpGGYK8Wqiek2GAYel0+FUieTzEoSPYY46aoW1z3CdQwR4rNtMy+UxPsiVQxnulqX266Cb+xe4VyDu/KU29BOhBsE7jn4WUnO+/RmUwEvhP9cMWu7qwK+AJ2J0hikx9AUeYWWAaLoHM4p7xQ+/sMD+Prjp+b1HsWqhZmK4YVFksIMDerRmqSGzoWioDFkNCVGSwznuxLKSooqcCtUTXBHgmsZfIhOl1tlzKfHee0/GoSSojUGw/0e8hjqhJK4IYvKSgJ8w5CO0Bikx9CcJGc+SxYI0UtYDqGkkm7i4z98Fn/4ym3oy6eavp4xhtvvPwrTZnh2pIA//uXtLVeB2zZzY+VOmIS3VUgCww55DFp7PYaMO9qyEZYd7TFEGavpku9FzVb9UFI2wmMI10dEkdaUmjCa5zFwjcEMegyvf+EGDE+XvdenhLv+RhpDVIGb9BiaI6/QMiDoMSx9w/Dg4Ql8/qFjePjoeKzXz5RNmDbDpoE8PvOzo/jDr+5t+TNLhgXuKCQdTgoP6vHqGBKsfOZ/A12ux9BstCe3SXFGe06Vdf9zdF98zmoqutKOYeCGKa7GEG4HEqUxpFXFM/C/evla/O7LzvdeLxqeKO+EawwZz3OQBW6tIA3DMiDoMSz9UNL+4RkA8AbAN2O8WAUAfPBV2/D2F2/Ct/aeadlzEo0rj3MnRd06hgQ9Bi7+5tMxNQbXYwgXuEVpDOcEj6EoaAzZlIqujOoeD2oMjaakRWkM/JrP6iZsm6Fq2J63EIWoKzQMJUW1xJCGoSnSMCwDlpv4fOCsYxgKgmEoVk3cfv+RyGybyaJzRzvYncZFa3oB+HegcRFfP5Owx8DDJnyT4ptwollJVR5Kctpum25NSz08j4FieAwlwWNoEkoK94WKwtEYokNJjDmaRtW0Aq0twmhNQkmNNAbpMTRnwQwDER1zq6f3EtEu99gAEf2YiA6531cs1PqWEmK66nIIJe0fLgAIbtD3HxzD33z/AL6150zN6ydmHY9hoCuNbvcOtlXDIHoMyYeSgpslEblN4JLUGEwQOTF5vqE2EqAt2wZRbYFbpMYgXA/+91Xh4nOGh5JcjyGOxqAqNaM9Z4WbgELFQMWwPeE5iuahpFAdQ4RwLanPQnsMNzDGdjDGdrqP/wTAPYyxrQDucR9LmiBWni71UFJJN3Fsoggg6DHwPPqvPHai5pwJ12NY2Z3xhrrMxgxDcWbbGUqKiLtrKiWclWShK62BiLzNsFEtQ7ifEb8Bj/YYnOuR0ZSAx5DRVHSnucfgagxxQklabR1DQbjmM2XXY2gUSlIah5JyoToG2RKjNRbaMIR5LYAvuD9/AcDrFnAtS4aSbnp/+OFOl0uNZ88WPBFY3KD5XevjJ6ZwcKQQOGdi1jEMK/Jpbwxkodra5i5et5lyssbVmU9AIHF2sVKbsjkfSrqJfDoYPmmkM1iMBdpPaw0E8XMlHd0ZDX25lOcZVA0L2ZSCfIanq/KspGC2UxT16hj4KYWK05cpk6rvMcTVGLzRnlrQKEsas5CGgQH4ERHtJqJb3WOrGWPD7s9nAayOOpGIbiWiXUS0a2xs7PlYa2wYY/jxvpGmladJUtRNdGedbJSl3mH1wFln0+/OaKG7SAMKOXd+X3n0ZOCcidkqerNOjyDeibNVA9nuUFJ480rcY9AtL6zDN8OGoSQr6DE0KnCbLhnoz6fQldECBW7ZlIqUqiCtKZjVQ+mqc9AY1vRmAThJB1WzifgsagwRr8s1rGOQhqEZC2kYrmOMvRBOc773ENH14pPMqTKK/J/DGLudMbaTMbZzaGjoeVhqfJ44NY3f++Iu/Pk3nk68UKoepaozuasroy35Xkn7h2fQlVZx0ZqewJ37dNlAfz6NV126Gl/fcyowc3iiqGNldwYABDG0tc290GbxOXyX6oymTNBjqPoeQ5xQksVYIDvHy0qK8GKmytwwqH4oybS9lNCutOoVuDVroge4GkNEuuq6/hwA12MwGoeSAhpDxGdlQhpDwAjKUFJTFswwMMZOu99HAXwDwDUARohoLQC430cXan1zZWTGGW73td2n8OVHa+Ph7aCoO0Pgcyn/P+hS5cBwAdvX9qIvlwqEg6bLBnqzGt569SZMlQz89zNnvecmZnUMdKUBwAsltaox8A2vN6slrjGYdq3H4IRTEuyuqlteTQHfDBt1WLVqNIb6bbfPlXT059LIp7VAE72s65l0ZfzjXmpuw3TVWo1hpmJ4hmGm3DyUlGoaSnLrGNxrQUReuFV6DM1ZEMNARF1E1MN/BvBqAE8D+DaAW9yX3QLgWwuxvvnA491XbOjDR779DB4/ca7JGfOnpFvIZ5yc8qWcrsoYw/6zM9i+psfZoEMeQ18uhWsvGMTavix+9IzfBXWyqGOw2zEMPJRUmENWkkLA6t5sWwrcwnfQWkSR13xwxnq2oDGE2rNrDdJVeSipO6N5nVR5KAlwjLHXKylGd9VUKCvJtlnIYzCdXkxxPYYYoSTnc4NNDCX1WSiPYTWAB4joCQCPAvgeY+yHAD4G4FVEdAjAK93HS4pJt9jqs7fsxGBXBv/444Nt/0x+t5hLaygtYY3h9FQZhYqJi9f2oiebqtEYenMpKAph2+oeHJ8ses9NFKsY6HJCSRnN6cXf6tCi2arjdfXlUomLz3qUxqCQl62UBEXdEkJJMTSGkGFopDH4oSQNpaoFw2KwmX9Xnk+rNZXPje7KU1pQY+CtSAa6UsilVMxUDOimHZjEFiagMUR81tq+LNKagv5cOvC5gJz5HIcF6ZXEGDsK4IqI4xMAXvH8ryg5xmd19GQ0rOrJ4rINfTg5WWr7ZxarJga70rBs5mWHLEUOuPULF6/twfB0GTMV5+6UiDBTMbFxIA8A2DSQxx7XE7NthsmijpWux8CHuswllNTlZt6cdcOBSWFGzCVOqUpLHoNp2ZguGxh0tZQwjsbghpJSMTQGmwVi7b7GEDQMvLNqfy4N03Lu7Cvu+/KNuyujeanF3lCiBqGksMbAvY2ebAo9Wee9morPTUJJr75kDR784xsDvba8edGyV1JT5BVKmMmijgF3k+rPpdrSxjlMSefi89IOJfFWGBetcTwGPscY8ENJALBxIIeZionpkoGpsgGbwdMYADejqeVQkoWujIreNvybVc3a8ZOtZiV96RcncMPf31e38V5Rt7xOpzyW3jSUpNZ6DGGNgXdW5R5DsWr6LbG5YUhrQoFbsNVGFJoS1Bi4UenOaOjNpTBTMVqrY4h4naIQhnqCRpRfF2kXmiMvUcJMFKsYdDepvlzKKw5qJ0582Q0lLeGspGdHCtg4kHM2iKxjBAqu1zDthpIAx2MAgJPnSl7Vs3gn3ZNt3WOYrZru52qJZyWNzFSxqje4SWmK0lIo6cDZAmYqJs4V9cjn+d8A4Id4mtUxiB4DEUGhWo2Bd1btz6ed7CPDQkV35zG7G7JjMEJN9BppDJoSMgzOZ/RkNd9jaKXyOaaY7GkM0jI0RV6hhJmY1b1Nqj+fQtmw2l7T4FS9Or3xl7LHcGKyhM2DXQB8EXmmbKCoW7Bs5nkMG1a4hmGy5Fc9Cx5DlyuStoIYSipUTW8mchIMT5exti8XOJaO0QE1/B6AX+Utops2DIt5HkMcjcGMmA2uKUqNxnDO7ZPUn3M8Bsb8Y774rNaKz00L3JiXzs2bJfZkU+jNpoSspMaN+LyfG3gWIlyIlllJzZGGIWEmirrvMeSd7+0MJ9luuCWf1pBLq0t6HsOJyZLnDXiGoWJ6d/DcMGwazHuv51lgPHwHYE4ag+cx5FJeI7ckMCwbo4Uq1vVlA8dX9WS81OY4nJ12XjsZYRj4v3k+lK7a6IbEtlnNnbOqUI3HwFuRrOhKeR7JhJtgwQ1DXqifMWK0xEiHRpvOeobB8RimygZ0q7HGEOyVFG8bS8l01dhIw5AgXAgdFDQGIDjoJGl4DL4royKfVgOD2ZcSMxUDUyXDE5h52GimYniGlRuG3mwKfbmUE0pyN6nBLj9U051tXWPwQ0nu5yZkzEcLVTAGrO0PegwbVuRxeqoc2zM5M1XfY+B1F13hdNUGdQymzWraT2sK1XgZvLNqX85vUDg+yz0G53O6MxoMi6FqWr7G0CRdFfBrHgqCYejNpTBeCBqeKLSIVNtm+OKzNAzNkIZhDvzHw8fwG599pOb4TMWAZTMvdbLfzYiYaqPHwEMm+bSGfFqDzRrHltvNQ0fGcf3H723ZS+LZW9xj6OX1CBWzxjDw152YLAt9kvzneoSCq7jwUJJokJJg2N3Q14Q8hg0rcjAshlF3E2y2Nh5uidIYjo47qbubBpwwnBdKapD1ZNu1tRUrezIYmw2uZ8rTGFJeAd3EbDCUlBfGe3KNoVHcv9YwcI3ByUriHVwbZyX5z6XjhpKkxxAbaRjmwC+em8QjRydq3G5+J8VTJ/ueB4+hVA16DMDCzmR45OgkTkyW8OSpqZbOOznpbKAbV3DD4N+5c8PAjwFOZtKpScdjWJFPBTaKrjmlq1quYXA2v6TCf2fcENC6vrDH4Dw+da55OrM40jLKY+DZXBev7QEwd49hXX/W80w4nmFwNQbAb3OeE9JVAcfr4plWzeoYAN9wFSpOy/CutBr4N47fdjtmKEkLzp6Q1EcahjkwMlOBzYIDTAA//stTJ3lxTaseg2UzvO3fHsF9zzbvCMI9hlxKE8YsLlxm0nG3ZfbTp2daOi/sMfQIWUlhjQEANg7kcepcGeMFPZCqCsCt0LUaDqoR0U0bumWjO6MKBimZa3jW3dTX9oc9Buf3PHWuXHNOmOFpX4vgBZQiB4ZnsLYvi35X01IVQkqlpnUMYY9hXV8Op0PrmSo7dTma6g/l4X/nfq8kf7ynP9qzBY3BDeMRkecpAo09hmBLjNZCSdIwNEcahjkwMuP85+QeAsdLnXRDSby4JmxAmjFRrOLBwxN46MhE09eKQ+D5APuFFKCPTzgb/NOnp1s67+S5EnqzmnfNsikFKZUiNQbA8Sx0y8a+4Zmaoi+vw2pMA+nNS3azkoDkQklnpioB7YKz3tUcTk/FMAxTjmHIpdRI8Xn/cAHb1/QEjqXVxuM9wwVuALCuP4fRQjVgUKZKhvdvwrOext01ZLxeSf5wpJY0Bnd9MxXDuz49osfQICtpPuKz1BiaIw1DizDGvGySiVA8drwYDCX1ZDQo1HpYYrzgvM9ojKwVvqk5GoM7f3dBDYPrMZxpzTCcmCx5wjPg5NXzthgzZQNE/oYP+J7FicmSd705/M42rs4wKxgGT2NIKJTkpKpma47n0ipWdqdjhZLOTJdBBFy0pseL73OqpoUjY7O4eG1v4HgmpbbUEgMA1rvhrZFp/+96qqRjheuJ+B5DUBzuFq63n5XUmvjM/215KA9oHEpqNtozCq/ATbbEaIo0DC0y7eZYA74h4ExyIdQNbSgKzamSlguAcYTJsi5qDAsbSpouGzhXMjDYlcbxiVJLv/dJIVWVwxvpTZcNx8gKm4342qhQEhC/wyo3DN0ZDT0ZDURJGoZKjfDMWb8iHy+UNFXByu4MVvdmajyGw6OzMG2G7WHDoClN227XGIYIL4b3SQIgaAzBUNKqHuf3G54ux65jAHyNYVYwDKLHkG3iMfD9PX4oSTbRi4s0DC3Cw0gAvLQ6zkSxir5cKnAH0z+H6mf+vrEyVrhhEDyGhWq9fcINI/3yC9YAAJ6J6TXYNsPJc+WAxwDA8xjEqmfOuv6ctzGIqaqAk64KxK9FEENJikLoyWheFtB8OTNVqRGeORtW5OIZhpkK1vZlMdCV8YrLOLy/1CVrg6EkxzC0VuDGu5uKAvRUyW9FwkNJE8VgVtK6/ixSKuG58VKsJnppLagxFKqGZ8zjis+Ab3xkHUPySMPQImJR0kQxbBj84jZOXz7dsvg8zj2GGKGkkpeuqnqx3oXqsMpnNf+Py9YCAJ6JKUCPFqrQTbvGMPTmnA16pmIG9AXAybzhG+5gKJTUMw+Pwfncxl7ex394AD/ZN1L3eY5u2hifrdYIz5wNKxyxl9cy/NvPj+L7Tw3XvG54yglHDXalca5kBGofDpydQVpTvIpxjlNZ3ThdNbxB8pBXwGMQQkmaqiCjKdBN2xW4Fe/4poE8nhufhRkxxjRMdCiJawzxxGfA3+DjZyW5hkGGkpoiDUMd/u3nR7H7eO0sBW4YiHwtgDMxW63ZpPpzKUy3KD5zwzBTMQOTyqLgPWrybtttACgvUCjphJtZtGNTP9b1ZfFUTAH6pBtn37gieGfdk/E9hrBhAPyUz7DH0KrGwK+heNdaL5R0ZqqMT993BO+9EWEYDgAAH99JREFU83HsO9PY8PG/lfoegyOgj81WUTEsfPy/n8VHv7uvJptqeLqCtX05DLgddEWjtX+4gItW9wTSdQHnbrtRHUOUx5BNqVjZnfE8Btv9rH6hRoRf22xo096yshvPjRcjtYswYfE5qDHEE58Bp4OrQvE9gLQnPsttrxnyCkVgWjb+9gcH8KVHjtc8x/+zbxnsqvEYJot6zSbVn5+DxiCEkEZnGoeTSrqTA55NKci7rn2r846T4th4Eat6MsinNVy6vi+2AM1DUDUaQ87XGKIMA399PY0hHEqybIZP/OjZQF0AIIaSnOvX53b4jOKnB5wU4oym4t3/ubthjcqZOsVtnA39vJahjN3Hz0E3bZyZruD+g/4c85mK4Q6xyXo3HWItwwF3sFGYZhqDHZGVBADr+7Oex3BisgSbIdDniV+jcFXy+UNdODZRgu56DI2I1hj8cBU/vVkoSVWpxiA2/lznjaVdaI68RBGMFqqwbOZVlIqMzDg6wvoVuYh0VT3QswdwO6y2HEry33e00Dic5DTQc3LA+QSv8gKFko5PlnCe28foBev68Nx40QvThHni5BT+wzW8J8+VQORnxXBEjSHKMPDQUzgrid99hkNJ+87M4FM/PYwvPhw0+LWhJK2uMb/3wCg2DeRxx2/txPB0Gb//5d04OjYb+Vpef7CuQSgJcIrcHjw8DlUhDHSl8ZXH/JGwvEfSGtdjAPw6gtFCBeOzek1GEtA8lGTadmRK6br+nGfQ9p50ihR3bOz3nuc1C2HDsGVlF3TTxsnJUtPNOu2FkpxJcLple/9mPBsNaB5K0hR/XGcc5DyG+MgrFAG/Y3ou0jBUsKbXifeOC+mqls0wWdIDXT4BfyZDK906x2er2DjgbBrNBOiy4Q+BT6vO9LK5ZiU9e7aAb+45PadzASdV9Tw31n3Zhl4whrrhlv/93X34i28+jYcOj+PEZAlrerM1d4i92RSKuoWpkl4jPgPAy7cN4ZotAzXahFiJK3JwxBFqf7o/WDgois/8c6MK3CqGhQePjOPG7atw1XkD+OjrXoBHn5vEjZ/4GX7zjkdrahK4YQh3VuWsX+F7DA8emcCOjf1441UbcM/+Ue+GgG/S6/qygmFw/ia48Lx9bT2PIdowWDbD6EwVA/l0zXPr+nM4PVUGYwx7T04hl1KxbXW39zy/RuEwD9c4Do3ONvcYPPHZDvRJ4vCfG/VKApwNPm5GEiDF51ZYqJnPG4noXiLaR0TPENH73eMfIaLTRLTX/bppIdbH/zNOl42a3jQjBae3/sruTCCnfKqku+MJa8VnxvxGYXEYn63iEvcusJkAzVs5AM7dVj6lzjmU9MmfHsIH7tqLw6OFls8t6xZGZqo4b8D3GADHMwhzcKSA3cfPgQj4yHeewXPjxZrNHfA3CMNikR7DFRv78dV3vaRmA0m5ImlYYzjo/l7PjhQC9QOzVRNpTfE2jnqhpIePTKBi2Lhx+yoAwJuv3oSH/uQV+KNXbcMjRyfwr/cdCbx+eLqM3qzm/fuEyac1DHalsW94Bk+dmsJLLxjEm67eCNNmuHv3afc9XOPSL3oMztoOnHVbYayp9RgyWv06hgNnZ1Comti5eUXNc+v6c6gYNs6VDOw5OYXLNvTVtBsBgKxWG0oCnLTj2BqDZQdmMXB6Y3oMogAehx0b+3HdhStbMiadykJ5DCaAP2KMXQLgxQDeQ0SXuM/9I2Nsh/v1/YVYnHjnFw4njUxXsLo3i8HuDMqG5d2d87hvuAqXd1idKgcNTKFi4DtPnKn5bNOyMVHUsW11DzSFIj2G/cMzeOSoUxVd0n2PAQDymbm33t57wtnEb7vvaMvncuH5vJXOBrGqN4uL1/bi73/0bE2mzVcePYmUSvibX78MB0dmsefElNcjSUT0EqI8hkb0RHRYPXi24BmYew/4XgNvySB+Vkm3aqal3XNgBPm0ihedP+AdG+rJ4H2v2IprLxjEg0fGA68/M1XxUkDrsWFFDj/ZNwKbAddeuBIXDHXjmi0DuOuxE7BthuGpMhQCVvdkajyGfWdmsKY369XNiGRS9TWGXcecpIqdmwdqnuO1DM+NF7H/zAyuFMJIgJ+yGq4xcLQlFTZrniXEwz+6aftjPTNCE8RssH14PTS1NcNww/ZV+M/ffVHDjCmJw4IYBsbYMGPscffnAoD9ANYvxFqiEPO4xXCSZTOMzVaxujfjCYE8M4l7DzVZSW5GRzhm/eVfnMD77tyDQyPBu/NJ1/MY6slgqCdTYxhKuol3fv4xvPfLj4Mx5mkMnHy69SE1gCN4n54qY0U+hW/tPR2rVYMIr3g+T7jz/893XoMXrO/D73/pcfzLvYfBmNOa+et7TuHVl6zBW67eiJdtXQmgVngGgneRUR5DI6JmMhwcmcX124Zw3mDeE5EB3lnVN669wpAgDmMM9x4Yw3UXrowURV96wUocHSt6mgDgeAz1hGfO+hU5VE0b2ZSCKzc5m/AtL9mMYxMlfOqnhzE8XcGqnqybKqqiO6N5NyFPnZ7GC9b3Rb5vo5YYjx6bxLq+rGcEAutxj92zfwS6ZQf0BUDwGEJeGhFhi3tTEN9jYP5Yz2zQMKtKc2FZc3tCSZJnwTUGItoM4EoAv3APvZeIniSiO4io1td1zrmViHYR0a6xsbGol8yLM1MVbFvdDVUhPDfuC4sTRUeUXtObxZDrGYy7d29RcwEAf0MLF7ntce/Owymd3NCs7M5gVYRhuO2+Izgz7YiOxyZKKOmm1yMJcOoZ5uIxcKHxIzdfCgD47P2teQ28R5KYTz/YncGXfvdFeO2Odfi7/34W/+trT+K7TwxjqmTgzVdvBBHhr37tUvTlUtixqb/mPcVip1YNA59PzClWTZyeKmPbqm7cuH0VHjoy4V2n2aqFbuGOlfcGEovcnh0p4PRU2Qsjhbn2wkEAwIOHfa/hrJtm2gjeTO/qzQOewbnpsjV4/ZXr8U/3HMR9B8cCxmWgK43Joo5i1cTR8SIuq2MYMinFCyUdHp3F4VHn75gxhl3HJnH1llpvAfCFcu7lhf9dfI+h1jhyw9A8K0nUGGpDST1Zram34HyO0pLHIInPgl5VIuoGcDeADzDGZgDcBuACADsADAP4RNR5jLHbGWM7GWM7h4aGEl/X6XNlnDfYhU0DeRwb92PRvIfMqt6s4DE4xyaLjT2GcGYS34jDXUi5oO14DNmAxnB8oojP3H8UL3T/sz723KQzBD4TNAyN2m7vOzMT2dP/iZNTUBXCqy9Zg9dduR5feexETS+oRhyfLKIvl/I2VU42peKf3rwDf/jKbbj78VP40N1PYsOKHK670PEULlzVjT1/8Sq8fFvtv2Mw7hwdp69HdyYYSjrkboxbV/fgxu2rUDVtPOSGfopV0xtCA/itv7+26yQAJ7XzYz84gJRKdQ3DxWt6MdCV9sJJFcPCRFGvmdwWhmcmvdS9HoBz9/3Xv34Ztq/pxVihGshq4oZh//AMGANesL5WXwAcjYF7DH94117ccsejMCwbJyfLGJmpRoaR+PtnUwqOTZSwujdTY9h8j6F26/AMQ5O7eF5oZli2Z3zFm4Btq3u8JIZGxPEqJHNjwa4qEaXgGIUvMca+DgCMsRHGmMUYswF8FsA1C7G2M1NlrO/PYcvKroDGwGsYVvdmsdL1GLhbPz6rgwhelSinz229LRa5nZ2u4Kz7XuEupNww8N44osfw/313HzSF8Om3XYUV+RQeOzaJsm55PZIAIJf2xyyemCgFMqcKFQOvv+1B/M3399f8zntPTmH7mh7k0ipuvf58VAw7UgOpx/GJEjYP1oaDAGeje/8rt+KTb70SmkL4rWs3B/oehecCcEQvoVWPoScbDCXxjKRtq50Yflda9cJJRd0MCMQ7Nw/gLVdvxKfvO4L/fuYs/umeQ7jv2TH85a9dilW90Ru9ohBecv4gHjo8AcYYfubWImwZarzBXbquD5pSa3ByaRWfeftVGOhKY9tqP+to0DUM3NOsF0ridQxl3cK+4RmcnirjO0+cwWPHJgEAV0cIz4Dzb8V1kXAYCagvPgMQQkkxNQbLDoz15Lzr+vPx/T+4ruF7AI7nkZahpLawUFlJBOBzAPYzxv5BOL5WeNmvA3j6+V7bTMVAwS0o2jzYhWPjRW9U5kiBGwZfCOR31ROzVazIp2viq96wHsFj2HvSEf8u39CHZ85MB1JZeXHbyu40VvVkMVnUoZs2Do8W8JP9o3jPDRdiTV8WOzcP4LFjk86mJoSSulyPgTGGt9z+MD7wlb3ec/fsH0XFsPHTA6OBz7RthidOTnkbwbbVPVjXl8VjEZXf9Tg6Vmx6l3fzFeuw9y9fjXdetyXWe85bYxA9hpEC0pqC8wa7kNFUvGzrEH6yfwS2zTBbNWsyhz5y86W4fEMfPvCVvfjkPYfwhqs24O0v2tTwM6+9cBBnZyrYP1zAR7+3D1tXdeOXL13T8JyrzluBJ/7q1YHNn7NpMI+ff+gG/MGNW71jK1zD8PTpGQz1ZLC6jqFKawoMi+GJU1OwbIaMpuC2+47g0ecm0ZvVsG1V7edx1nuGodZ48L+1zLxCSc62UzFs/HifI+iL4j9R45YanFazkiTxWair+lIA7wBwYyg19eNE9BQRPQngBgB/+HwsZmK26m0iXt54fw5bhrpQNiyvcd7ITBVEwFB3BtmUip6M5hWjnZ4qe7qDSFpTkE+rAY1hz8kppFTCG3duRFG3vB5DgOMxZDQF3RkNq3oz3rEHDjkhipuvWAfAueM75nYwzWdEj8ExDM+cmcGZ6QoeODzuCcPffdKJG08UdTwhTFg7Oj6LQtUM3CFevWUAu45Nxpoffa6o4/RUGZeuiw5riOTSauyskHCmUCuENYaDI7O4cKjbM9y/ctkajMxUsfvEOSeUlA4ahmxKxW1vvwq5tIoXrO/FR1/3gqbrfukFTjjofXc+jpOTZfy/N18aa+Oql87KnxM9qsGuNCaKOp4+PY0XNLjeXK949DnHQ/jQa7bj0OgsvrHnNHZuHqjrpQF+C4+GHkODUFIz8VlVCAo5I3IfPjqBj9x86ZxCQjxBQ5I8C5WV9ABjjBhjl4upqYyxdzDGLnOP38wYq+0o1gbe9JmH8eGvPwUgaBjOd//Qj7oC9Mi00/6Y/xGv7MlgfLYK07Kx69g5XFXHPe8PVT/vPTGFS9b24qpNzutFAXp8VsdQTwZEhFXuH/1ooYoHDk9g40DOy/fnMWLGEPIYnFDSPftHQQQoBNz12EnMVAzcf3AM//OFG6BQMF2TC+FXCkLjzs0DGJmpeiM3G8FbX9QLa8wVTVXQlVbRlVZbvjPsDqWrHhwpBAq1XnHxamQ0Bd97chizlVqPAXDunO/54MvxX+++tmmxFQCcN5jH+v4cjowVcdNla3CtoBskxUBXGrpp4+BooeH15uLtL56bwIYVOdzykvOwcSAH3bIj6xdELlzVjVxKxWUbat+/XlYSAPTn01iRT8XKFEqpCs6VDPzGizbhTTs3Nn19FH//xivwd2+8Yk7nShrT8X7YiYkSjowV8bODY7BshtPutKwNrsYA+CmrI4UKVvf6dyiDXWlMzOp44tQ0Zqumd8cYpi+f9jwGy2Z46vQ0dmzsx9bV3UhrCp4RqoPHZ6uefsH73J+dLuMXRyc8wRZwCsj4XVsukK7qeAw/fXYUOzb244aLVuFru0/hh0+fhW7ZeNuLN+Gq81bgHsEw7D05hZ6MhvNX+hsnj0HzmHQjuIDOi9qSpCebatlbAJwOq7ppo2pamKkYGJ6uYKsQrunOaLhx+yp876lhlAwrID6LrOhKxzIKgBMCuX7bELIpBR++6eKW1xwHHsJ0hOcGhsH929h9/Bx2bOyHpip498svAAC85PzBhp/xm9eehx9/8PqAx8ZppDEAwMVre2OF/XpzKezY2I+/+rVLmr62Hj3ZVOQaJfOn4w0DzyKZLhvYd2YGp8+VkVIJK7szWNObRTal4Lkx1zDMVLG6x4/pDnY7bTEeOjwOIuAlF0T/h+vP+d06D40WUNIt7NjUj5Sq4OI1PXjqlO8xjBUEw+AaoZ/sH0WhauJawfCkNcVz9UWPIZd2slGePDWFGy9ahbdcswljhSr+zw8OYH1/Dldu7MeN21fjmTMzXs793pNTuHxjXyC8sG1VD3qzWkzDMI2NA7majKQk6M1pLesLgDhVzMKhEcfjC8fxf/XytRgrVB2vK6EN5k9v2o4fvP96Lw01acSst0aGIS3E8fnfyW9cswnfee91uHJTY48ho6l111+vwI3zT2/Zgb99/eUN3x8A7vy9F+M/f/dFTRvlSRYGaRgOj3sbz4NHxnFmqoy1fTkoCkFRyBGg3Rj96EwFq4X0w5XdGUwUdTx4ZByXrO2taYfB6c+nvMpnXl3MhT3ehZTH8sdnqxjqcd5nsCsNIuCHT58FAFwbMjxXu+EkUWPgxW6MATdevAo3XDSEVT3OOm+6bA2I/AyYe58dxQ+eGsaBswW8MLRZKAp5Ajcn3A6a8/SZ6bZ4C4BzjecSRxZbbx8SMpJEbty+yh9on5Bh6M2mPE+zHQy4dTIr8qmGqbCiOMxDhEQUGR5qhUahJMDxcuP8e124qlve7S9iOtow2DbDw0cm8Irtq7B1VTcePOwYBjFvfMvKLjx9egZ/9NUnMFHUA9Wig93ORK3Hj08F8tDD9AlT3PaenEJ/PuWldl62vg+FiomTk85YxMmi7onYmqpgsCuD/7+9uw+uqr7zOP7+5IEkEEiAEIiJCBg0EggPphSNOirWAqLS1o60WqV2211Gd6m72yq1O107dlpHpy22u263stWtj7NSq9tH2+jU2gcVgSIqLlh1AUEQSxCU52//OL8T7kluHpSQey/3+5rJ5J5zz735/c7Jvd/ze9619wANowZ3mm5jehiklHpHHQ92GzWklAk1QygqLODjzXUAzG2KGq5PGllObWUZS369joX3rGByXQVXtXTuKfSBMcN4edtutu/ay+NrtzL5xkd5eFVykr22d/fz2vZ3+rx9IfaNjzbxtXmT3vPr4h5Nb+850D4ZXMdpNwYOKGJmw8jE8dkuXghqYm1Ft43hcRtDUYFo7MOgHY/LyZXz5d6fvA4ML73xNtt37+P0+ipa6qt45tW3eHX7O9RWHv4COXFEOVt27uHnazbzqRkncMVpJ7Q/V1UeTZC37+ChTnfzqSoGRo3Pu/Ye4Lfr3mRyXWX7hzq+035uUxtv7d7HIYsatWNxm0a6wHNGfRXfv6KZlpS/HQ92O6dhRPvfuPqceu64opnJxx++c5x5SjVbdu7hosnHce9nZ6SdbyduZ3ho5SYW3b+S3fsOcN2y1YkZU58/Sg3PsdHDBzK6i/ER3YlHMr+2fTcPrdzE3KaatD1xLmiKeki/n3aMTBgWSpFdjXiODQiBoaFmcK/bSHqjpqKMu66azpxJNT0f7HJWXof9ePqClvrhVJQVc+fvX2XP/r3UppQYFrSMob66nHNPqU6MzgTa2wKKC9V+955OZVnUk2TRfSvZ3PYut6b0pDhpVDnFheKxtVvbqyCqUkoG1YNLeD6ksSNJfGjCyMS++AvxnJMPD5gaOKCI8zocd+15J3H6icP5cOOoLu88J9VVMKCogJt++iIVZcUsW3g6C+9+lr+9ezn/e80ZVA4c0L58Z3ddJzMhDpDffXw9+w4e4u/OPjHtcbMaR7Fk/pQuOw5km0ElRSy9spmpacYYpIpLDOm6nB6pdKPU3bElr0sMv1v/JuOqBlFTUcYHxw1rXzkqdUbMqvIS5k2t7RQU4HCxfurxQxOjjzuKi9+ta7dy/eyGRCN1SVEhV542hmUrNrL0yVfa/2Zs5JBSigrE9LHd9ySJnTm+ips/NomZp4zs9rihgwYwa2JND9URhUypq0SCJfOnMG30UG6//FS2tO3h8w+s4lDoYXVcRWmnaq5Mi6s6nn99J7MaR3HiiPK0xxUUiIun1LbfYeeCcxtGpi3hpSoLpYR0g9Sc60nelhj2HzzE06+8xUemRZO6DiktpqmuklUbdvQ4VXIsrvLprn0BDk+9fcGkGj575rhOz183u4HVm9pYtmIjQKLx7m/OHMfZJ4/odUNdaXEhl36g+xG678XiOQ28sXMPZ4cSyLTRQ/nKhY18+cdr+HbrOta83kbjUapGOhKpk+It7KK0cCxrqqvkXy+cwNwmr/Jx713eBoY/bdjB7n0HE1UILfXD31NgGFc1iC/NaeBj0+q6Pa5lfBX/fP5JLGgZm/YOvbiwgO9+cioXfudJ3ti5N7FUZX11OfXV6e92+0O6ro2XfXA0qzbs4LbWdQDMm5I1M6a3i6dxPqO+iqa6vq9OyXaFBWJBmg4FzvVG3gaGkqJCLphUk6jW+dSMMZQUFbaPeO6JJD53Vs93o0NKi7kmZb6bdKoHl/KDBdN5Yt229jVvs5Ukbpo3kbVbdrJm084uZ/jMpPKSIhbPbuixSs0515l6MxdONmtubrbly5dnOhl5adOOd/n+E3/mulkNiTUhnHPZT9KzZtac7rm8LTG4I1dbWda+sI9z7tiRO10xnHPO9QsPDM455xI8MDjnnEvwwOCccy7BA4NzzrkEDwzOOecSPDA455xL8MDgnHMuIedHPkvaBrz2Pl9eBbzZh8nJlGMhH56H7OB5yA79kYcTzCztHOo5HxiOhKTlXQ0JzyXHQj48D9nB85AdMp0Hr0pyzjmX4IHBOedcQr4Hhv/MdAL6yLGQD89DdvA8ZIeM5iGv2xicc851lu8lBueccx14YHDOOZeQt4FB0ixJL0laL+n6TKenNyQdL+lxSS9Iel7SorB/mKRfSVoXfndeqDnLSCqUtFLST8L2WElPhevxgKQBPb1HJkmqlPSgpLWSXpR0Wq5dB0nXhv+jNZLuk1SaC9dB0n9J2ippTcq+tOdekdtCflZLmpa5lB/WRR5uCf9PqyU9JKky5bnFIQ8vSfrw0U5fXgYGSYXAvwGzgQnAJyRNyGyqeuUA8E9mNgGYAVwd0n090Gpm44HWsJ3tFgEvpmzfDHzLzOqBvwCfyUiqem8J8AszawAmE+UlZ66DpFrgH4BmM5sIFALzyY3rcCcwq8O+rs79bGB8+PkccHs/pbEnd9I5D78CJppZE/B/wGKA8BmfDzSG1/x7+A47avIyMADTgfVm9mcz2wfcD1yc4TT1yMw2m9mK8Phtoi+jWqK03xUOuwuYl5kU9o6kOuAC4I6wLeBc4MFwSFbnQVIFcBawFMDM9pnZDnLsOhAt7VsmqQgYCGwmB66DmT0BvNVhd1fn/mLgvy3yR6BSUk3/pLRr6fJgZo+a2YGw+UegLjy+GLjfzPaa2SvAeqLvsKMmXwNDLbAhZXtj2JczJI0BpgJPASPNbHN4agswMkPJ6q1vA18EDoXt4cCOlA9Ftl+PscA24AehOuwOSYPIoetgZpuAW4H/JwoIbcCz5NZ1SNXVuc/Vz/pVwM/D437PQ74GhpwmqRxYBnzezHamPmdR/+Os7YMsaS6w1cyezXRajkARMA243cymArvpUG2UA9dhKNGd6FjgOGAQnas2clK2n/ueSLqBqNr4nkylIV8Dwybg+JTturAv60kqJgoK95jZj8LuN+Licfi9NVPp64UW4CJJrxJV4Z1LVF9fGao0IPuvx0Zgo5k9FbYfJAoUuXQdzgNeMbNtZrYf+BHRtcml65Cqq3OfU591SQuAucBldniQWb/nIV8DwzPA+NADYwBRw84jGU5Tj0Jd/FLgRTP7ZspTjwBXhsdXAg/3d9p6y8wWm1mdmY0hOu+PmdllwOPAJeGwbM/DFmCDpJPDrpnAC+TQdSCqQpohaWD4v4rzkDPXoYOuzv0jwBWhd9IMoC2lyimrSJpFVMV6kZm9k/LUI8B8SSWSxhI1pD99VBNjZnn5A8whavl/Gbgh0+npZZrPICoirwZWhZ85RHX0rcA64NfAsEyntZf5ORv4SXg8Lvyzrwf+ByjJdPp6SPsUYHm4Fj8GhubadQBuBNYCa4AfAiW5cB2A+4jaRfYTld4+09W5B0TUA/Fl4DmiXljZmof1RG0J8Wf7P1KOvyHk4SVg9tFOn0+J4ZxzLiFfq5Kcc851wQODc865BA8MzjnnEjwwOOecS/DA4JxzLsEDg3Pvg6SvSjqvD95nV1+kx7m+5N1VncsgSbvMrDzT6XAulZcYnAskXS7paUmrJH0vrBmxS9K3wroFrZJGhGPvlHRJePwNRWtkrJZ0a9g3RtJjYV+rpNFh/1hJf5D0nKSbOvz9L0h6JrzmxrBvkKSfSvpTWDfh0v49Ky4feWBwDpB0CnAp0GJmU4CDwGVEk8stN7NG4DfAVzq8bjjwEaDRonn04y/77wB3hX33ALeF/UuIJt+bRDTyNX6f84mmOphONKr6VElnEU1s97qZTbZo3YRf9HnmnevAA4NzkZnAqcAzklaF7XFEU4M/EI65m2haklRtwB5gqaSPAvEcN6cB94bHP0x5XQvRdAjx/tj54WclsAJoIAoUzwEfknSzpDPNrO0I8+lcj4p6PsS5vCCiO/zFiZ3Sv3Q4LtEoZ2YHJE0nCiSXANcQzRjbnXQNewK+bmbf6/REtBzlHOAmSa1m9tUe3t+5I+IlBucircAlkqqhfQ3hE4g+I/Fso58Enkx9UVgbo8LMfgZcS7TMJ8DviWaPhahK6rfh8e867I/9ErgqvB+SaiVVSzoOeMfM7gZuIZre27mjyksMzgFm9oKkLwOPSiogmvXyaqJFeKaH57YStUOkGgw8LKmU6K7/H8P+vyda4e0LRKu9fTrsXwTcK+k6Uqa0NrNHQzvHH6JZsNkFXA7UA7dIOhTStLBvc+5cZ95d1blueHdSl4+8Ksk551yClxicc84leInBOedcggcG55xzCR4YnHPOJXhgcM45l+CBwTnnXMJfAbp2aa+QaoLCAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Testing for 20 episodes ...\n",
            "Episode 1: reward: 200.000, steps: 200\n",
            "Episode 2: reward: 200.000, steps: 200\n",
            "Episode 3: reward: 200.000, steps: 200\n",
            "Episode 4: reward: 200.000, steps: 200\n",
            "Episode 5: reward: 200.000, steps: 200\n",
            "Episode 6: reward: 200.000, steps: 200\n",
            "Episode 7: reward: 200.000, steps: 200\n",
            "Episode 8: reward: 200.000, steps: 200\n",
            "Episode 9: reward: 200.000, steps: 200\n",
            "Episode 10: reward: 200.000, steps: 200\n",
            "Episode 11: reward: 200.000, steps: 200\n",
            "Episode 12: reward: 200.000, steps: 200\n",
            "Episode 13: reward: 200.000, steps: 200\n",
            "Episode 14: reward: 200.000, steps: 200\n",
            "Episode 15: reward: 200.000, steps: 200\n",
            "Episode 16: reward: 200.000, steps: 200\n",
            "Episode 17: reward: 200.000, steps: 200\n",
            "Episode 18: reward: 200.000, steps: 200\n",
            "Episode 19: reward: 200.000, steps: 200\n",
            "Episode 20: reward: 200.000, steps: 200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f2824bfdc90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    }
  ]
}