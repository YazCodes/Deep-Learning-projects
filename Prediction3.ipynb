{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Prediction3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPIMTI5mv35rl7OXPSezps3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YazCodes/Deep-Learning-projects/blob/main/Prediction3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CUKWUcft3Hg"
      },
      "source": [
        "DQN for cartpole"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQNVaN0XtuNB",
        "outputId": "ae98bd42-2e53-4260-82db-dac1dbd8b548"
      },
      "source": [
        "# install keras rl2 (we need to install keras-rl2 so it works with the tensorflow 2 version that comes pre-installed with colab)\n",
        "!pip install keras-rl2"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras-rl2 in /usr/local/lib/python3.7/dist-packages (1.0.4)\n",
            "Requirement already satisfied: tensorflow>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from keras-rl2) (2.4.1)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (0.3.3)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.19.5)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (2.4.1)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (0.36.2)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.12)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (0.10.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (2.4.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.12.1)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (3.7.4.3)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.32.0)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (2.10.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.1.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (3.3.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (0.4.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (54.1.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (1.27.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (3.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (4.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (4.7.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (3.4.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zr6MBOzWuFJY",
        "outputId": "0661446d-e197-48b3-c075-8798094585ce"
      },
      "source": [
        "!pip install gym"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.19.5)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCP8mBGnuJ1E"
      },
      "source": [
        "# load the gym module\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "# import the usual Keras modules for creating deep neural networks\n",
        "from keras import Sequential\n",
        "from keras.layers import Input, Flatten, Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "ENV_NAME = 'CartPole-v0'\n",
        "env = gym.make(ENV_NAME)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhQ1EbZLwhWD",
        "outputId": "36b2cf90-80ea-4cda-8073-d688fcc5cac6"
      },
      "source": [
        "print(env.observation_space.shape) #gives us a tupel. 0 is the first element of the tupel"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUIQR1wtxnwL",
        "outputId": "2a7a65d4-44cf-4a08-faa8-beeaefa45504"
      },
      "source": [
        "#number of actions \n",
        "print(env.action_space.n)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dEJwofrluTC2",
        "outputId": "fa79488c-46db-4e7d-ba79-9e45ed6281f2"
      },
      "source": [
        "import rl\n",
        "from rl.memory import SequentialMemory  # import the exerience replay buffer module\n",
        "from rl.policy import BoltzmannQPolicy, LinearAnnealedPolicy, EpsGreedyQPolicy  # import the policy\n",
        "from rl.agents.dqn import DQNAgent      # import the DQN agent\n",
        "\n",
        "memory = SequentialMemory(limit=10000, window_length=1) #setting up the experince replay buffer\n",
        "#limit = the numer of steps of episodes stored in the replay buffer\n",
        "\n",
        "# define the policy (how we select the actions)\n",
        "# setup the Linear annealed policy with the BoltzmannQPolicy as the inner policy\n",
        "policy =  LinearAnnealedPolicy(inner_policy=EpsGreedyQPolicy(),   # policy used to select actions\n",
        "                               attr='eps',                        # attribute in the inner policy to vary             \n",
        "                               value_max=1,                       # maximum value of attribute that is varying\n",
        "                               value_min=.1,                      # minimum value of attribute that is varying\n",
        "                               value_test=.05,                    # test if the value selected is < 0.05\n",
        "                               nb_steps=10000)                    # the number of steps between value_max and value_min\n",
        "#BoltzmannQPolicy has a paramaeter tau the higher the value of tau more exploaration will be down. Lower value = less exploration\n",
        "#default value of tau is one -\n",
        "#need to change the value_max and value_min. \n",
        "# Q-Network\n",
        "model = Sequential() #sequnetial model \n",
        "model.add(Input(shape=(1,env.observation_space.shape[0]))) # 1 = one observation and env.observation_space.shape is the number of states within our observation. 0 = the first element of the tupel\n",
        "model.add(Flatten())\n",
        "# extra layers here\n",
        "model.add(Dense(16, activation='relu')) #one layer network\n",
        "model.add(Dense(env.action_space.n, activation='linear'))   # the output is the number of actions in the action space. Activation has to be linear due to how the q value does its calculation\n",
        "print(model.summary())\n",
        "\n",
        "# define the agent using the DQNAgent class\n",
        "dqn = DQNAgent(model=model,                     # Q-Network model created above ^\n",
        "               nb_actions=env.action_space.n,   # number of actions used above - the data from the enviroment\n",
        "               memory=memory,                   # experience replay memory\n",
        "               nb_steps_warmup=10,              # how many steps are waited before starting experience replay\n",
        "               target_model_update=1e-2,        # how often the target network is updated\n",
        "               policy=policy)                   # the action selection policy\n",
        "\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "\n",
        "history = dqn.fit(env, nb_steps=10000, visualize=False, verbose=2) #visualize false to save time\n",
        "\n",
        "# summarize the history for number  of episode steps\n",
        "plt.plot(history.history['nb_episode_steps'])\n",
        "plt.ylabel('nb_episode_steps')\n",
        "plt.xlabel('episodes')\n",
        "plt.show()\n",
        "dqn.test(env, nb_episodes=20, visualize=False) #testing for 20 episodes, reward should all be 200- evaluating my algortithm \n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_2 (Flatten)          (None, 4)                 0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 16)                80        \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 2)                 34        \n",
            "=================================================================\n",
            "Total params: 114\n",
            "Trainable params: 114\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Training for 10000 steps ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n",
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:40: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "   16/10000: episode: 1, duration: 0.659s, episode steps:  16, steps per second:  24, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 0.471019, mae: 0.596157, mean_q: -0.182515, mean_eps: 0.998830\n",
            "   29/10000: episode: 2, duration: 0.088s, episode steps:  13, steps per second: 147, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 0.443210, mae: 0.554835, mean_q: -0.123217, mean_eps: 0.998020\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:40: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:40: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "   47/10000: episode: 3, duration: 0.125s, episode steps:  18, steps per second: 144, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  loss: 0.396315, mae: 0.501150, mean_q: 0.048796, mean_eps: 0.996625\n",
            "   65/10000: episode: 4, duration: 0.130s, episode steps:  18, steps per second: 139, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 0.348840, mae: 0.498110, mean_q: 0.236352, mean_eps: 0.995005\n",
            "   96/10000: episode: 5, duration: 0.218s, episode steps:  31, steps per second: 142, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.613 [0.000, 1.000],  loss: 0.291153, mae: 0.509336, mean_q: 0.350248, mean_eps: 0.992800\n",
            "  106/10000: episode: 6, duration: 0.075s, episode steps:  10, steps per second: 133, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.255768, mae: 0.532700, mean_q: 0.444271, mean_eps: 0.990955\n",
            "  170/10000: episode: 7, duration: 0.423s, episode steps:  64, steps per second: 151, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.422 [0.000, 1.000],  loss: 0.204565, mae: 0.605330, mean_q: 0.682243, mean_eps: 0.987625\n",
            "  184/10000: episode: 8, duration: 0.096s, episode steps:  14, steps per second: 146, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 0.186646, mae: 0.750025, mean_q: 1.030666, mean_eps: 0.984115\n",
            "  208/10000: episode: 9, duration: 0.159s, episode steps:  24, steps per second: 151, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 0.169660, mae: 0.819352, mean_q: 1.210769, mean_eps: 0.982405\n",
            "  220/10000: episode: 10, duration: 0.083s, episode steps:  12, steps per second: 144, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.152103, mae: 0.884225, mean_q: 1.370187, mean_eps: 0.980785\n",
            "  233/10000: episode: 11, duration: 0.095s, episode steps:  13, steps per second: 137, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 0.146845, mae: 0.921023, mean_q: 1.450544, mean_eps: 0.979660\n",
            "  245/10000: episode: 12, duration: 0.085s, episode steps:  12, steps per second: 142, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.155451, mae: 0.994431, mean_q: 1.610450, mean_eps: 0.978535\n",
            "  282/10000: episode: 13, duration: 0.250s, episode steps:  37, steps per second: 148, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 0.148083, mae: 1.055428, mean_q: 1.749572, mean_eps: 0.976330\n",
            "  292/10000: episode: 14, duration: 0.072s, episode steps:  10, steps per second: 139, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.148209, mae: 1.134070, mean_q: 1.937825, mean_eps: 0.974215\n",
            "  308/10000: episode: 15, duration: 0.106s, episode steps:  16, steps per second: 151, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 0.205950, mae: 1.210517, mean_q: 2.059636, mean_eps: 0.973045\n",
            "  323/10000: episode: 16, duration: 0.102s, episode steps:  15, steps per second: 146, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.181786, mae: 1.254748, mean_q: 2.131088, mean_eps: 0.971650\n",
            "  368/10000: episode: 17, duration: 0.293s, episode steps:  45, steps per second: 153, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 0.173331, mae: 1.350870, mean_q: 2.372160, mean_eps: 0.968950\n",
            "  394/10000: episode: 18, duration: 0.178s, episode steps:  26, steps per second: 146, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.198611, mae: 1.479142, mean_q: 2.644393, mean_eps: 0.965755\n",
            "  409/10000: episode: 19, duration: 0.109s, episode steps:  15, steps per second: 138, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.143285, mae: 1.538083, mean_q: 2.838123, mean_eps: 0.963910\n",
            "  432/10000: episode: 20, duration: 0.158s, episode steps:  23, steps per second: 146, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.435 [0.000, 1.000],  loss: 0.237612, mae: 1.667662, mean_q: 3.048106, mean_eps: 0.962200\n",
            "  477/10000: episode: 21, duration: 0.285s, episode steps:  45, steps per second: 158, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 0.296485, mae: 1.787404, mean_q: 3.233772, mean_eps: 0.959140\n",
            "  488/10000: episode: 22, duration: 0.074s, episode steps:  11, steps per second: 149, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.334888, mae: 1.922631, mean_q: 3.460004, mean_eps: 0.956620\n",
            "  516/10000: episode: 23, duration: 0.209s, episode steps:  28, steps per second: 134, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 0.296002, mae: 1.969387, mean_q: 3.580573, mean_eps: 0.954865\n",
            "  529/10000: episode: 24, duration: 0.095s, episode steps:  13, steps per second: 137, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.301598, mae: 2.045417, mean_q: 3.783531, mean_eps: 0.953020\n",
            "  554/10000: episode: 25, duration: 0.174s, episode steps:  25, steps per second: 144, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.361692, mae: 2.150215, mean_q: 3.986610, mean_eps: 0.951310\n",
            "  619/10000: episode: 26, duration: 0.422s, episode steps:  65, steps per second: 154, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 0.327134, mae: 2.294858, mean_q: 4.270854, mean_eps: 0.947260\n",
            "  638/10000: episode: 27, duration: 0.119s, episode steps:  19, steps per second: 159, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 0.434411, mae: 2.501395, mean_q: 4.668276, mean_eps: 0.943480\n",
            "  652/10000: episode: 28, duration: 0.098s, episode steps:  14, steps per second: 143, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.214 [0.000, 1.000],  loss: 0.381202, mae: 2.543410, mean_q: 4.766113, mean_eps: 0.941995\n",
            "  695/10000: episode: 29, duration: 0.284s, episode steps:  43, steps per second: 151, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.442 [0.000, 1.000],  loss: 0.532244, mae: 2.690713, mean_q: 5.061838, mean_eps: 0.939430\n",
            "  719/10000: episode: 30, duration: 0.162s, episode steps:  24, steps per second: 148, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 0.587893, mae: 2.811543, mean_q: 5.245948, mean_eps: 0.936415\n",
            "  737/10000: episode: 31, duration: 0.121s, episode steps:  18, steps per second: 149, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.650599, mae: 2.904046, mean_q: 5.392935, mean_eps: 0.934525\n",
            "  754/10000: episode: 32, duration: 0.114s, episode steps:  17, steps per second: 149, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.706 [0.000, 1.000],  loss: 0.525015, mae: 2.934274, mean_q: 5.459347, mean_eps: 0.932950\n",
            "  767/10000: episode: 33, duration: 0.093s, episode steps:  13, steps per second: 140, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.722353, mae: 3.031914, mean_q: 5.629190, mean_eps: 0.931600\n",
            "  776/10000: episode: 34, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.582336, mae: 3.052244, mean_q: 5.685893, mean_eps: 0.930610\n",
            "  849/10000: episode: 35, duration: 0.473s, episode steps:  73, steps per second: 154, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 0.602244, mae: 3.206238, mean_q: 6.016091, mean_eps: 0.926920\n",
            "  872/10000: episode: 36, duration: 0.143s, episode steps:  23, steps per second: 161, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.391 [0.000, 1.000],  loss: 0.634645, mae: 3.382574, mean_q: 6.379919, mean_eps: 0.922600\n",
            "  885/10000: episode: 37, duration: 0.093s, episode steps:  13, steps per second: 140, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.599659, mae: 3.436242, mean_q: 6.526561, mean_eps: 0.920980\n",
            "  897/10000: episode: 38, duration: 0.083s, episode steps:  12, steps per second: 145, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.750714, mae: 3.512490, mean_q: 6.573895, mean_eps: 0.919855\n",
            "  908/10000: episode: 39, duration: 0.078s, episode steps:  11, steps per second: 142, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.588530, mae: 3.515377, mean_q: 6.644282, mean_eps: 0.918820\n",
            "  921/10000: episode: 40, duration: 0.090s, episode steps:  13, steps per second: 144, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 0.964769, mae: 3.623603, mean_q: 6.746919, mean_eps: 0.917740\n",
            "  957/10000: episode: 41, duration: 0.233s, episode steps:  36, steps per second: 154, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 0.687330, mae: 3.672262, mean_q: 6.945774, mean_eps: 0.915535\n",
            "  982/10000: episode: 42, duration: 0.164s, episode steps:  25, steps per second: 153, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.360 [0.000, 1.000],  loss: 0.980243, mae: 3.819727, mean_q: 7.172836, mean_eps: 0.912790\n",
            "  995/10000: episode: 43, duration: 0.091s, episode steps:  13, steps per second: 143, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 0.997686, mae: 3.896815, mean_q: 7.204174, mean_eps: 0.911080\n",
            " 1007/10000: episode: 44, duration: 0.081s, episode steps:  12, steps per second: 148, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 0.562314, mae: 3.857929, mean_q: 7.294855, mean_eps: 0.909955\n",
            " 1038/10000: episode: 45, duration: 0.212s, episode steps:  31, steps per second: 146, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.419 [0.000, 1.000],  loss: 0.921969, mae: 4.002469, mean_q: 7.566311, mean_eps: 0.908020\n",
            " 1067/10000: episode: 46, duration: 0.191s, episode steps:  29, steps per second: 152, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 0.915842, mae: 4.106281, mean_q: 7.731458, mean_eps: 0.905320\n",
            " 1083/10000: episode: 47, duration: 0.113s, episode steps:  16, steps per second: 141, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 0.987930, mae: 4.158661, mean_q: 7.841131, mean_eps: 0.903295\n",
            " 1096/10000: episode: 48, duration: 0.091s, episode steps:  13, steps per second: 143, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 1.052435, mae: 4.229185, mean_q: 7.970276, mean_eps: 0.901990\n",
            " 1111/10000: episode: 49, duration: 0.102s, episode steps:  15, steps per second: 148, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 1.381157, mae: 4.327417, mean_q: 8.032679, mean_eps: 0.900730\n",
            " 1124/10000: episode: 50, duration: 0.092s, episode steps:  13, steps per second: 141, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.477999, mae: 4.256500, mean_q: 8.089574, mean_eps: 0.899470\n",
            " 1155/10000: episode: 51, duration: 0.214s, episode steps:  31, steps per second: 145, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.387 [0.000, 1.000],  loss: 1.055125, mae: 4.398217, mean_q: 8.274719, mean_eps: 0.897490\n",
            " 1167/10000: episode: 52, duration: 0.080s, episode steps:  12, steps per second: 151, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 1.303586, mae: 4.496932, mean_q: 8.404998, mean_eps: 0.895555\n",
            " 1179/10000: episode: 53, duration: 0.080s, episode steps:  12, steps per second: 151, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 1.429626, mae: 4.543064, mean_q: 8.474363, mean_eps: 0.894475\n",
            " 1206/10000: episode: 54, duration: 0.182s, episode steps:  27, steps per second: 149, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.370 [0.000, 1.000],  loss: 1.427894, mae: 4.605394, mean_q: 8.590096, mean_eps: 0.892720\n",
            " 1220/10000: episode: 55, duration: 0.095s, episode steps:  14, steps per second: 148, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.827059, mae: 4.601742, mean_q: 8.723452, mean_eps: 0.890875\n",
            " 1234/10000: episode: 56, duration: 0.091s, episode steps:  14, steps per second: 153, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 0.717957, mae: 4.617414, mean_q: 8.907530, mean_eps: 0.889615\n",
            " 1243/10000: episode: 57, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 1.123569, mae: 4.723773, mean_q: 9.094666, mean_eps: 0.888580\n",
            " 1278/10000: episode: 58, duration: 0.238s, episode steps:  35, steps per second: 147, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 1.290414, mae: 4.811365, mean_q: 9.049772, mean_eps: 0.886600\n",
            " 1306/10000: episode: 59, duration: 0.188s, episode steps:  28, steps per second: 149, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 1.465244, mae: 4.904844, mean_q: 9.157908, mean_eps: 0.883765\n",
            " 1334/10000: episode: 60, duration: 0.191s, episode steps:  28, steps per second: 146, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 1.182631, mae: 4.957857, mean_q: 9.310753, mean_eps: 0.881245\n",
            " 1345/10000: episode: 61, duration: 0.075s, episode steps:  11, steps per second: 147, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 1.237416, mae: 5.029648, mean_q: 9.504758, mean_eps: 0.879490\n",
            " 1369/10000: episode: 62, duration: 0.168s, episode steps:  24, steps per second: 143, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.599490, mae: 5.094330, mean_q: 9.453084, mean_eps: 0.877915\n",
            " 1401/10000: episode: 63, duration: 0.222s, episode steps:  32, steps per second: 144, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 1.667301, mae: 5.155748, mean_q: 9.573746, mean_eps: 0.875395\n",
            " 1418/10000: episode: 64, duration: 0.117s, episode steps:  17, steps per second: 145, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 1.201648, mae: 5.164745, mean_q: 9.736715, mean_eps: 0.873190\n",
            " 1431/10000: episode: 65, duration: 0.102s, episode steps:  13, steps per second: 127, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 1.860328, mae: 5.284280, mean_q: 9.866488, mean_eps: 0.871840\n",
            " 1456/10000: episode: 66, duration: 0.172s, episode steps:  25, steps per second: 145, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.640 [0.000, 1.000],  loss: 1.633903, mae: 5.329824, mean_q: 9.979902, mean_eps: 0.870130\n",
            " 1473/10000: episode: 67, duration: 0.123s, episode steps:  17, steps per second: 138, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.294 [0.000, 1.000],  loss: 1.382605, mae: 5.320150, mean_q: 9.948462, mean_eps: 0.868240\n",
            " 1498/10000: episode: 68, duration: 0.164s, episode steps:  25, steps per second: 152, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 1.526905, mae: 5.385195, mean_q: 10.112113, mean_eps: 0.866350\n",
            " 1528/10000: episode: 69, duration: 0.191s, episode steps:  30, steps per second: 157, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 1.472513, mae: 5.462231, mean_q: 10.327833, mean_eps: 0.863875\n",
            " 1576/10000: episode: 70, duration: 0.332s, episode steps:  48, steps per second: 145, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.310055, mae: 5.538594, mean_q: 10.541590, mean_eps: 0.860365\n",
            " 1589/10000: episode: 71, duration: 0.088s, episode steps:  13, steps per second: 147, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 1.681443, mae: 5.666866, mean_q: 10.756889, mean_eps: 0.857620\n",
            " 1612/10000: episode: 72, duration: 0.153s, episode steps:  23, steps per second: 150, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 1.598823, mae: 5.718889, mean_q: 10.798937, mean_eps: 0.856000\n",
            " 1625/10000: episode: 73, duration: 0.085s, episode steps:  13, steps per second: 153, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.885389, mae: 5.700528, mean_q: 10.992665, mean_eps: 0.854380\n",
            " 1646/10000: episode: 74, duration: 0.140s, episode steps:  21, steps per second: 150, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 1.923111, mae: 5.832408, mean_q: 11.028313, mean_eps: 0.852850\n",
            " 1660/10000: episode: 75, duration: 0.096s, episode steps:  14, steps per second: 146, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 1.407393, mae: 5.818235, mean_q: 11.070112, mean_eps: 0.851275\n",
            " 1701/10000: episode: 76, duration: 0.266s, episode steps:  41, steps per second: 154, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 1.759551, mae: 5.943976, mean_q: 11.261648, mean_eps: 0.848800\n",
            " 1733/10000: episode: 77, duration: 0.207s, episode steps:  32, steps per second: 155, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 1.551500, mae: 6.012760, mean_q: 11.464415, mean_eps: 0.845515\n",
            " 1748/10000: episode: 78, duration: 0.104s, episode steps:  15, steps per second: 145, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 1.485397, mae: 6.065703, mean_q: 11.644084, mean_eps: 0.843400\n",
            " 1762/10000: episode: 79, duration: 0.097s, episode steps:  14, steps per second: 144, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 2.572407, mae: 6.189821, mean_q: 11.628988, mean_eps: 0.842095\n",
            " 1777/10000: episode: 80, duration: 0.103s, episode steps:  15, steps per second: 145, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 1.503920, mae: 6.184005, mean_q: 11.750124, mean_eps: 0.840790\n",
            " 1815/10000: episode: 81, duration: 0.245s, episode steps:  38, steps per second: 155, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.248754, mae: 6.275896, mean_q: 11.836827, mean_eps: 0.838405\n",
            " 1833/10000: episode: 82, duration: 0.125s, episode steps:  18, steps per second: 145, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 1.868924, mae: 6.289730, mean_q: 11.892365, mean_eps: 0.835885\n",
            " 1847/10000: episode: 83, duration: 0.104s, episode steps:  14, steps per second: 135, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.431329, mae: 6.286029, mean_q: 12.067684, mean_eps: 0.834445\n",
            " 1869/10000: episode: 84, duration: 0.145s, episode steps:  22, steps per second: 152, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 1.851425, mae: 6.400226, mean_q: 12.284319, mean_eps: 0.832825\n",
            " 1904/10000: episode: 85, duration: 0.237s, episode steps:  35, steps per second: 148, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 2.214223, mae: 6.500223, mean_q: 12.310558, mean_eps: 0.830260\n",
            " 1948/10000: episode: 86, duration: 0.292s, episode steps:  44, steps per second: 151, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 1.745229, mae: 6.548776, mean_q: 12.515659, mean_eps: 0.826705\n",
            " 1967/10000: episode: 87, duration: 0.125s, episode steps:  19, steps per second: 152, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  loss: 2.058628, mae: 6.668548, mean_q: 12.685293, mean_eps: 0.823870\n",
            " 2050/10000: episode: 88, duration: 0.549s, episode steps:  83, steps per second: 151, episode reward: 83.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 2.156824, mae: 6.782795, mean_q: 12.890866, mean_eps: 0.819280\n",
            " 2083/10000: episode: 89, duration: 0.218s, episode steps:  33, steps per second: 151, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 3.059975, mae: 6.972895, mean_q: 13.042085, mean_eps: 0.814060\n",
            " 2135/10000: episode: 90, duration: 0.339s, episode steps:  52, steps per second: 153, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 2.236896, mae: 6.994040, mean_q: 13.348897, mean_eps: 0.810235\n",
            " 2146/10000: episode: 91, duration: 0.081s, episode steps:  11, steps per second: 136, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 2.100964, mae: 7.059421, mean_q: 13.544905, mean_eps: 0.807400\n",
            " 2186/10000: episode: 92, duration: 0.271s, episode steps:  40, steps per second: 147, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.036181, mae: 7.127682, mean_q: 13.684366, mean_eps: 0.805105\n",
            " 2223/10000: episode: 93, duration: 0.245s, episode steps:  37, steps per second: 151, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.568 [0.000, 1.000],  loss: 2.844387, mae: 7.259253, mean_q: 13.784012, mean_eps: 0.801640\n",
            " 2270/10000: episode: 94, duration: 0.301s, episode steps:  47, steps per second: 156, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 2.884968, mae: 7.366944, mean_q: 13.947208, mean_eps: 0.797860\n",
            " 2279/10000: episode: 95, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 1.790979, mae: 7.328097, mean_q: 14.037700, mean_eps: 0.795340\n",
            " 2332/10000: episode: 96, duration: 0.357s, episode steps:  53, steps per second: 148, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 2.342254, mae: 7.452906, mean_q: 14.296936, mean_eps: 0.792550\n",
            " 2359/10000: episode: 97, duration: 0.192s, episode steps:  27, steps per second: 140, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 2.550829, mae: 7.570509, mean_q: 14.468303, mean_eps: 0.788950\n",
            " 2373/10000: episode: 98, duration: 0.098s, episode steps:  14, steps per second: 142, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.087832, mae: 7.599901, mean_q: 14.584261, mean_eps: 0.787105\n",
            " 2426/10000: episode: 99, duration: 0.338s, episode steps:  53, steps per second: 157, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 2.761066, mae: 7.688449, mean_q: 14.691833, mean_eps: 0.784090\n",
            " 2438/10000: episode: 100, duration: 0.083s, episode steps:  12, steps per second: 144, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 3.590202, mae: 7.852226, mean_q: 14.772003, mean_eps: 0.781165\n",
            " 2467/10000: episode: 101, duration: 0.193s, episode steps:  29, steps per second: 150, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  loss: 3.195037, mae: 7.825835, mean_q: 14.877189, mean_eps: 0.779320\n",
            " 2493/10000: episode: 102, duration: 0.175s, episode steps:  26, steps per second: 148, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 3.230577, mae: 7.873177, mean_q: 14.880198, mean_eps: 0.776845\n",
            " 2522/10000: episode: 103, duration: 0.192s, episode steps:  29, steps per second: 151, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.586 [0.000, 1.000],  loss: 2.545937, mae: 7.885954, mean_q: 15.075159, mean_eps: 0.774370\n",
            " 2562/10000: episode: 104, duration: 0.253s, episode steps:  40, steps per second: 158, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.989432, mae: 8.000610, mean_q: 15.289883, mean_eps: 0.771265\n",
            " 2588/10000: episode: 105, duration: 0.171s, episode steps:  26, steps per second: 152, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.911792, mae: 8.058772, mean_q: 15.478321, mean_eps: 0.768295\n",
            " 2635/10000: episode: 106, duration: 0.324s, episode steps:  47, steps per second: 145, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 2.669327, mae: 8.114883, mean_q: 15.616854, mean_eps: 0.765010\n",
            " 2660/10000: episode: 107, duration: 0.162s, episode steps:  25, steps per second: 155, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 2.912224, mae: 8.212318, mean_q: 15.883670, mean_eps: 0.761770\n",
            " 2706/10000: episode: 108, duration: 0.302s, episode steps:  46, steps per second: 153, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 3.017766, mae: 8.337329, mean_q: 16.065753, mean_eps: 0.758575\n",
            " 2725/10000: episode: 109, duration: 0.119s, episode steps:  19, steps per second: 159, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 2.896895, mae: 8.345179, mean_q: 16.084631, mean_eps: 0.755650\n",
            " 2745/10000: episode: 110, duration: 0.133s, episode steps:  20, steps per second: 150, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 2.989977, mae: 8.467022, mean_q: 16.330975, mean_eps: 0.753895\n",
            " 2778/10000: episode: 111, duration: 0.217s, episode steps:  33, steps per second: 152, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 2.207418, mae: 8.516569, mean_q: 16.548094, mean_eps: 0.751510\n",
            " 2803/10000: episode: 112, duration: 0.175s, episode steps:  25, steps per second: 143, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 3.428415, mae: 8.646191, mean_q: 16.657778, mean_eps: 0.748900\n",
            " 2822/10000: episode: 113, duration: 0.131s, episode steps:  19, steps per second: 145, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 2.759343, mae: 8.617283, mean_q: 16.620148, mean_eps: 0.746920\n",
            " 2860/10000: episode: 114, duration: 0.239s, episode steps:  38, steps per second: 159, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 3.453368, mae: 8.710829, mean_q: 16.719296, mean_eps: 0.744355\n",
            " 2921/10000: episode: 115, duration: 0.400s, episode steps:  61, steps per second: 152, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 2.807033, mae: 8.808072, mean_q: 17.022947, mean_eps: 0.739900\n",
            " 2968/10000: episode: 116, duration: 0.312s, episode steps:  47, steps per second: 151, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 3.116648, mae: 8.960337, mean_q: 17.343358, mean_eps: 0.735040\n",
            " 3018/10000: episode: 117, duration: 0.323s, episode steps:  50, steps per second: 155, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 2.755558, mae: 9.063671, mean_q: 17.603739, mean_eps: 0.730675\n",
            " 3028/10000: episode: 118, duration: 0.068s, episode steps:  10, steps per second: 147, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 3.235114, mae: 9.159618, mean_q: 17.731004, mean_eps: 0.727975\n",
            " 3048/10000: episode: 119, duration: 0.132s, episode steps:  20, steps per second: 151, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 4.711133, mae: 9.299144, mean_q: 17.848774, mean_eps: 0.726625\n",
            " 3065/10000: episode: 120, duration: 0.118s, episode steps:  17, steps per second: 145, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  loss: 3.830591, mae: 9.273247, mean_q: 17.805769, mean_eps: 0.724960\n",
            " 3082/10000: episode: 121, duration: 0.120s, episode steps:  17, steps per second: 142, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 3.844251, mae: 9.275783, mean_q: 17.808148, mean_eps: 0.723430\n",
            " 3100/10000: episode: 122, duration: 0.139s, episode steps:  18, steps per second: 129, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 3.682282, mae: 9.278597, mean_q: 17.844758, mean_eps: 0.721855\n",
            " 3160/10000: episode: 123, duration: 0.380s, episode steps:  60, steps per second: 158, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 3.032974, mae: 9.409049, mean_q: 18.288055, mean_eps: 0.718345\n",
            " 3196/10000: episode: 124, duration: 0.231s, episode steps:  36, steps per second: 156, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 3.394654, mae: 9.548753, mean_q: 18.514905, mean_eps: 0.714025\n",
            " 3219/10000: episode: 125, duration: 0.143s, episode steps:  23, steps per second: 161, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.609 [0.000, 1.000],  loss: 3.648711, mae: 9.581687, mean_q: 18.531465, mean_eps: 0.711370\n",
            " 3302/10000: episode: 126, duration: 0.534s, episode steps:  83, steps per second: 155, episode reward: 83.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 4.258323, mae: 9.701896, mean_q: 18.740924, mean_eps: 0.706600\n",
            " 3332/10000: episode: 127, duration: 0.195s, episode steps:  30, steps per second: 154, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 4.133648, mae: 9.783558, mean_q: 18.945358, mean_eps: 0.701515\n",
            " 3360/10000: episode: 128, duration: 0.189s, episode steps:  28, steps per second: 148, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.607 [0.000, 1.000],  loss: 4.674830, mae: 9.906427, mean_q: 19.111815, mean_eps: 0.698905\n",
            " 3423/10000: episode: 129, duration: 0.422s, episode steps:  63, steps per second: 149, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 3.380056, mae: 10.012843, mean_q: 19.500323, mean_eps: 0.694810\n",
            " 3471/10000: episode: 130, duration: 0.302s, episode steps:  48, steps per second: 159, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.054414, mae: 10.175686, mean_q: 19.976006, mean_eps: 0.689815\n",
            " 3521/10000: episode: 131, duration: 0.335s, episode steps:  50, steps per second: 149, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  loss: 4.395644, mae: 10.300771, mean_q: 19.998778, mean_eps: 0.685405\n",
            " 3571/10000: episode: 132, duration: 0.338s, episode steps:  50, steps per second: 148, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  loss: 4.048407, mae: 10.409263, mean_q: 20.296859, mean_eps: 0.680905\n",
            " 3589/10000: episode: 133, duration: 0.122s, episode steps:  18, steps per second: 148, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.656110, mae: 10.622150, mean_q: 20.290981, mean_eps: 0.677845\n",
            " 3603/10000: episode: 134, duration: 0.102s, episode steps:  14, steps per second: 137, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 4.438659, mae: 10.468971, mean_q: 20.059361, mean_eps: 0.676405\n",
            " 3622/10000: episode: 135, duration: 0.126s, episode steps:  19, steps per second: 151, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 2.299729, mae: 10.456564, mean_q: 20.554295, mean_eps: 0.674920\n",
            " 3675/10000: episode: 136, duration: 0.345s, episode steps:  53, steps per second: 154, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 4.475100, mae: 10.692948, mean_q: 20.775489, mean_eps: 0.671680\n",
            " 3696/10000: episode: 137, duration: 0.137s, episode steps:  21, steps per second: 154, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 4.556111, mae: 10.786416, mean_q: 20.982107, mean_eps: 0.668350\n",
            " 3758/10000: episode: 138, duration: 0.402s, episode steps:  62, steps per second: 154, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 5.775056, mae: 10.838921, mean_q: 20.846570, mean_eps: 0.664615\n",
            " 3812/10000: episode: 139, duration: 0.342s, episode steps:  54, steps per second: 158, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 4.792285, mae: 10.859271, mean_q: 21.089500, mean_eps: 0.659395\n",
            " 3824/10000: episode: 140, duration: 0.083s, episode steps:  12, steps per second: 145, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 5.587531, mae: 11.035419, mean_q: 21.346982, mean_eps: 0.656425\n",
            " 3852/10000: episode: 141, duration: 0.184s, episode steps:  28, steps per second: 152, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 5.518928, mae: 11.031222, mean_q: 21.272376, mean_eps: 0.654625\n",
            " 3964/10000: episode: 142, duration: 0.717s, episode steps: 112, steps per second: 156, episode reward: 112.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.491964, mae: 11.170399, mean_q: 21.753876, mean_eps: 0.648325\n",
            " 3989/10000: episode: 143, duration: 0.162s, episode steps:  25, steps per second: 155, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 5.904983, mae: 11.350980, mean_q: 21.966773, mean_eps: 0.642160\n",
            " 4014/10000: episode: 144, duration: 0.167s, episode steps:  25, steps per second: 150, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 4.814376, mae: 11.402686, mean_q: 22.163968, mean_eps: 0.639910\n",
            " 4045/10000: episode: 145, duration: 0.215s, episode steps:  31, steps per second: 144, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 4.968372, mae: 11.492821, mean_q: 22.356145, mean_eps: 0.637390\n",
            " 4061/10000: episode: 146, duration: 0.104s, episode steps:  16, steps per second: 154, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 5.058833, mae: 11.511657, mean_q: 22.325026, mean_eps: 0.635275\n",
            " 4138/10000: episode: 147, duration: 0.501s, episode steps:  77, steps per second: 154, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 5.994153, mae: 11.610397, mean_q: 22.454032, mean_eps: 0.631090\n",
            " 4198/10000: episode: 148, duration: 0.388s, episode steps:  60, steps per second: 155, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 5.245045, mae: 11.658804, mean_q: 22.640147, mean_eps: 0.624925\n",
            " 4221/10000: episode: 149, duration: 0.146s, episode steps:  23, steps per second: 157, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.391 [0.000, 1.000],  loss: 4.387105, mae: 11.836681, mean_q: 23.074204, mean_eps: 0.621190\n",
            " 4310/10000: episode: 150, duration: 0.578s, episode steps:  89, steps per second: 154, episode reward: 89.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 5.329762, mae: 11.878384, mean_q: 23.098662, mean_eps: 0.616150\n",
            " 4346/10000: episode: 151, duration: 0.248s, episode steps:  36, steps per second: 145, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 5.825636, mae: 11.917719, mean_q: 23.145939, mean_eps: 0.610525\n",
            " 4380/10000: episode: 152, duration: 0.228s, episode steps:  34, steps per second: 149, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 4.226340, mae: 12.040410, mean_q: 23.629152, mean_eps: 0.607375\n",
            " 4402/10000: episode: 153, duration: 0.147s, episode steps:  22, steps per second: 149, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 4.101233, mae: 12.128483, mean_q: 23.856803, mean_eps: 0.604855\n",
            " 4512/10000: episode: 154, duration: 0.712s, episode steps: 110, steps per second: 154, episode reward: 110.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 5.536807, mae: 12.320840, mean_q: 24.013827, mean_eps: 0.598915\n",
            " 4594/10000: episode: 155, duration: 0.521s, episode steps:  82, steps per second: 157, episode reward: 82.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 4.797627, mae: 12.462210, mean_q: 24.409945, mean_eps: 0.590275\n",
            " 4659/10000: episode: 156, duration: 0.448s, episode steps:  65, steps per second: 145, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 5.690589, mae: 12.628836, mean_q: 24.671261, mean_eps: 0.583660\n",
            " 4730/10000: episode: 157, duration: 0.446s, episode steps:  71, steps per second: 159, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 4.823057, mae: 12.740655, mean_q: 24.931199, mean_eps: 0.577540\n",
            " 4778/10000: episode: 158, duration: 0.300s, episode steps:  48, steps per second: 160, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.892048, mae: 12.868718, mean_q: 25.019679, mean_eps: 0.572185\n",
            " 4818/10000: episode: 159, duration: 0.272s, episode steps:  40, steps per second: 147, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 4.644998, mae: 12.951956, mean_q: 25.374267, mean_eps: 0.568225\n",
            " 4864/10000: episode: 160, duration: 0.292s, episode steps:  46, steps per second: 157, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 5.925158, mae: 13.070372, mean_q: 25.535908, mean_eps: 0.564355\n",
            " 4907/10000: episode: 161, duration: 0.291s, episode steps:  43, steps per second: 148, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 5.846570, mae: 13.146774, mean_q: 25.644365, mean_eps: 0.560350\n",
            " 4959/10000: episode: 162, duration: 0.343s, episode steps:  52, steps per second: 151, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.374878, mae: 13.208376, mean_q: 25.695036, mean_eps: 0.556075\n",
            " 5027/10000: episode: 163, duration: 0.434s, episode steps:  68, steps per second: 157, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 6.414646, mae: 13.303253, mean_q: 25.951649, mean_eps: 0.550675\n",
            " 5093/10000: episode: 164, duration: 0.427s, episode steps:  66, steps per second: 155, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 5.421966, mae: 13.361149, mean_q: 26.208905, mean_eps: 0.544645\n",
            " 5159/10000: episode: 165, duration: 0.430s, episode steps:  66, steps per second: 153, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 5.998023, mae: 13.570136, mean_q: 26.625960, mean_eps: 0.538705\n",
            " 5292/10000: episode: 166, duration: 0.842s, episode steps: 133, steps per second: 158, episode reward: 133.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 5.892191, mae: 13.784444, mean_q: 27.062332, mean_eps: 0.529750\n",
            " 5395/10000: episode: 167, duration: 0.655s, episode steps: 103, steps per second: 157, episode reward: 103.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 6.663863, mae: 14.054202, mean_q: 27.567098, mean_eps: 0.519130\n",
            " 5461/10000: episode: 168, duration: 0.434s, episode steps:  66, steps per second: 152, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 6.641450, mae: 14.192985, mean_q: 27.874732, mean_eps: 0.511525\n",
            " 5531/10000: episode: 169, duration: 0.444s, episode steps:  70, steps per second: 158, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 6.136430, mae: 14.236868, mean_q: 28.025635, mean_eps: 0.505405\n",
            " 5592/10000: episode: 170, duration: 0.398s, episode steps:  61, steps per second: 153, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 6.978396, mae: 14.447804, mean_q: 28.380964, mean_eps: 0.499510\n",
            " 5652/10000: episode: 171, duration: 0.379s, episode steps:  60, steps per second: 158, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 5.761308, mae: 14.569559, mean_q: 28.774811, mean_eps: 0.494065\n",
            " 5736/10000: episode: 172, duration: 0.538s, episode steps:  84, steps per second: 156, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 8.933012, mae: 14.696114, mean_q: 28.674324, mean_eps: 0.487585\n",
            " 5860/10000: episode: 173, duration: 0.792s, episode steps: 124, steps per second: 157, episode reward: 124.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 6.246912, mae: 14.811330, mean_q: 29.251369, mean_eps: 0.478225\n",
            " 5930/10000: episode: 174, duration: 0.459s, episode steps:  70, steps per second: 152, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 7.338156, mae: 15.030233, mean_q: 29.598010, mean_eps: 0.469495\n",
            " 6081/10000: episode: 175, duration: 0.987s, episode steps: 151, steps per second: 153, episode reward: 151.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 7.360896, mae: 15.263897, mean_q: 30.094719, mean_eps: 0.459550\n",
            " 6219/10000: episode: 176, duration: 0.901s, episode steps: 138, steps per second: 153, episode reward: 138.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 6.820304, mae: 15.577078, mean_q: 30.776793, mean_eps: 0.446545\n",
            " 6295/10000: episode: 177, duration: 0.493s, episode steps:  76, steps per second: 154, episode reward: 76.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.539 [0.000, 1.000],  loss: 6.474642, mae: 15.763934, mean_q: 31.260305, mean_eps: 0.436915\n",
            " 6382/10000: episode: 178, duration: 0.563s, episode steps:  87, steps per second: 154, episode reward: 87.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 6.845456, mae: 15.996835, mean_q: 31.671600, mean_eps: 0.429580\n",
            " 6434/10000: episode: 179, duration: 0.331s, episode steps:  52, steps per second: 157, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 6.608476, mae: 16.080809, mean_q: 31.854889, mean_eps: 0.423325\n",
            " 6533/10000: episode: 180, duration: 0.640s, episode steps:  99, steps per second: 155, episode reward: 99.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 7.636335, mae: 16.251001, mean_q: 32.108468, mean_eps: 0.416530\n",
            " 6642/10000: episode: 181, duration: 0.714s, episode steps: 109, steps per second: 153, episode reward: 109.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 6.489078, mae: 16.482082, mean_q: 32.724041, mean_eps: 0.407170\n",
            " 6704/10000: episode: 182, duration: 0.409s, episode steps:  62, steps per second: 152, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.657269, mae: 16.679267, mean_q: 32.919420, mean_eps: 0.399475\n",
            " 6831/10000: episode: 183, duration: 0.808s, episode steps: 127, steps per second: 157, episode reward: 127.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 7.574139, mae: 16.774809, mean_q: 33.270531, mean_eps: 0.390970\n",
            " 7018/10000: episode: 184, duration: 1.206s, episode steps: 187, steps per second: 155, episode reward: 187.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 7.979166, mae: 17.086379, mean_q: 33.859314, mean_eps: 0.376840\n",
            " 7117/10000: episode: 185, duration: 0.631s, episode steps:  99, steps per second: 157, episode reward: 99.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 8.256977, mae: 17.393061, mean_q: 34.458807, mean_eps: 0.363970\n",
            " 7206/10000: episode: 186, duration: 0.576s, episode steps:  89, steps per second: 155, episode reward: 89.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 8.418201, mae: 17.554033, mean_q: 34.804903, mean_eps: 0.355510\n",
            " 7343/10000: episode: 187, duration: 0.869s, episode steps: 137, steps per second: 158, episode reward: 137.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 7.676091, mae: 17.770761, mean_q: 35.334722, mean_eps: 0.345340\n",
            " 7543/10000: episode: 188, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 7.685359, mae: 18.144448, mean_q: 36.124880, mean_eps: 0.330175\n",
            " 7638/10000: episode: 189, duration: 0.612s, episode steps:  95, steps per second: 155, episode reward: 95.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 8.376502, mae: 18.400264, mean_q: 36.605712, mean_eps: 0.316900\n",
            " 7708/10000: episode: 190, duration: 0.446s, episode steps:  70, steps per second: 157, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 10.530629, mae: 18.638632, mean_q: 36.894178, mean_eps: 0.309475\n",
            " 7844/10000: episode: 191, duration: 0.893s, episode steps: 136, steps per second: 152, episode reward: 136.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 8.200853, mae: 18.783145, mean_q: 37.375381, mean_eps: 0.300205\n",
            " 7946/10000: episode: 192, duration: 0.649s, episode steps: 102, steps per second: 157, episode reward: 102.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.539 [0.000, 1.000],  loss: 8.993882, mae: 18.932152, mean_q: 37.642313, mean_eps: 0.289495\n",
            " 8050/10000: episode: 193, duration: 0.675s, episode steps: 104, steps per second: 154, episode reward: 104.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  loss: 8.001803, mae: 19.065665, mean_q: 38.011819, mean_eps: 0.280225\n",
            " 8250/10000: episode: 194, duration: 1.294s, episode steps: 200, steps per second: 155, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 8.149727, mae: 19.311772, mean_q: 38.516001, mean_eps: 0.266545\n",
            " 8343/10000: episode: 195, duration: 0.608s, episode steps:  93, steps per second: 153, episode reward: 93.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  loss: 7.633474, mae: 19.671076, mean_q: 39.323955, mean_eps: 0.253360\n",
            " 8543/10000: episode: 196, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 7.925452, mae: 19.908752, mean_q: 39.804753, mean_eps: 0.240175\n",
            " 8662/10000: episode: 197, duration: 0.765s, episode steps: 119, steps per second: 156, episode reward: 119.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 8.099557, mae: 20.229912, mean_q: 40.480312, mean_eps: 0.225820\n",
            " 8850/10000: episode: 198, duration: 1.198s, episode steps: 188, steps per second: 157, episode reward: 188.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 9.268274, mae: 20.563684, mean_q: 41.107075, mean_eps: 0.212005\n",
            " 8974/10000: episode: 199, duration: 0.807s, episode steps: 124, steps per second: 154, episode reward: 124.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  loss: 10.107120, mae: 20.923333, mean_q: 41.803804, mean_eps: 0.197965\n",
            " 9174/10000: episode: 200, duration: 1.304s, episode steps: 200, steps per second: 153, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.457679, mae: 21.141702, mean_q: 42.361567, mean_eps: 0.183385\n",
            " 9326/10000: episode: 201, duration: 0.999s, episode steps: 152, steps per second: 152, episode reward: 152.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 10.089575, mae: 21.497260, mean_q: 43.043535, mean_eps: 0.167545\n",
            " 9493/10000: episode: 202, duration: 1.117s, episode steps: 167, steps per second: 149, episode reward: 167.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 9.357002, mae: 21.757866, mean_q: 43.571893, mean_eps: 0.153190\n",
            " 9635/10000: episode: 203, duration: 0.923s, episode steps: 142, steps per second: 154, episode reward: 142.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 10.659602, mae: 22.026585, mean_q: 44.021717, mean_eps: 0.139285\n",
            " 9835/10000: episode: 204, duration: 1.336s, episode steps: 200, steps per second: 150, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.222831, mae: 22.294040, mean_q: 44.671416, mean_eps: 0.123895\n",
            "done, took 66.303 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Testing for 20 episodes ...\n",
            "Episode 1: reward: 184.000, steps: 184\n",
            "Episode 2: reward: 200.000, steps: 200\n",
            "Episode 3: reward: 151.000, steps: 151\n",
            "Episode 4: reward: 200.000, steps: 200\n",
            "Episode 5: reward: 169.000, steps: 169\n",
            "Episode 6: reward: 200.000, steps: 200\n",
            "Episode 7: reward: 144.000, steps: 144\n",
            "Episode 8: reward: 200.000, steps: 200\n",
            "Episode 9: reward: 155.000, steps: 155\n",
            "Episode 10: reward: 154.000, steps: 154\n",
            "Episode 11: reward: 158.000, steps: 158\n",
            "Episode 12: reward: 191.000, steps: 191\n",
            "Episode 13: reward: 168.000, steps: 168\n",
            "Episode 14: reward: 164.000, steps: 164\n",
            "Episode 15: reward: 136.000, steps: 136\n",
            "Episode 16: reward: 158.000, steps: 158\n",
            "Episode 17: reward: 145.000, steps: 145\n",
            "Episode 18: reward: 162.000, steps: 162\n",
            "Episode 19: reward: 200.000, steps: 200\n",
            "Episode 20: reward: 151.000, steps: 151\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f52ad7fbdd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    }
  ]
}