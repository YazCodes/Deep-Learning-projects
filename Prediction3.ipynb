{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Prediction3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPvwrqi9In0pj1VHnFUX5dt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YazCodes/Deep-Learning-projects/blob/main/Prediction3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CUKWUcft3Hg"
      },
      "source": [
        "DQN for cartpole"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQNVaN0XtuNB",
        "outputId": "a0dddcd4-7e11-4081-97ed-26791a02eac8"
      },
      "source": [
        "# install keras rl2 (we need to install keras-rl2 so it works with the tensorflow 2 version that comes pre-installed with colab)\n",
        "!pip install keras-rl2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-rl2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/34/94ffeab44eef43e22a01d82aa0ca062a97392c2c2415ba8b210e72053285/keras_rl2-1.0.4-py3-none-any.whl (53kB)\n",
            "\r\u001b[K     |██████▏                         | 10kB 15.2MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 20kB 18.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 30kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 40kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 51kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 3.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from keras-rl2) (2.4.1)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (0.3.3)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (2.10.0)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (2.4.1)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.32.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (2.4.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.12)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (3.7.4.3)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (3.12.4)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.12.1)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.1.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (0.36.2)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.19.5)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (0.10.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (54.1.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (0.4.3)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (1.27.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (1.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (2.10)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (3.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (1.3.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (4.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (4.7.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (3.4.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (0.4.8)\n",
            "Installing collected packages: keras-rl2\n",
            "Successfully installed keras-rl2-1.0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zr6MBOzWuFJY",
        "outputId": "0661446d-e197-48b3-c075-8798094585ce"
      },
      "source": [
        "!pip install gym"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.19.5)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCP8mBGnuJ1E"
      },
      "source": [
        "# load the gym module\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "# import the usual Keras modules for creating deep neural networks\n",
        "from keras import Sequential\n",
        "from keras.layers import Input, Flatten, Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "ENV_NAME = 'CartPole-v0'\n",
        "env = gym.make(ENV_NAME)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhQ1EbZLwhWD",
        "outputId": "36b2cf90-80ea-4cda-8073-d688fcc5cac6"
      },
      "source": [
        "print(env.observation_space.shape) #gives us a tupel. 0 is the first element of the tupel"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUIQR1wtxnwL",
        "outputId": "2a7a65d4-44cf-4a08-faa8-beeaefa45504"
      },
      "source": [
        "#number of actions \n",
        "print(env.action_space.n)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dEJwofrluTC2",
        "outputId": "b75b412a-3be2-4f82-8072-786639f9d989"
      },
      "source": [
        "import rl\n",
        "from rl.memory import SequentialMemory  # import the exerience replay buffer module\n",
        "from rl.policy import BoltzmannQPolicy, LinearAnnealedPolicy  # import the policy\n",
        "from rl.agents.dqn import DQNAgent      # import the DQN agent\n",
        "\n",
        "memory = SequentialMemory(limit=10000, window_length=1) #setting up the experince replay buffer\n",
        "#limit = the numer of steps of episodes stored in the replay buffer\n",
        "\n",
        "# define the policy (how we select the actions)\n",
        "# setup the Linear annealed policy with the BoltzmannQPolicy as the inner policy\n",
        "policy =  LinearAnnealedPolicy(inner_policy=BoltzmannQPolicy(),   # policy used to select actions\n",
        "                               attr='tau',                        # attribute in the inner policy to vary             \n",
        "                               value_max=1,                       # maximum value of attribute that is varying\n",
        "                               value_min=.1,                      # minimum value of attribute that is varying\n",
        "                               value_test=.05,                    # test if the value selected is < 0.05\n",
        "                               nb_steps=10000)                    # the number of steps between value_max and value_min\n",
        "#BoltzmannQPolicy has a paramaeter tau the higher the value of tau more exploaration will be down. Lower value = less exploration\n",
        "#default value of tau is one -\n",
        "#need to change the value_max and value_min. \n",
        "# Q-Network\n",
        "model = Sequential() #sequnetial model \n",
        "model.add(Input(shape=(1,env.observation_space.shape[0]))) # 1 = one observation and env.observation_space.shape is the number of states within our observation. 0 = the first element of the tupel\n",
        "model.add(Flatten())\n",
        "# extra layers here\n",
        "model.add(Dense(16, activation='relu')) #one layer network\n",
        "model.add(Dense(env.action_space.n, activation='linear'))   # the output is the number of actions in the action space. Activation has to be linear due to how the q value does its calculation\n",
        "print(model.summary())\n",
        "\n",
        "# define the agent using the DQNAgent class\n",
        "dqn = DQNAgent(model=model,                     # Q-Network model created above ^\n",
        "               nb_actions=env.action_space.n,   # number of actions used above - the data from the enviroment\n",
        "               memory=memory,                   # experience replay memory\n",
        "               nb_steps_warmup=10,              # how many steps are waited before starting experience replay\n",
        "               target_model_update=1e-2,        # how often the target network is updated\n",
        "               policy=policy)                   # the action selection policy\n",
        "\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "\n",
        "history = dqn.fit(env, nb_steps=10000, visualize=False, verbose=2) #visualize false to save time\n",
        "\n",
        "# summarize the history for number  of episode steps\n",
        "plt.plot(history.history['nb_episode_steps'])\n",
        "plt.ylabel('nb_episode_steps')\n",
        "plt.xlabel('episodes')\n",
        "plt.show()\n",
        "\n",
        "dqn.test(env, nb_episodes=20, visualize=False) #testing for 20 episodes, reward should all be 200- evaluating my algortithm "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_1 (Flatten)          (None, 4)                 0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 16)                80        \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 2)                 34        \n",
            "=================================================================\n",
            "Total params: 114\n",
            "Trainable params: 114\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Training for 10000 steps ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n",
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:40: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "   17/10000: episode: 1, duration: 0.512s, episode steps:  17, steps per second:  33, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 0.577095, mae: 0.638507, mean_q: 0.254334, mean_tau: 0.998785\n",
            "   44/10000: episode: 2, duration: 0.182s, episode steps:  27, steps per second: 148, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 0.452965, mae: 0.622560, mean_q: 0.362587, mean_tau: 0.997300\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:40: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "   55/10000: episode: 3, duration: 0.086s, episode steps:  11, steps per second: 128, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.375977, mae: 0.604610, mean_q: 0.428774, mean_tau: 0.995590\n",
            "   74/10000: episode: 4, duration: 0.124s, episode steps:  19, steps per second: 153, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 0.396759, mae: 0.661294, mean_q: 0.559907, mean_tau: 0.994240\n",
            "  133/10000: episode: 5, duration: 0.380s, episode steps:  59, steps per second: 155, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 0.417894, mae: 0.682793, mean_q: 0.690476, mean_tau: 0.990730\n",
            "  182/10000: episode: 6, duration: 0.310s, episode steps:  49, steps per second: 158, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 0.352848, mae: 0.733916, mean_q: 0.924944, mean_tau: 0.985870\n",
            "  215/10000: episode: 7, duration: 0.217s, episode steps:  33, steps per second: 152, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 0.353840, mae: 0.808517, mean_q: 1.134209, mean_tau: 0.982180\n",
            "  228/10000: episode: 8, duration: 0.086s, episode steps:  13, steps per second: 152, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 0.321988, mae: 0.874157, mean_q: 1.337240, mean_tau: 0.980110\n",
            "  250/10000: episode: 9, duration: 0.141s, episode steps:  22, steps per second: 156, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.682 [0.000, 1.000],  loss: 0.266010, mae: 0.890823, mean_q: 1.443720, mean_tau: 0.978535\n",
            "  266/10000: episode: 10, duration: 0.116s, episode steps:  16, steps per second: 138, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  loss: 0.277238, mae: 0.951905, mean_q: 1.624946, mean_tau: 0.976825\n",
            "  287/10000: episode: 11, duration: 0.150s, episode steps:  21, steps per second: 140, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.619 [0.000, 1.000],  loss: 0.255833, mae: 1.007038, mean_q: 1.796913, mean_tau: 0.975160\n",
            "  324/10000: episode: 12, duration: 0.236s, episode steps:  37, steps per second: 157, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.568 [0.000, 1.000],  loss: 0.268303, mae: 1.107664, mean_q: 1.997260, mean_tau: 0.972550\n",
            "  340/10000: episode: 13, duration: 0.101s, episode steps:  16, steps per second: 159, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  loss: 0.255565, mae: 1.233811, mean_q: 2.232307, mean_tau: 0.970165\n",
            "  352/10000: episode: 14, duration: 0.077s, episode steps:  12, steps per second: 156, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.225945, mae: 1.275546, mean_q: 2.376107, mean_tau: 0.968905\n",
            "  367/10000: episode: 15, duration: 0.105s, episode steps:  15, steps per second: 142, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.243357, mae: 1.337974, mean_q: 2.484141, mean_tau: 0.967690\n",
            "  407/10000: episode: 16, duration: 0.260s, episode steps:  40, steps per second: 154, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 0.250363, mae: 1.429493, mean_q: 2.675748, mean_tau: 0.965215\n",
            "  424/10000: episode: 17, duration: 0.115s, episode steps:  17, steps per second: 148, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 0.241371, mae: 1.517879, mean_q: 2.916876, mean_tau: 0.962650\n",
            "  476/10000: episode: 18, duration: 0.340s, episode steps:  52, steps per second: 153, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.596 [0.000, 1.000],  loss: 0.249080, mae: 1.661008, mean_q: 3.208838, mean_tau: 0.959545\n",
            "  496/10000: episode: 19, duration: 0.135s, episode steps:  20, steps per second: 149, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.650 [0.000, 1.000],  loss: 0.240260, mae: 1.813773, mean_q: 3.497384, mean_tau: 0.956305\n",
            "  506/10000: episode: 20, duration: 0.075s, episode steps:  10, steps per second: 133, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.256940, mae: 1.865567, mean_q: 3.597727, mean_tau: 0.954955\n",
            "  524/10000: episode: 21, duration: 0.117s, episode steps:  18, steps per second: 154, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.381384, mae: 1.981820, mean_q: 3.741495, mean_tau: 0.953695\n",
            "  535/10000: episode: 22, duration: 0.073s, episode steps:  11, steps per second: 152, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.275701, mae: 1.991565, mean_q: 3.791452, mean_tau: 0.952390\n",
            "  563/10000: episode: 23, duration: 0.201s, episode steps:  28, steps per second: 139, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 0.274402, mae: 2.071055, mean_q: 3.968910, mean_tau: 0.950635\n",
            "  578/10000: episode: 24, duration: 0.105s, episode steps:  15, steps per second: 142, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.262259, mae: 2.154419, mean_q: 4.133213, mean_tau: 0.948700\n",
            "  606/10000: episode: 25, duration: 0.199s, episode steps:  28, steps per second: 141, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.607 [0.000, 1.000],  loss: 0.325927, mae: 2.261957, mean_q: 4.342072, mean_tau: 0.946765\n",
            "  623/10000: episode: 26, duration: 0.114s, episode steps:  17, steps per second: 149, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 0.339992, mae: 2.354592, mean_q: 4.517266, mean_tau: 0.944740\n",
            "  641/10000: episode: 27, duration: 0.128s, episode steps:  18, steps per second: 141, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.261688, mae: 2.408012, mean_q: 4.695662, mean_tau: 0.943165\n",
            "  656/10000: episode: 28, duration: 0.103s, episode steps:  15, steps per second: 146, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.380105, mae: 2.511718, mean_q: 4.781676, mean_tau: 0.941680\n",
            "  670/10000: episode: 29, duration: 0.093s, episode steps:  14, steps per second: 151, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 0.288090, mae: 2.541518, mean_q: 4.814829, mean_tau: 0.940375\n",
            "  707/10000: episode: 30, duration: 0.238s, episode steps:  37, steps per second: 155, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.568 [0.000, 1.000],  loss: 0.456499, mae: 2.663286, mean_q: 5.023715, mean_tau: 0.938080\n",
            "  722/10000: episode: 31, duration: 0.107s, episode steps:  15, steps per second: 140, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.405298, mae: 2.733407, mean_q: 5.146673, mean_tau: 0.935740\n",
            "  738/10000: episode: 32, duration: 0.109s, episode steps:  16, steps per second: 147, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 0.419106, mae: 2.795620, mean_q: 5.266092, mean_tau: 0.934345\n",
            "  752/10000: episode: 33, duration: 0.095s, episode steps:  14, steps per second: 148, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.786 [0.000, 1.000],  loss: 0.347931, mae: 2.830994, mean_q: 5.390453, mean_tau: 0.932995\n",
            "  767/10000: episode: 34, duration: 0.103s, episode steps:  15, steps per second: 145, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.516368, mae: 2.934300, mean_q: 5.475160, mean_tau: 0.931690\n",
            "  793/10000: episode: 35, duration: 0.179s, episode steps:  26, steps per second: 145, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 0.463364, mae: 2.995992, mean_q: 5.623980, mean_tau: 0.929845\n",
            "  803/10000: episode: 36, duration: 0.074s, episode steps:  10, steps per second: 135, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.493668, mae: 3.060104, mean_q: 5.706761, mean_tau: 0.928225\n",
            "  855/10000: episode: 37, duration: 0.328s, episode steps:  52, steps per second: 159, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.404 [0.000, 1.000],  loss: 0.507043, mae: 3.169441, mean_q: 5.921430, mean_tau: 0.925435\n",
            "  885/10000: episode: 38, duration: 0.197s, episode steps:  30, steps per second: 153, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.367 [0.000, 1.000],  loss: 0.511971, mae: 3.279457, mean_q: 6.095281, mean_tau: 0.921745\n",
            "  917/10000: episode: 39, duration: 0.203s, episode steps:  32, steps per second: 158, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.594 [0.000, 1.000],  loss: 0.431901, mae: 3.361874, mean_q: 6.349374, mean_tau: 0.918955\n",
            "  946/10000: episode: 40, duration: 0.188s, episode steps:  29, steps per second: 154, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  loss: 0.577025, mae: 3.475568, mean_q: 6.524474, mean_tau: 0.916210\n",
            "  970/10000: episode: 41, duration: 0.163s, episode steps:  24, steps per second: 147, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 0.562914, mae: 3.560795, mean_q: 6.697226, mean_tau: 0.913825\n",
            "  986/10000: episode: 42, duration: 0.104s, episode steps:  16, steps per second: 154, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.443511, mae: 3.632705, mean_q: 6.915554, mean_tau: 0.912025\n",
            " 1015/10000: episode: 43, duration: 0.189s, episode steps:  29, steps per second: 154, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 0.454894, mae: 3.671246, mean_q: 6.970437, mean_tau: 0.910000\n",
            " 1030/10000: episode: 44, duration: 0.110s, episode steps:  15, steps per second: 137, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 0.969495, mae: 3.830811, mean_q: 7.105184, mean_tau: 0.908020\n",
            " 1041/10000: episode: 45, duration: 0.075s, episode steps:  11, steps per second: 146, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.570747, mae: 3.793459, mean_q: 7.131073, mean_tau: 0.906850\n",
            " 1054/10000: episode: 46, duration: 0.088s, episode steps:  13, steps per second: 148, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 0.537041, mae: 3.854320, mean_q: 7.329636, mean_tau: 0.905770\n",
            " 1072/10000: episode: 47, duration: 0.136s, episode steps:  18, steps per second: 133, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  loss: 0.505800, mae: 3.903881, mean_q: 7.471512, mean_tau: 0.904375\n",
            " 1095/10000: episode: 48, duration: 0.154s, episode steps:  23, steps per second: 150, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.609 [0.000, 1.000],  loss: 0.673694, mae: 3.986715, mean_q: 7.575259, mean_tau: 0.902530\n",
            " 1109/10000: episode: 49, duration: 0.091s, episode steps:  14, steps per second: 154, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.214 [0.000, 1.000],  loss: 0.811225, mae: 4.028405, mean_q: 7.580490, mean_tau: 0.900865\n",
            " 1159/10000: episode: 50, duration: 0.326s, episode steps:  50, steps per second: 153, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.779733, mae: 4.170816, mean_q: 7.955367, mean_tau: 0.897985\n",
            " 1192/10000: episode: 51, duration: 0.220s, episode steps:  33, steps per second: 150, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 0.801694, mae: 4.298521, mean_q: 8.144827, mean_tau: 0.894250\n",
            " 1217/10000: episode: 52, duration: 0.159s, episode steps:  25, steps per second: 157, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  loss: 1.058666, mae: 4.439582, mean_q: 8.327435, mean_tau: 0.891640\n",
            " 1243/10000: episode: 53, duration: 0.176s, episode steps:  26, steps per second: 148, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.577 [0.000, 1.000],  loss: 0.823691, mae: 4.470042, mean_q: 8.461053, mean_tau: 0.889345\n",
            " 1264/10000: episode: 54, duration: 0.140s, episode steps:  21, steps per second: 150, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 0.804285, mae: 4.543493, mean_q: 8.659643, mean_tau: 0.887230\n",
            " 1285/10000: episode: 55, duration: 0.138s, episode steps:  21, steps per second: 152, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 0.495225, mae: 4.562349, mean_q: 8.836031, mean_tau: 0.885340\n",
            " 1306/10000: episode: 56, duration: 0.139s, episode steps:  21, steps per second: 151, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 1.092252, mae: 4.679208, mean_q: 8.886215, mean_tau: 0.883450\n",
            " 1319/10000: episode: 57, duration: 0.091s, episode steps:  13, steps per second: 143, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 0.846295, mae: 4.743448, mean_q: 9.095406, mean_tau: 0.881920\n",
            " 1338/10000: episode: 58, duration: 0.131s, episode steps:  19, steps per second: 145, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.316 [0.000, 1.000],  loss: 0.960130, mae: 4.780273, mean_q: 9.081516, mean_tau: 0.880480\n",
            " 1359/10000: episode: 59, duration: 0.142s, episode steps:  21, steps per second: 147, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.821769, mae: 4.810664, mean_q: 9.214207, mean_tau: 0.878680\n",
            " 1407/10000: episode: 60, duration: 0.319s, episode steps:  48, steps per second: 150, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 1.277691, mae: 5.017798, mean_q: 9.551009, mean_tau: 0.875575\n",
            " 1444/10000: episode: 61, duration: 0.239s, episode steps:  37, steps per second: 155, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.405 [0.000, 1.000],  loss: 1.193019, mae: 5.135228, mean_q: 9.733295, mean_tau: 0.871750\n",
            " 1468/10000: episode: 62, duration: 0.160s, episode steps:  24, steps per second: 150, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 1.030341, mae: 5.221431, mean_q: 9.903834, mean_tau: 0.869005\n",
            " 1486/10000: episode: 63, duration: 0.131s, episode steps:  18, steps per second: 138, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 1.252677, mae: 5.293098, mean_q: 10.069530, mean_tau: 0.867115\n",
            " 1496/10000: episode: 64, duration: 0.069s, episode steps:  10, steps per second: 145, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 1.247205, mae: 5.374013, mean_q: 10.336498, mean_tau: 0.865855\n",
            " 1515/10000: episode: 65, duration: 0.125s, episode steps:  19, steps per second: 152, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.316 [0.000, 1.000],  loss: 1.191930, mae: 5.390941, mean_q: 10.335993, mean_tau: 0.864550\n",
            " 1537/10000: episode: 66, duration: 0.173s, episode steps:  22, steps per second: 127, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 1.452342, mae: 5.472734, mean_q: 10.525803, mean_tau: 0.862705\n",
            " 1550/10000: episode: 67, duration: 0.093s, episode steps:  13, steps per second: 139, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.231 [0.000, 1.000],  loss: 1.569168, mae: 5.526691, mean_q: 10.493546, mean_tau: 0.861130\n",
            " 1565/10000: episode: 68, duration: 0.108s, episode steps:  15, steps per second: 139, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 1.205047, mae: 5.569252, mean_q: 10.541543, mean_tau: 0.859870\n",
            " 1581/10000: episode: 69, duration: 0.110s, episode steps:  16, steps per second: 145, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 1.193333, mae: 5.568318, mean_q: 10.575862, mean_tau: 0.858475\n",
            " 1602/10000: episode: 70, duration: 0.144s, episode steps:  21, steps per second: 146, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 1.256056, mae: 5.654782, mean_q: 10.855609, mean_tau: 0.856810\n",
            " 1616/10000: episode: 71, duration: 0.091s, episode steps:  14, steps per second: 154, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 1.290783, mae: 5.678363, mean_q: 10.862416, mean_tau: 0.855235\n",
            " 1678/10000: episode: 72, duration: 0.412s, episode steps:  62, steps per second: 150, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.387 [0.000, 1.000],  loss: 1.222100, mae: 5.794199, mean_q: 11.109763, mean_tau: 0.851815\n",
            " 1693/10000: episode: 73, duration: 0.101s, episode steps:  15, steps per second: 149, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 1.897219, mae: 5.891782, mean_q: 11.172791, mean_tau: 0.848350\n",
            " 1743/10000: episode: 74, duration: 0.326s, episode steps:  50, steps per second: 153, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.460 [0.000, 1.000],  loss: 1.411770, mae: 5.989146, mean_q: 11.460816, mean_tau: 0.845425\n",
            " 1755/10000: episode: 75, duration: 0.080s, episode steps:  12, steps per second: 150, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 1.592944, mae: 6.059733, mean_q: 11.704472, mean_tau: 0.842635\n",
            " 1769/10000: episode: 76, duration: 0.097s, episode steps:  14, steps per second: 144, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 2.008551, mae: 6.118400, mean_q: 11.641625, mean_tau: 0.841465\n",
            " 1782/10000: episode: 77, duration: 0.097s, episode steps:  13, steps per second: 134, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 1.513364, mae: 6.109565, mean_q: 11.612600, mean_tau: 0.840250\n",
            " 1819/10000: episode: 78, duration: 0.249s, episode steps:  37, steps per second: 149, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 1.955056, mae: 6.231052, mean_q: 11.819690, mean_tau: 0.838000\n",
            " 1854/10000: episode: 79, duration: 0.231s, episode steps:  35, steps per second: 151, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 1.697190, mae: 6.278068, mean_q: 11.930230, mean_tau: 0.834760\n",
            " 1895/10000: episode: 80, duration: 0.274s, episode steps:  41, steps per second: 150, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 1.595331, mae: 6.347997, mean_q: 12.097886, mean_tau: 0.831340\n",
            " 1909/10000: episode: 81, duration: 0.095s, episode steps:  14, steps per second: 147, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 1.581785, mae: 6.507738, mean_q: 12.603007, mean_tau: 0.828865\n",
            " 1983/10000: episode: 82, duration: 0.497s, episode steps:  74, steps per second: 149, episode reward: 74.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 1.851735, mae: 6.602316, mean_q: 12.636690, mean_tau: 0.824905\n",
            " 2014/10000: episode: 83, duration: 0.204s, episode steps:  31, steps per second: 152, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  loss: 1.887687, mae: 6.714317, mean_q: 12.879692, mean_tau: 0.820180\n",
            " 2075/10000: episode: 84, duration: 0.414s, episode steps:  61, steps per second: 147, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 1.803950, mae: 6.848447, mean_q: 13.180883, mean_tau: 0.816040\n",
            " 2094/10000: episode: 85, duration: 0.125s, episode steps:  19, steps per second: 152, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 1.730018, mae: 6.938167, mean_q: 13.327301, mean_tau: 0.812440\n",
            " 2118/10000: episode: 86, duration: 0.159s, episode steps:  24, steps per second: 151, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.813797, mae: 7.003617, mean_q: 13.463505, mean_tau: 0.810505\n",
            " 2157/10000: episode: 87, duration: 0.253s, episode steps:  39, steps per second: 154, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 1.792962, mae: 7.083072, mean_q: 13.722723, mean_tau: 0.807670\n",
            " 2200/10000: episode: 88, duration: 0.278s, episode steps:  43, steps per second: 155, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.442 [0.000, 1.000],  loss: 2.747672, mae: 7.219789, mean_q: 13.688387, mean_tau: 0.803980\n",
            " 2227/10000: episode: 89, duration: 0.177s, episode steps:  27, steps per second: 153, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 2.553415, mae: 7.260754, mean_q: 13.736537, mean_tau: 0.800830\n",
            " 2279/10000: episode: 90, duration: 0.339s, episode steps:  52, steps per second: 154, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 1.651011, mae: 7.358015, mean_q: 14.274432, mean_tau: 0.797275\n",
            " 2358/10000: episode: 91, duration: 0.502s, episode steps:  79, steps per second: 157, episode reward: 79.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.557 [0.000, 1.000],  loss: 2.796682, mae: 7.529127, mean_q: 14.314873, mean_tau: 0.791380\n",
            " 2418/10000: episode: 92, duration: 0.387s, episode steps:  60, steps per second: 155, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 2.732367, mae: 7.638110, mean_q: 14.552553, mean_tau: 0.785125\n",
            " 2471/10000: episode: 93, duration: 0.347s, episode steps:  53, steps per second: 153, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 1.873182, mae: 7.786035, mean_q: 15.112566, mean_tau: 0.780040\n",
            " 2502/10000: episode: 94, duration: 0.199s, episode steps:  31, steps per second: 156, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  loss: 2.601500, mae: 7.905698, mean_q: 15.204703, mean_tau: 0.776260\n",
            " 2525/10000: episode: 95, duration: 0.158s, episode steps:  23, steps per second: 145, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 1.966602, mae: 8.029985, mean_q: 15.615114, mean_tau: 0.773830\n",
            " 2542/10000: episode: 96, duration: 0.126s, episode steps:  17, steps per second: 135, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 2.353043, mae: 8.059575, mean_q: 15.584888, mean_tau: 0.772030\n",
            " 2575/10000: episode: 97, duration: 0.218s, episode steps:  33, steps per second: 151, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 2.104066, mae: 8.095096, mean_q: 15.685819, mean_tau: 0.769780\n",
            " 2635/10000: episode: 98, duration: 0.402s, episode steps:  60, steps per second: 149, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 2.524297, mae: 8.211588, mean_q: 15.883425, mean_tau: 0.765595\n",
            " 2691/10000: episode: 99, duration: 0.371s, episode steps:  56, steps per second: 151, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 2.517137, mae: 8.364429, mean_q: 16.255920, mean_tau: 0.760375\n",
            " 2737/10000: episode: 100, duration: 0.306s, episode steps:  46, steps per second: 150, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 2.865246, mae: 8.553177, mean_q: 16.599182, mean_tau: 0.755785\n",
            " 2753/10000: episode: 101, duration: 0.111s, episode steps:  16, steps per second: 144, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.251627, mae: 8.679772, mean_q: 16.510627, mean_tau: 0.752995\n",
            " 2780/10000: episode: 102, duration: 0.184s, episode steps:  27, steps per second: 147, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 2.240969, mae: 8.559507, mean_q: 16.608387, mean_tau: 0.751060\n",
            " 2807/10000: episode: 103, duration: 0.176s, episode steps:  27, steps per second: 153, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 2.612609, mae: 8.692832, mean_q: 16.948715, mean_tau: 0.748630\n",
            " 2836/10000: episode: 104, duration: 0.198s, episode steps:  29, steps per second: 146, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 3.255598, mae: 8.844288, mean_q: 17.139878, mean_tau: 0.746110\n",
            " 2896/10000: episode: 105, duration: 0.386s, episode steps:  60, steps per second: 155, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 2.895584, mae: 8.937821, mean_q: 17.375921, mean_tau: 0.742105\n",
            " 2932/10000: episode: 106, duration: 0.232s, episode steps:  36, steps per second: 155, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.624040, mae: 9.030378, mean_q: 17.465534, mean_tau: 0.737785\n",
            " 2991/10000: episode: 107, duration: 0.403s, episode steps:  59, steps per second: 146, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 2.875295, mae: 9.133505, mean_q: 17.762396, mean_tau: 0.733510\n",
            " 3050/10000: episode: 108, duration: 0.397s, episode steps:  59, steps per second: 149, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 3.302335, mae: 9.317755, mean_q: 18.072567, mean_tau: 0.728200\n",
            " 3084/10000: episode: 109, duration: 0.221s, episode steps:  34, steps per second: 154, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.714461, mae: 9.401872, mean_q: 18.317958, mean_tau: 0.724015\n",
            " 3143/10000: episode: 110, duration: 0.370s, episode steps:  59, steps per second: 160, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 3.179623, mae: 9.507637, mean_q: 18.472437, mean_tau: 0.719830\n",
            " 3174/10000: episode: 111, duration: 0.201s, episode steps:  31, steps per second: 155, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 3.500429, mae: 9.604732, mean_q: 18.654149, mean_tau: 0.715780\n",
            " 3242/10000: episode: 112, duration: 0.427s, episode steps:  68, steps per second: 159, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 3.223271, mae: 9.715981, mean_q: 18.867028, mean_tau: 0.711325\n",
            " 3343/10000: episode: 113, duration: 0.666s, episode steps: 101, steps per second: 152, episode reward: 101.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 4.108930, mae: 9.949999, mean_q: 19.185842, mean_tau: 0.703720\n",
            " 3396/10000: episode: 114, duration: 0.341s, episode steps:  53, steps per second: 156, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 3.968497, mae: 10.090772, mean_q: 19.565308, mean_tau: 0.696790\n",
            " 3436/10000: episode: 115, duration: 0.260s, episode steps:  40, steps per second: 154, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 4.090376, mae: 10.235937, mean_q: 19.795324, mean_tau: 0.692605\n",
            " 3486/10000: episode: 116, duration: 0.351s, episode steps:  50, steps per second: 142, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 3.988424, mae: 10.276220, mean_q: 19.853466, mean_tau: 0.688555\n",
            " 3536/10000: episode: 117, duration: 0.325s, episode steps:  50, steps per second: 154, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 3.188888, mae: 10.401973, mean_q: 20.307945, mean_tau: 0.684055\n",
            " 3611/10000: episode: 118, duration: 0.497s, episode steps:  75, steps per second: 151, episode reward: 75.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 3.918282, mae: 10.496201, mean_q: 20.349556, mean_tau: 0.678430\n",
            " 3646/10000: episode: 119, duration: 0.236s, episode steps:  35, steps per second: 148, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 4.240813, mae: 10.638478, mean_q: 20.601303, mean_tau: 0.673480\n",
            " 3732/10000: episode: 120, duration: 0.560s, episode steps:  86, steps per second: 154, episode reward: 86.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 4.037725, mae: 10.712576, mean_q: 20.794319, mean_tau: 0.668035\n",
            " 3789/10000: episode: 121, duration: 0.371s, episode steps:  57, steps per second: 154, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 3.399675, mae: 10.835090, mean_q: 21.144230, mean_tau: 0.661600\n",
            " 3845/10000: episode: 122, duration: 0.360s, episode steps:  56, steps per second: 156, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 4.007638, mae: 11.066468, mean_q: 21.574442, mean_tau: 0.656515\n",
            " 3933/10000: episode: 123, duration: 0.568s, episode steps:  88, steps per second: 155, episode reward: 88.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 3.994652, mae: 11.120856, mean_q: 21.657922, mean_tau: 0.650035\n",
            " 4003/10000: episode: 124, duration: 0.446s, episode steps:  70, steps per second: 157, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 5.073375, mae: 11.335458, mean_q: 21.933541, mean_tau: 0.642925\n",
            " 4041/10000: episode: 125, duration: 0.248s, episode steps:  38, steps per second: 153, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 4.265710, mae: 11.399670, mean_q: 22.110864, mean_tau: 0.638065\n",
            " 4071/10000: episode: 126, duration: 0.189s, episode steps:  30, steps per second: 159, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 4.441486, mae: 11.388398, mean_q: 22.115183, mean_tau: 0.635005\n",
            " 4148/10000: episode: 127, duration: 0.516s, episode steps:  77, steps per second: 149, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 4.950583, mae: 11.591605, mean_q: 22.432588, mean_tau: 0.630190\n",
            " 4298/10000: episode: 128, duration: 0.931s, episode steps: 150, steps per second: 161, episode reward: 150.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 4.410244, mae: 11.721623, mean_q: 22.743489, mean_tau: 0.619975\n",
            " 4331/10000: episode: 129, duration: 0.207s, episode steps:  33, steps per second: 160, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 4.221826, mae: 11.858362, mean_q: 23.082338, mean_tau: 0.611740\n",
            " 4396/10000: episode: 130, duration: 0.418s, episode steps:  65, steps per second: 156, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 4.622717, mae: 11.932243, mean_q: 23.272956, mean_tau: 0.607330\n",
            " 4475/10000: episode: 131, duration: 0.501s, episode steps:  79, steps per second: 158, episode reward: 79.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 4.164406, mae: 12.058276, mean_q: 23.526378, mean_tau: 0.600850\n",
            " 4553/10000: episode: 132, duration: 0.505s, episode steps:  78, steps per second: 155, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 5.781456, mae: 12.274878, mean_q: 23.810773, mean_tau: 0.593785\n",
            " 4603/10000: episode: 133, duration: 0.325s, episode steps:  50, steps per second: 154, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.806809, mae: 12.286983, mean_q: 24.048994, mean_tau: 0.588025\n",
            " 4638/10000: episode: 134, duration: 0.245s, episode steps:  35, steps per second: 143, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 6.677536, mae: 12.481555, mean_q: 24.180432, mean_tau: 0.584200\n",
            " 4690/10000: episode: 135, duration: 0.331s, episode steps:  52, steps per second: 157, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 4.236977, mae: 12.398653, mean_q: 24.265247, mean_tau: 0.580285\n",
            " 4800/10000: episode: 136, duration: 0.696s, episode steps: 110, steps per second: 158, episode reward: 110.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 4.433142, mae: 12.588564, mean_q: 24.711995, mean_tau: 0.572995\n",
            " 4890/10000: episode: 137, duration: 0.575s, episode steps:  90, steps per second: 156, episode reward: 90.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 4.996926, mae: 12.827284, mean_q: 25.136209, mean_tau: 0.563995\n",
            " 4967/10000: episode: 138, duration: 0.502s, episode steps:  77, steps per second: 153, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 4.423174, mae: 13.033197, mean_q: 25.612387, mean_tau: 0.556480\n",
            " 5015/10000: episode: 139, duration: 0.307s, episode steps:  48, steps per second: 157, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 3.720842, mae: 13.091806, mean_q: 25.787927, mean_tau: 0.550855\n",
            " 5104/10000: episode: 140, duration: 0.576s, episode steps:  89, steps per second: 154, episode reward: 89.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 4.041755, mae: 13.310224, mean_q: 26.291697, mean_tau: 0.544690\n",
            " 5170/10000: episode: 141, duration: 0.440s, episode steps:  66, steps per second: 150, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 5.884387, mae: 13.473257, mean_q: 26.346031, mean_tau: 0.537715\n",
            " 5268/10000: episode: 142, duration: 0.623s, episode steps:  98, steps per second: 157, episode reward: 98.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 5.486183, mae: 13.581922, mean_q: 26.605011, mean_tau: 0.530335\n",
            " 5312/10000: episode: 143, duration: 0.281s, episode steps:  44, steps per second: 157, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 5.556916, mae: 13.748265, mean_q: 26.941912, mean_tau: 0.523945\n",
            " 5396/10000: episode: 144, duration: 0.543s, episode steps:  84, steps per second: 155, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 5.001770, mae: 13.776303, mean_q: 27.084291, mean_tau: 0.518185\n",
            " 5443/10000: episode: 145, duration: 0.293s, episode steps:  47, steps per second: 161, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 6.189267, mae: 14.026326, mean_q: 27.484325, mean_tau: 0.512290\n",
            " 5502/10000: episode: 146, duration: 0.389s, episode steps:  59, steps per second: 152, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.441 [0.000, 1.000],  loss: 4.956995, mae: 13.961539, mean_q: 27.460710, mean_tau: 0.507520\n",
            " 5616/10000: episode: 147, duration: 0.725s, episode steps: 114, steps per second: 157, episode reward: 114.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 5.219733, mae: 14.183665, mean_q: 27.879862, mean_tau: 0.499735\n",
            " 5686/10000: episode: 148, duration: 0.456s, episode steps:  70, steps per second: 154, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 5.047899, mae: 14.397861, mean_q: 28.390078, mean_tau: 0.491455\n",
            " 5769/10000: episode: 149, duration: 0.556s, episode steps:  83, steps per second: 149, episode reward: 83.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 5.327809, mae: 14.506506, mean_q: 28.502069, mean_tau: 0.484570\n",
            " 5845/10000: episode: 150, duration: 0.507s, episode steps:  76, steps per second: 150, episode reward: 76.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.447 [0.000, 1.000],  loss: 5.067294, mae: 14.624946, mean_q: 28.833786, mean_tau: 0.477415\n",
            " 5968/10000: episode: 151, duration: 0.772s, episode steps: 123, steps per second: 159, episode reward: 123.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 6.424724, mae: 14.826084, mean_q: 29.110081, mean_tau: 0.468460\n",
            " 6116/10000: episode: 152, duration: 0.928s, episode steps: 148, steps per second: 159, episode reward: 148.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 5.338088, mae: 15.027223, mean_q: 29.655559, mean_tau: 0.456265\n",
            " 6196/10000: episode: 153, duration: 0.526s, episode steps:  80, steps per second: 152, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 4.795175, mae: 15.229119, mean_q: 30.020689, mean_tau: 0.446005\n",
            " 6343/10000: episode: 154, duration: 0.946s, episode steps: 147, steps per second: 155, episode reward: 147.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 6.236522, mae: 15.443898, mean_q: 30.382371, mean_tau: 0.435790\n",
            " 6408/10000: episode: 155, duration: 0.417s, episode steps:  65, steps per second: 156, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.446 [0.000, 1.000],  loss: 5.841557, mae: 15.567836, mean_q: 30.626447, mean_tau: 0.426250\n",
            " 6498/10000: episode: 156, duration: 0.587s, episode steps:  90, steps per second: 153, episode reward: 90.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.456 [0.000, 1.000],  loss: 6.327381, mae: 15.761795, mean_q: 31.089243, mean_tau: 0.419275\n",
            " 6609/10000: episode: 157, duration: 0.723s, episode steps: 111, steps per second: 153, episode reward: 111.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 5.145561, mae: 15.839598, mean_q: 31.293097, mean_tau: 0.410230\n",
            " 6700/10000: episode: 158, duration: 0.589s, episode steps:  91, steps per second: 155, episode reward: 91.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 4.644321, mae: 15.984628, mean_q: 31.641735, mean_tau: 0.401140\n",
            " 6826/10000: episode: 159, duration: 0.807s, episode steps: 126, steps per second: 156, episode reward: 126.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 5.343832, mae: 16.274668, mean_q: 32.255614, mean_tau: 0.391375\n",
            " 6901/10000: episode: 160, duration: 0.470s, episode steps:  75, steps per second: 160, episode reward: 75.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 6.615306, mae: 16.415057, mean_q: 32.473783, mean_tau: 0.382330\n",
            " 7012/10000: episode: 161, duration: 0.715s, episode steps: 111, steps per second: 155, episode reward: 111.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 5.409502, mae: 16.521080, mean_q: 32.683366, mean_tau: 0.373960\n",
            " 7173/10000: episode: 162, duration: 1.022s, episode steps: 161, steps per second: 158, episode reward: 161.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 7.649769, mae: 16.848994, mean_q: 33.222908, mean_tau: 0.361720\n",
            " 7330/10000: episode: 163, duration: 0.995s, episode steps: 157, steps per second: 158, episode reward: 157.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 6.667174, mae: 17.016642, mean_q: 33.636956, mean_tau: 0.347410\n",
            " 7530/10000: episode: 164, duration: 1.298s, episode steps: 200, steps per second: 154, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 6.156155, mae: 17.331407, mean_q: 34.361299, mean_tau: 0.331345\n",
            " 7668/10000: episode: 165, duration: 0.917s, episode steps: 138, steps per second: 151, episode reward: 138.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 7.832826, mae: 17.608458, mean_q: 34.730225, mean_tau: 0.316135\n",
            " 7823/10000: episode: 166, duration: 0.994s, episode steps: 155, steps per second: 156, episode reward: 155.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 6.651241, mae: 17.803938, mean_q: 35.193189, mean_tau: 0.302950\n",
            " 7977/10000: episode: 167, duration: 0.979s, episode steps: 154, steps per second: 157, episode reward: 154.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 5.801001, mae: 18.083758, mean_q: 35.828180, mean_tau: 0.289045\n",
            " 8124/10000: episode: 168, duration: 0.935s, episode steps: 147, steps per second: 157, episode reward: 147.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  loss: 6.720350, mae: 18.285963, mean_q: 36.201018, mean_tau: 0.275500\n",
            " 8290/10000: episode: 169, duration: 1.071s, episode steps: 166, steps per second: 155, episode reward: 166.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 7.293790, mae: 18.502976, mean_q: 36.625064, mean_tau: 0.261415\n",
            " 8490/10000: episode: 170, duration: 1.281s, episode steps: 200, steps per second: 156, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 7.670532, mae: 18.703848, mean_q: 37.038198, mean_tau: 0.244945\n",
            " 8682/10000: episode: 171, duration: 1.214s, episode steps: 192, steps per second: 158, episode reward: 192.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 6.473673, mae: 19.057582, mean_q: 37.850596, mean_tau: 0.227305\n",
            " 8837/10000: episode: 172, duration: 0.997s, episode steps: 155, steps per second: 155, episode reward: 155.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 7.753268, mae: 19.316574, mean_q: 38.268439, mean_tau: 0.211690\n",
            " 9037/10000: episode: 173, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 7.330695, mae: 19.546376, mean_q: 38.773803, mean_tau: 0.195715\n",
            " 9237/10000: episode: 174, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 7.371597, mae: 19.784669, mean_q: 39.269139, mean_tau: 0.177715\n",
            " 9437/10000: episode: 175, duration: 1.289s, episode steps: 200, steps per second: 155, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 8.224055, mae: 20.116561, mean_q: 39.950925, mean_tau: 0.159715\n",
            " 9602/10000: episode: 176, duration: 1.054s, episode steps: 165, steps per second: 156, episode reward: 165.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  loss: 7.821775, mae: 20.316382, mean_q: 40.399957, mean_tau: 0.143290\n",
            " 9781/10000: episode: 177, duration: 1.165s, episode steps: 179, steps per second: 154, episode reward: 179.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 8.058986, mae: 20.565195, mean_q: 40.848813, mean_tau: 0.127810\n",
            " 9981/10000: episode: 178, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 8.112255, mae: 20.903043, mean_q: 41.551165, mean_tau: 0.110755\n",
            "done, took 65.632 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEGCAYAAABhMDI9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOydeZhkZX3vv79zTu3V+zb7AgwICAwyIIq4giJGzaImxhgSTQg3xCXX3ERjkuuTJ8nNvS55olEDLgEjokaC0UgQgsgiIswAMywzwywMTM90T/f0Vl3bWd/7xznve86pOrV1V00v836ep5+uOlWn6q1e3t/5/lZijEEikUgkEo6y1AuQSCQSyfJCGgaJRCKRhJCGQSKRSCQhpGGQSCQSSQhpGCQSiUQSQlvqBSyWwcFBtmXLlqVehkQikawodu3adZIxNhT12Io3DFu2bMHOnTuXehkSiUSyoiCiF2s9Jl1JEolEIgkhDYNEIpFIQkjDIJFIJJIQ0jBIJBKJJIQ0DBKJRCIJ0VHDQEQbieh+InqOiJ4loo94x/uJ6F4iOuB97/OOExF9nogOEtEeInpFJ9cnkUgkkmo6rRgsAB9jjJ0H4HIANxLReQA+DuA+xtg2APd59wHgrQC2eV/XA/hyh9cnkUgkkgo6WsfAGBsDMObdnieivQDWA3gngNd7T7sVwE8B/Jl3/BvM7QX+KBH1EtFa73UkEolkxXDPs+PYvrEXw93Jus977ngOBcPCpVv6xbGXpoq444lR8LEIMVXB+y7fjP5MXDzni/cfxHnruvGGc4bbvvZTVuBGRFsAXAzgFwBGApv9OIAR7/Z6AEcDp416x0KGgYiuh6sosGnTpo6tWSKRSBaCYTm44Zu78MdXnY0PvWlb3ed+7t792Dc+j4f/7I3i2G2/eBE3PXgYRAAfmdOXieO3Lt8snvOFnxzA+y/f3BHDcEqCz0SUBXAHgI8yxnLBxzx10NK0IMbYzYyxHYyxHUNDkRXdEolEsmSUTBsOc783IleyMDpTwlReF8cKhoX+TBwv/J+3Ye9fXwMAyOuW//qGjbLpoD+TaP/icQoMAxHF4BqF2xhj/+4dPkFEa73H1wKY8I4fA7AxcPoG75hEIpGsGMqeQTBtp+FzC4a74e8ZnQuc7yCpudtzMqaACCgGDMN00QAA9GdibVtzkE5nJRGArwHYyxj7XOChHwC4zrt9HYD/CBz/bS876XIAczK+IJFIVholwzUMhtWEYfA2/N2js/75po1kXAUAEBEycQ0Fw1cfMwXXMPSl4+gEnY4xXAHg/QCeJqKnvGN/DuDvAXyXiD4I4EUA7/EeuwvAtQAOAigC+N0Or08ikUjaDnchGU0pBve5QcWgmzaSmirup+OqMCAAMFXgimEFGgbG2MMAqMbDb4p4PgNwYyfXJJFIJJ2GGwa9BcWwZ3QWjDEQEcqmg1TcNwyZRA3F0CHDICufJRKJpM3wGEMjV5LjMBQNG/2ZOE7mDRyfKwPwXEkxf3tOx9VwjIErhg65kqRhkEgkkjbTrGHgyuJVZwwAAPYcnRXnB11JbozBNwwzRQMKAT2pFRh8lkgkktORkuEahEZZSdyNdMnmPigE7B1zs/mDwWcASCdUFAOupOmCgb50HIpSy1O/OKRhkEgkkjbTbPCZxw36M3FkExpyZddQ6KZTrRj0sGLoVHwBkIZBIpFI2k6pSVcS3+zTcRXpuCbSXMumjVTc354zEYqhU/EFQBoGiUQiaTvlJusYuGHIJDSkE6qII5Sq0lXDimG6YKCvQ8VtgDQMEolE0nbKTaarckOQSWhIx1WUDBuMMTf4HAumq7qKgTfVmy6YHWuHAUjDIJFIJG2n6RiD7j4vE1eRjrmZR4btwGEI1TGk4xosh8GwHTDGMFM0OtYOA5CGQSKRSNpOqcleSZWuJN4cDwASWiDG4BmJgm4jV7ZgO6xj7TAAaRgkEomk7TRbx8CzkjJx15VUMGzo3rlBV1I64TapKOiWqHruVDsM4BTOY5BIJJLThWab6PFq5nTCz0riaiMVC6erAkDRsEVcQqarSiQSyQqCu4MaGYa8YSGuKYipitv2wrDEuWHF4LmSDAvT+c62wwCkYZBIJJK202zwuajbyHpuorTXWlsohmAdA1cMuh2YxSANg0QikawY/OAzg+PUHlBZ0C2kvcByOq7CsBzhXqpsuw24iuFUxBikYZBIJJI2Uw6M9DSd2qqhYFhCDfDNn89aSARcSVxVFA0L00UDcU0Rz+8E0jBIJBJJmykF2lfUizMUdBuZBFcM7ubPW2qnomIMuo0Zrx2GOyCzM0jDIJFIJG2mZDZpGAwLmUS0YgjOY/CzkixMzOsY7OqcGwno/MznrxPRBBE9Ezj2HSJ6yvs6wkd+EtEWIioFHvvnTq5NIpFIOkXZdMA7YtcLQBf0alfSdEEHEK585uohr9s4PlvC+t5UJ5Yt6HQdwy0A/gnAN/gBxtiv89tE9FkAc4HnH2KMbe/wmiQSiaSjlE0b3akYZotmQ1dSusKVNFMwAYSDz4pCYu7zsZkSrjhrsIOr77BiYIw9CGA66jFyHWTvAXB7J9cgkUgkpxLGGEqmLaar1TMMRcPy01UT3JXkKoZgHQPgGo7xuTIKht1xxbCUMYYrAZxgjB0IHNtKRE8S0QNEdGWtE4noeiLaSUQ7JycnO79SiUQiaRLTZrAd5huGuq4kWygF35XkZSVp4e05k1BxYGIeAFa1YXgvwmphDMAmxtjFAP4ngG8RUXfUiYyxmxljOxhjO4aGhk7BUiUSiaQ5eOC5kWIwLAeG7SDLXUkxPyspoSlVYzvTcQ0vnCwAANatRsNARBqAXwXwHX6MMaYzxqa827sAHAJw9lKsTyKRSBYKr2HoTtY3DEWDT28Lu5JmimYo8MzJxFWYtlsstyoNA4CrAOxjjI3yA0Q0RESqd/sMANsAHF6i9UkkEsmCEIYh5W74tVxJorNqwq98BgDbYaHAM4d3WI1rCgY6WPUMdD5d9XYAPwdwDhGNEtEHvYd+A9VB59cC2OOlr34PwA2MscjAtUQikSxXSsIw1FcMwVkMgJuFxGvWgjUMHO5yWteTrHIztZuOpqsyxt5b4/jvRBy7A8AdnVyPRCKRdBpe9dwoxiAMg+dKUhRCKuaO8KzMSAJ8l9P6vs66kQBZ+SyRSCRtpSr4XMuVxMd6Jvzrc775RxkGPsVtXY80DBKJRLKiKDeZlVQQwefqLqqpKMXgGZBOB54BaRgkEomkrZQM1xCIrKQaioFnJWVDisE1CFExBq4YOl3DAEjDIJFIJG2l3GTwOV/2x3pyfMMgYwwSiUSyami2wG3a64nUFxjRyTf/KFdSX8Z9vY196fYttgadbqInkUgkpxXNxhhO5nX0pmOIqf71OVcMiQjD8NaXr8VwVxKbBjpvGKRikEgkkjbC01W7ku51t1kjxjBV0KsK1eoFn5MxteNdVTnSMEgkEkkbKZk2NIUQUxXENQV6DcNwMm9gMJsIHUuJdNWl3ZqlYZBIJJI28NCBSfzql36G6YIhrvjjqlLXlVRpGDJ1gs+nEhljkEgkkjawZ3QOT7w0iwMn8kh6G3xcq20YpvIGBrLNu5JOJVIxSCQSSRvQvaDzvG41VAyG5WCuZFYpBl7EJl1JEolEsgooBwyAMAyaElngxofx1FIMUVlJpxLpSpJIJJI2oJs20nEVCpG44o9rSmRW0sm8O75zIFOhGOrUMZxKpGGQSCSSNlA2HXQlNXzszecA7jydmq6kKU8xDHVFKwYZfJZIJJJVgG657bLfs2OjOBbTFOiWg10vzuCe58bx+1eegcFsAifnoxVDSgafJRKJZPWgWw4SWnhLTXiK4fbHXsJNDxzGGz/zU9y/fwJTBc8wVMQYLtrQi6vPG8H56yLH3Z8ypGGQSCSSNlA2qwfs8ODzxLyOzQNp9Kbj+NL9BzGVNxDXlFBnVQDoz8Txld/egb4Oj+5sRKdHe36diCaI6JnAsU8R0TEiesr7ujbw2CeI6CAR7Seit3RybRKJRNJOohQDr2OYyJWxbTiLt5w/gt1H53BstoShbAJEnR3RuVA6rRhuAXBNxPF/YIxt977uAgAiOg/uLOjzvXO+RERL62iTSCSSJolUDJ4r6WRex1BXEpdu6YdhO3jowMkqN9JyoqOGgTH2IIDpJp/+TgDfZozpjLEXABwEcFnHFieRSE4Jjxw8iaPTxaVeRseppRhKpo2pgoHhrgQu2dwHAJgrmVUN9JYTSxVj+CMi2uO5mvq8Y+sBHA08Z9Q7VgURXU9EO4lo5+TkZKfXKpFIFsGHv/0UvvbwC0u9jI5TNu2qwrSYquBErgzGgOHuBAayCZw5lAGAqqrn5cRSGIYvAzgTwHYAYwA+2+oLMMZuZoztYIztGBoaavf6JBJJGymbNnTLXupl1OX4bAl7x3KLeo1aisG03aKG4a4kAOCyrf0AgAFpGHwYYycYYzZjzAHwFfjuomMANgaeusE7JpFIVjCG7cDyNsflyqd/vB8fvv1Jcf+xF6bhOK2tuWw6SGhhxRA0FMNdriHYsdk1DIOna4whCiJaG7j7KwB4xtIPAPwGESWIaCuAbQAeO9Xrk0gk7cWyHdhseRuGk3kdBd2dwbxvPIf33PRzPHp4qqXXcAvcqhUDZ8gzDK8+awDJmIKzhrOLXHXn6GjlMxHdDuD1AAaJaBTA/wbweiLaDrdo/AiAPwAAxtizRPRdAM8BsADcyBhb3vpTIpHUxXYYHOZ+X87MlUwYnqrJlVwDkStbLb2GHqEY4oGxnTymsLYnhSf/8s1L3kG1Hh01DIyx90Yc/lqd5/8tgL/t3IokEsmphDeQs1aAYeBr9dccPUchCsdhMGynarPn85z7M/GQeuCtL5YrTZssIvoIEXWTy9eI6AkienMnFyeRSFY2vOV0q/76U81cyRTN7viaW4mL6N65VYrBMwY8vrBSaEXLfIAxlgPwZgB9AN4P4O87siqJRLIqMK3lrxgchyEXUAzcQES1y64Fz7qqFWMYWsWGgdduXwvgXxljzwaOSSQSSRXcICxnxZA3LDjMXavjsAW5vxorhmSbVntqaMUw7CKie+Aahh8TUReA5k2qRCI57TBWgGKYK5rituk4VbGGZiib0Yoh4cUYhrtXlmJoJfj8QbhFaYcZY0UiGgDwu51ZlkQiWQ3wzXU5ZyXNlQKGwWYBV1L7FMPQMi5mi6Jpw8AYc4hoC4DfIiIG4GHG2J2dWphEIln58M11ORuGXMAwGJYj0latNiiG2ApVDK1kJX0JwA0AnoZblPYHRPTFTi1MIpGsfFaeYnAWFDCvpRj4qM413SsrxtCKK+mNAM5lzC1hJKJb4RajSSQSSSQLqQk41eTKlYqhfTGGK84axGfefRFesakv6rRlSyvB54MANgXubwRwoL3LkUgkqwnhSlq+gqG2YmglxmDWjjG865INUJSVlcDZimLoArCXiB6D287iMgA7iegHAMAYe0cH1ieRSFYwvitp+SqGyuDzgrKSatQxrFRaMQx/1bFVSCSSVYkhDMMSL6QOcxXBZ91eQFZSDcWwUmklK+kBItoMYBtj7L+JKAVAY4zNd255EolkJcPdMstbMfjN8gzbgWl5WUktrJkrhsQqUQytZCX9PoDvAbjJO7QBwPc7sSiJRLI64Jk9y7rArTLGsAjFkFwliqEV83YjgCsA5ACAMXYAwHAnFiWRSFYH5gpoojdXMkVaqWk7frV2S72SPFfS6aYYAOiMMYPfISINbhBaIpFIIlkJLTFyJVPMSggqhlbWzNNVK0d7rlRa+RQPENGfA0gR0dUA/g3ADzuzLIlEshrg7pjlrhj4mM1gHYPRomKIawqIVlZaai1aMQwfBzAJt/L5DwDcxRj7ZEdWJZFIVgXLfVAPYwxzJVO0xTYCvZJabYmRXCVqAWjNMHyIMfYVxti7GWPvYox9hYg+Uu8EIvo6EU0Q0TOBY58mon1EtIeI7iSiXu/4FiIqEdFT3tc/L/AzSSSSZcJyb4lRMGzYDvNdSVbAldRiE71EbHUEnoHWDMN1Ecd+p8E5twC4puLYvQBezhi7EMDzAD4ReOwQY2y793VDC2uTSCTLEL/yeXkaBt5ALxxjcNdq1jFmhybzeOjApLivm/aqKW4DmqhjIKL3AvhNAFt5lbNHN4Dpeucyxh70OrIGj90TuPsogHc1u1iJRLKyEIphmfbE4Kmqg12+YWjGlXTTA4fw0IGT+Pkn3gTAUwyrJFUVaK7A7REAYwAGAXw2cHwewJ5Fvv8HAHwncH8rET0JNyX2LxhjD0WdRETXA7geADZt2hT1FIlEsgwQhmGZKgZuGPi8BD0QfK7nSpovWygatrhfPt0UA2PsRQAvEtFVAEreXIazAbwMbiB6QRDRJwFYAG7zDo0B2MQYmyKiSwB8n4jO9+ZMV67pZgA3A8COHTuW51+cRCLxN9llGmMQhqHLzUoK9kqql5VUMGyRogqsPsXQiol7EECSiNYDuAfA++HGEFqGiH4HwC8BeB9v480Y0xljU97tXQAOATh7Ia8vkUiWB9YyH9QzX3bbYfRnouoYahuGom5Btxx429eqUwytfBJijBUB/CqALzHG3g3g/FbfkIiuAfCnAN7hvR4/PkREqnf7DADbABxu9fUlEsnyIZiVxJahO6nkXfVnEioUqowx1F5vwXMjcVVxOisGIqJXAXgfgB95x+r+JIjodgA/B3AOEY0S0QcB/BPcFt73VqSlvhbAHiJ6Cm5PphsYY3WD2xKJZHkTbF29HEWDLgbsqIipittEj2cl1XMl6a7S4K0wyqa9aqqegdbabn8EbmrpnYyxZ72r+vvrncAYe2/E4a/VeO4dAO5oYT0SiWSZY1i+NbAdBnWZDawRk9c0FXFVCVU+14uLFA1LnN+djEG3HCRXUR1DK223H4QbZ+D3DwP4ML9PRF9gjH2ovcuTSCQrmaCffjnGGcqmA4WAmEqIa0rzriTdNSi8q6purS7F0M5PckUbX0sikawCgu6Y5Tj32Q0aqyAixFQFptU4K8l2mIhN+K6k1aUYVo+Jk0gky46gK2kZ2gWULVts6DGNwllJNQxDyQzXLwBSMUgkEknTLH/F4IjmdzFVgW75wedarqSi7k984ymrZfP07ZXUiOUVVZJIJEtO0DAszxiDrxjiqoKC4W/6Zg1DVjCChW22cDmd1oqBiNI1HvrHRa5FIpGsMoJX3UvVFuPodBE7/uZevDhVqHoseKUf1xQUvaByXFVqKoZChWIo87Gep6NiIKJXE9FzAPZ59y8ioi/xxxljt7R/eRKJZCUTDOC20sa6nRyZKuBk3sDhyWrDoFt+xXJMVZD3Nv10QoVVoygv2CNJN23o1uqa3ga0phj+AcBbAPC2FbvhFqVJJBJJJOECt6UxDDylNNjbiOMO2PGCzyqJ+oR0jM+Arl5z0N2kW454/dNSMQAAY+xoxaHqn7REIpF4hIPPS2MYuGopRRoGJ6QYePwgnXBLvHjA3LIdvO+rj+KRgyeFuwlwjc7prhiOEtGrATAiihHRnwDY26F1SSSSVYBp+9XOrQSfC7qF3/zKozg0mV/0GvjGzWMBQYLB54SmiPhBOh5WDCfzBn52cAqPHp4KKYayZQvXEj9nNdCKYbgBwI0A1gM4BmC7d18ikUgiMW0HKW/jbcUwvDhVxCOHpvDYC4tvl8ZdPZGKIVjHoCpik+dr5rUM0wUDADBVMMLpqqYjqqDT8VY6DC1vWmmJcRJuAz2JRCJpCtN2XTV5vTXDwN0/U3l90WvgrxUdYwi7kjgZ4Upy1zxT9AxD3sC63nC6askMq4zVQDOjPb8AoOZvlDH24VqPSSSS0xvTZgtSDLxf0ZR3pb4YGgWfE5qvGDgp4UoKr2O6YKBoWNAUgsMYdMtXDJnE6jEMzbiSdgLYBSAJ4BUADnhf2wHEO7c0iUSy0jEt/4q8leAzNwzT7TAMXoyhZFQbBj3Q4ygeCB5nKmIMM8KVpKOg28gkNCQ0FWXT9jOZTidXEmPsVgAgov8B4DWMMcu7/88AImcySyQSCeC6cfjVdyuKgW/m7TAM3MiUrbBhsB0Gw/YNV1z1mzfwTb4yxsAVQyaugghhxbCKDEMrwec+AN2B+1nvmEQikURiOYt0JeXboRi84LPhVBz3h/QAYVdSZVYSjzHMlkzkShbSCQ1JTYVuOkIxpE6nGEOAvwfwJBHdD7cv0msBfKoTi5JIJCsf22GwHSY23oUEn9vjSoqOMYhWFryJnlZtGHgdA18HY8DobBGZuArTdkS6Kp/nsFpo+pMwxv4FwCsB3Al30tqruJupFkT0dSKaIKJnAsf6ieheIjrgfe/zjhMRfZ6IDhLRHiJ6xcI+kkQiWQ6YormcZxhaqHzmAeOpgr7oWdG1DUM9xeBeM1cqBgA4Ol1COq4hoSmeYrBXVXwBaL2J3mUAroSrFi5t4vm3ALim4tjHAdzHGNsG4D7vPgC8FcA27+t6AF9ucW0SiaQFnjk2h10vznTs9blh4D58u4W227p3rmkzzAfqBhaCCD43MAyJKMUglIuJrJfCOlcykUmoSMZU6JaNgm6JYPVqoZUmen8Pd+7zc97Xh4no7+qd440DraxQeScArjRuBfDLgePfYC6PAuglorXNrk8ikbTGp3+8H3/9n8917PX51bZfLNZ6jAEAphcZZ2joShJ1DH7wORWRlXTmUEY8LhSD5SqG1RRfAFpTDNcCuJox9nXG2NfhKoFfWsB7jjDGxrzb4wBGvNvrAQR7MY16x6ogouuJaCcR7ZycnFzAEiQSSa5shqp42w2/2uabZitN9PRABtFiaxn8yuewYuFZSokIVxLPMDIddxDPdMHAWcNd/uMJNZSuygviVgutupJ6A7d7FvvmzHUetuxAZIzdzBjbwRjbMTQ0tNhlSCSnJUXdrkrhbCeGcCXxQO4CFUMDw3DgxDzG5koN11EzxhBR4JZO+CqnYLjDeM4cjlYMBcNeVVXPQGuG4f/AzUq6hYhuhVv09rcLeM8T3EXkfZ/wjh8DsDHwvA3eMYlE0gHyuhXZWK5dcDcMz/pZSLoqAEwX6rfF+NDtT+LTd++v+bhu8iZ6dsXxsCsprlUHny3bEcVtQ9kEetMxAG4BnBtjcLy6htNUMTDGbgdwOYB/h5+V9J0FvOcPAFzn3b4OwH8Ejv+2l510OYC5gMtJIpG0mYJhRbaJaBci+LyAAjfDcqB5XVkbuZKmCgZyZbPm46KOoUHwOR5Vx+AwoVj6M3H0Z9xmD+mEqxjKpo2ifhrHGIjoCgA5xtgP4Ba6/SkRbW5wzu0Afg7gHCIaJaIPwq2HuJqIDgC4yrsPAHcBOAzgIICvAPjDVj+MRCJpnoJuiavmTsCv+rmrphVXkm45yCY1pGJqwyK3fNkSm3+9dVS5kmoUuKkKCSNh2Q6mvVTVvkwcg5kEAFcxJGLclbT6FEMrn+bLAC4ioosA/E8AXwPwDQCvq3UCY+y9NR56U8RzGWQbb4nklKBbtufqYXAcBkWhhue0CjcEIvjcomKIqwqyCa1ujMF2GEqmXdcwBOcxBD9rraykuKpA826bAVdSfzqgGOJuryTdtMHgxyRWC63EGCxv834ngC8yxr4IoKvBORKJZBkSmkJWZ1NdDJV1DC0Fn20HiZiCgUy8riuJD82pbxicyNtVwWfNNxBcMZi270rqy8TRn3UNQybhKoayl6662hRDK4Zhnog+AeC3APyIiBQAsc4sSyKRdJJ8IE21U3EGs8KV1GoTvbiqoD8Trxt85hPX9DqfwYgwBu7tcNZUQvWD0FrAlTRTNKAqhO6khoEKxWBYDmyHnb4xBgC/DkAH8EHG2DjcrKFPd2RVEomko1SOp+wExiKDz3FNRX8mUbfALV+2xPNroQdaf5dChiE8q5krhqAryfKCz33pOIhIuJLcyufqNt2rhVYmuI0D+Fzg/ktwYwwSiWSFUQgphk65kni66kIUg4OEpmAg67qSGGMgqo6DcOXTKMbQm4pj3CyHFYNlI64pIubAg88xTUFMCbuSuFJY25MEAPSm46IHFOBmKa0mGioGInrY+z5PRLnK751fokQiaTf5UIyhM4qhsvK5lSZ6rmJwYwy65YRcX0H4LIRahoF5U9Z6Uq7XO6gYdNMRNRaAH3yOBRWD7WCmYIr6havOHcE3P/hKnDmUjeyttFpoaBgYY6/xvncxxrorv3d+iRKJpN0UT4Fi4K6khcxj4IphbW8KAHB8tiweOziRx6V/+984PlsKKIZo42Y5DIxBGIZyhSuJxxcA36UUVxVRQ2HaDmZLhjAMmqrgNdsGQ88HVteQHqDFlhhE9Aoi+jARfYiILu7UoiQSSWdpV/D5B7uP411ffiSyNbZwJfGspBab6CU0BRv7XMMwOlMUjx2cyGNyXseBiXxDVxI/3i0MQzgQHTQMQVcSEUFTCKbDkCtZwrAECZ572ikGDhH9FdxuqAMABgHcQkR/0amFSSSSzlFok2HYfXQWO1+ciWyNzdNVeauJllxJtutK2tCXBgAcnfYNA1/vbNEQn8OwnEjjxLOVhCvJCGclBQPI3DDEAy4ly3YwVzIjDUNIMayyGEMrn+Z9AC5ijJUB0Yb7KQB/04mFSSSSzlEwqtM2G3HLz17A0ZkS/vKXzvNfx9uYT87r6E6GN08eY4irClSFWpvH4KWrDmbjSMYUjM74TfKKBjcMZkj5uNlH4St37s7irqBgBlbZilYM3JBpKqFo2CiZdrRhCJx7OqerHgeQDNxPQDa5k0hWJOENtTnF8PDBk7jnufHQMa4UJueraw0Mz3UU07hhaH59PPhMRNjQl8bRgCupZPqGIah8jIg34C0/ohWDLTKmAL9XUizwnbfjiHQlyRgDAGAOwLNed9V/AfAMgFlvHOfnO7M8iUTSCYIbarP9ksqmI+oGKl/nZEStgRlQDFqLisGNMbib9sa+VEgxCFdSyaj6HPvGc7jxW0+I9+Yxhujgs1tdzYlrYcOgKYQpr7iuu4FiWG0tMVoxc3d6X5yftncpEonkVFHQbSRjCsqm03SBm27ZmC9boZoC3zBUKwZe+awpBJWo5SZ6fKPe0JcOjSAtesV5s0UTwdIG3bLx80NT+NGeMXz8mpdhY39aFMkCVF0AACAASURBVL71BILPX3nwMHrSMZRNG0NdCXG+6JWkVSuGSMMQTFeNnaaGgTF2KxGlAGxijNVufi6RSJY9Bd3CQCaBY7OlpoPPuuXAchjKpiN86rweIsqVZNoOiNxupapKrTfRE4YhhVzZEkHgkuFu9rNFIzRDgY/ZBHx3E3eTBesYvvP4USRjChwWzixSFQKR71LSVBIGr15WUrCFxmqhlaykt8MNNt/t3d9ORD/o1MIkEknnKBgWBryGcM0Gn7nLaV73Zx/kvduRisFhiKlunKAVxeA4DJbDxBX5xn43M4mnrIoYQ8kUBW58fdzIFSrSWDMJDTGVkNctjM2VcGiygIlcORQnICLEVEUoB00h5DzXWb2spNXWDgNoLcbwKQCXAZgFAMbYUwDO6MCaJBJJh8nrbm6+plDTioG7nOYDcYZCPcXgtc4G3KvxZmc+GxVprhtELYMbZyib0VlJhh1QDN537kqKawqSmopDE3lw+1Qw7KospoSqhILPnHqGIb3KAs9Aa4bBZIzNVRzr3JQPiWSVsH98Ht9+7KWlXkaIgu4Ol+HjKZuBK4ZgADpfL8ZgO6Gr72CB2/37JnDbL14UQeKo9+FGZWNFLYMfYzCQ1y2RiqqbtjAM/Dt3JSU0Bcm4igMT+dB7BesYAODXLtmA153tzpFvaBg8o5JZZYFnoDXD8CwR/SYAlYi2EdEXADzSoXVJJKuG2x97CX9+59Mt+dg7TUG3kUloXgC6+eAz4CsG03bEFXmtdFXue1cUCrXE+Mw9+/HJO5/B27/wcKiqGQB029vMvY23Nx1DJq4KxVDyDMdcycR82UR/Ou6tL+BKqpjTkNAUpGKqSHtd77XaqFQMn3rH+Xjz+WsAQPRLSsfVkJHgcDdU6jRXDB8CcD7c1tvfgpu++tGFvCkRnUNETwW+ckT0USL6FBEdCxy/diGvL5EsJ3JlEw4DZor1R1SeSgqG5Q6b0dTmYwzeJjvvzVfmfvxkTMHJvFFVeWzavitJUyhU+XxstoSLN/Vi3/g8vv9kuByKGxs+H4GIsLE/LQxI2VMDDnMNEm+F7Qaf3TVxV5JQH5qCZEwBY65b65cuXOutvfbVPu+wGqUWALdvkqrQ6R1jYIwVGWOfZIxd6n39Ba+CBgBPQTT7WvsZY9sZY9sBXAKgCD8V9h/4Y4yxu5p9TYlkucKvsBsNtT+VFHQLmYTmTSFrrBgYY+JqnBe18c+1ZSADw3aQK4VrHAzLdyWpih98LugWZosmrj5vBAlNEQFejm6FYwwAsKYniRM5V5UUTf/5DoMIouuWLdQEr+zWba4YVNHMb21PEpefOeAdr70FcsVQyzDw80/3GEMjrljgeW8CcIgx9mIb1yKRLBtypdqZO0sBn/ecTWhIenOLa3HPs+MoGhYsh4mgLTcI3F2zZSADAJis+HzBJnWq4qerjs25LqH1vSl0p2JCgXCMCMPQlfSfVzJssckDQH8mIc4rCcUQnuyWiCnCNbWxL40dm/twxlAGL1tTu0E0d4NF1TBwkjH1tI8xdIrfAHB74P4fEdEeIvo6EfVFnUBE1xPRTiLaOTk5eWpWKTmteeyFacwVzcZPjIBfEU/VmUR2KuGZRJm4O4WsVvD5+GwJ1//rLvznnrHQc3jwmbuSNg+6weHKOEPZcsRmrCqKUAzHvBba63tT6EpqVYrBCMQFOF1JTQS6y6aDtb1+d56BgCupJGIM4TkNPMYAuFlOXckYfvKx14sW2lHElMaK4fx13Th37eqbPrCkhoGI4gDeAeDfvENfBnAmgO0AxgB8Nuo8xtjNjLEdjLEdQ0NDp2StktMX03bwvq8+im8tMLOIX+kuF8XAN3Q3+KzWDD5Pe66v+bIVeg7/PLy4baunGCo/n9uLiKer+vMYjnlB5HW9KU8JVBgGO0IxJHwDUjQsMUkNAPq4YQhkJVWlq6q+YeB1EY3gAed6huFfP/hK3PC6M5t6vZVEOw1D9dy9xrwVwBOMsRMAwBg7wRizGWMOgK/ArZuQSJYU3XJg2gy58gIVg+dKWjaKwXOzZBMaEppSM/g8V+KuGyusGDzDwpXDlkHPlVShGPSQK0kRhuH4bAmqQhjuSqA7qVW5kirTVQFXMRiW48URbKztSYnHBrMBxSDSVf2sJN6Mj6em8rqIRjQTY1ittGwYiKibiLoiHvrHBbz/exFwIxHR2sBjvwK3UZ9EsqTwq85gZ85mcRwmgrW8IdtSwxVDuoFiEIbBtENxiPkKV9L63hQ0hSIUgz/vQAukqx6fLWFNdxKaqqArqUUohnC6KuAaMf7eZdPBmm5fMfTXdSXZwiXF23i0qhgq24mfDjQdTieiSwF8HUCXe5dmAXyAMbYLABhjt7TyxkSUAXA1gD8IHP5/RLQdAANwpOIxiWRJ4IZhIQNtCoYFnqUZ1YF0KeAuoGxCdQ1DjawkbhiKhh1SDDnhSnI39K6khsFsIiLGEFAMRLC87qrHZktY58UIuhJ1gs9qOPgM+Kokm9SEUelOuhXcuhXtSuKGwe/W2pxh0ESMYfVlHTWilU/8NQB/yBh7CACI6DUA/gXAhQt5Y8ZYAe40uOCx9y/ktSSSTsILu0oLMAzBwOryjDE0diWVTTtkFLlBCL7OQDZelY4bnHegBhXDXAmXbHLzSrqSWlWaa1S6alfS3aomPMOQiqnoS8cxX/bSbjUFJcMvuAv2SuIGYaQ7id50DMOBjqr14FlJPenTTzG04kqyuVEAAMbYwwCq5/lJJKuMxbiSeHwhGVOWTYyBb+yZuIZEnXTVKMXQm/aDxXndQlxzewt1JbWqWQ0hV5LqKgbbYRibLWOdV3nclYyhZNqh1hh6RFZSlhuGnJvRlIqpohVGV1JDXFMwW/J/vn53VV8x/O4VW3DPR18LRWkuHBo7jWMMDRUDEb3Cu/kAEd0ENybAAPw65EwGyWkA36jKTfYUChIsAgvOLV5KipVZSTU+V5RhGMwmhAHI6xa6PN9/NqHh+Gw5dH6wjkEhgu1VKlsOCxgG9/x82RLZRVHpqtzPz2slUnEVvV4rDFcxqJgNpBMLxWDaQnkkY2rdSudKmslKWq0040qqTBn9K+87wTUQEsmqhqdPlhehGLYOZrBvfN4tzlriFgo8MJuOq0hoCgzLgeOwqivpKFfSYDaOsVk33ZRXTwPu5syznQC3UloP1DHwCW7HZv3iNsA3DPMBwxDlSuLB54mc70rq9TbsdExFIqZg1ms5oirkxxhsp251cz1O56ykhoaBMfYGACCiJIBfA7AlcJ40DJJVj3AlLSjG4BsGwI0zNJsV0ylKhg1VIbfjqLdx65ZTZbByNRRDwbBhOwx5rxEf4BqGoCuJP5+7khSvu+rxWb+GAfCrioOpwNGVz+778OBzKq5iuCuBvnQMivdZZr319mfiflaS6ccYWoX3SqpX+bxaacWUfh/A2wGYAPKBL4lkVbMYw8BdSdwwLId+SQXDQjqmhnL7ozKuQoZBKAY3cJvXLRR0C1mvHURXQgvNRuCvx4PPmjePgbfDEFlJAcXAicpKEjGGeS/GEFdxw+vPxK0fcEudEpoqKtMHMnG/iZ5lh+Y6t0IqrkJVSCqGBmxgjF3TsZVIJMuUdgSfhWFYBplJJcMWw+uDiqGSoCuJP85nJM+X3SE5vIFdJqG5oz9tB5rqZzoFeyVZDsNcyYSqkHAN8dhBMGXVsF1FExyXmdBUxDUllJU0mE0IQxVUDAPZOPaNz7ttwW0HfQscu/neyzbh4o29C1YcK5lWfmKPENEFHVuJRLJMETGGBbqSUjEVa7wWDsshZbVg2KIjaD3F4AefrVCMAQgqBt+VBPh9mIRiiAUmuDkM+bKFTNxVK0C0YtBNJ6QWON1JDScCWUlB4ppfWT3gNdVzlY6zYMXQn4nj1WfV7qW0mmnlJ/YaALuIaL/X5O5pItrTqYVJJO1gKq/j2n98CC9NLTwjaDEFbvNlC90pTVzZLocit5JhIe3FE/jVcGWRG2NM1GCUKmIMgPu58gHDwF1KfB40f71KxZDXbXEO4BeuhRVD9GaeTWhCiaQr4iHBADNXMXzdp+MV/2JpxZX01o6tQiLpEIcmC3huLIe94zlsGlhY0DcYY2CM4Yd7xvDg85P4zLsvanhurmyiKxlDMqYim9CWRS1D0bDFxuorhrArKa9bsB2GuKq4LTG8nwFvP5EvW1VZSUBQMYSDz7wlRkG3RLwA8BVDriLGEKUYugKtKZJVhsG/z41XwbBqvpakPq0M6nkx6quTi5NIFgu/yl/I1T6HD3txmHs1+/CBSdz9zHhT5+ZKFrq9zW8gG6+aWRDFg89P4oZ/3QUrYh5yFI+9MI0bb3ui6dGhBcMW4yh5cLjy58PdSCM9CZi26wJKaIrYnOdKJgqGn5XEVYDfGjscfOaVz/mAMQHcWoFkTAkpBt74rpKugEGpdCUFFQZvw+0qhoUHn09n5E8sgmeOzeFDtz8ZOahcsrLgG5Te5PjKKIxAYLZsOMjrllAPjZgvmyLd8cyhLJ47PtfwnIcPnsTdz47jJ/smmlrfzw6exI+eHsNcyYTjMNzwr7vw1n98CL9+08/FBv/Ze/bjh7uPA3BdSXwcJa8zqGUYeLO62ZKBhKYII8d9/V0VhqFQYRgSsWrDEHQlAahqvW3UMAz8vJhKVTOYw66khFiLLhXDgpA/sQh+fmgKP9x9HAcnZDbuSqcsqpYXrhiChqFk2pgvu24W025sGHJlS1xlX7qlH4cmCw0zk/jV8zd/0dz8B36VPlsykSubuPvZcUwXdPzihWkcODEPALj9sZfwX8+MAXDdPakKV1JlVpJQDNwwFE3XHeYZhnHPMFS7kvxhOsHXV8md+VzQLWTilYYh3GG1VlyA/xyjqpeDz+/P+FXSRcNGvxdzkDSPNAwR8A6NB6RhWPHobXAlRRkGoLn01VzJFFfZl25xG8ftfHGmwTnu6z/4/CSOnCw0fA9eWDZbNDDj5fK/9eVuB/u5kgnG3DRRsW7TFptzsoZi4Gm2fCDObNFAIuYOu1EVwp5RV/nwsZaiLbZoXlcZfFZg29UxBsDd8HMVwed6rqRKNxLgF8OlYqowUvvGXKPIR49Kmkcahgh4IdNB72pLsnIRimExriTb3zTLpi2u0BsVvDHGvKwk9wr2gg09iGsKHn9huu55ubKJjf3ujIPbftE4jJf3WlHMlUzRFmKzF2ifK5lekzo/y6igB7OSPMVgNlYMCc1NM/3l7euxyzNu2QauJG4Y3CZ67myKSldSd6ViMG0kIoPPnmGIaCkSnLmQjrnP2zuWAwBsWuJK85XI6ddovAn4IHGpGFY+nVAM+XJzhkG33AIrvqElNBXbN/Ti8YaKwcQZg1ls7s/g0cP1jQjgK4a5kil61AQNA9/k58smbMftYZSuVAxWdIyBT0qbLhpi8tln3n0hrj5vGN/bNYrtG3sB1HEleRu2QuS9ty1UBqc7GROtMgBXMVQaD6C+YuCupFRMFcV73DBsXmA22umMVAwR8H/456ViWPH4WUntCj4HFEMDVxJ3xwQngF26tQ/PHpsToyejmC9b6EpqOGdNFw5O5BtmG/H1BBUDv0oOGoZcyRJ/2366ai1XkgVVIVHQNlcyxeZLRLjm5Wvx1esuFYHeuKYgripiCFCVYlAIhu3AYQhlJQHVMYZaKabZhPtzjFQMsYBi8B4/PlfGQCYeSnOVNIc0DBHwGMORqWJoU5CsPLhBWFTwOZCdVjCadyVx102wCduOLf2wHIanXpqtc56bybRtOIuSaYuOpLXgV+mzRVO0nh7IJJBNaK5hKPqKgbfcFsHnOq6k7qQmlAVjaNilNJNQkecFbhUtMYKdW6uzkqqDz63GGPja0nEVSU2FV1i94NqV050lMwxEdMSrnn6KiHZ6x/qJ6F4iOuB971uKtfErQdthODLVOPjXCnc+Obqk/XLmiib+befRJXv/U01b6hgCFwfB312j1+QB1WD+/cvX9QCor0bd2ocYto1kAQAHJuor1/mybxhmiiaIXGPUk4qFFINuOeI2d+doqhtQngnMMgBcw9CTioWuzhsbBs0vcLNsxFSC6hkEra5hCA/rCY7jDMKD1pVVz0A4+KwoJIzHZhlfWBBLrRjewBjbzhjb4d3/OID7GGPbANzn3T/llEy/bL+d7qSZgoE//s5ufP+p4217zVb50dNj+F/f24PRmeUxNKbT8E19sXUM3LURnGvcyJU0F+FK6vOmjlVuxJyyaYu4xFnDXQCA50/Uj3UFXUlzRQPdyRhUhdCdiiFXMkNVxSfEPAN/c97Ql6r6e+CGIbgJNxpykw10WA2O9QQgDAQQ7UoCfANX0C1RgBeEZ3fVS1flhowrnc0yI2lBLLVhqOSdAG71bt8K4JeXYhFFw8Z5a7uhEHCgwT9lK/B/msrh56cS7naY7kD75weenxRui+VCpWLYO5bDwQZX4JXoliPcQcHK5YauJM8wBNs2a6qCnlQMM8Xonz9XGfyKf6Q7UfdvkHm1AQAwVzIwWzLFyMuelBZSDECw/sDfXDf2pzE647qrjs+W8NWHDuPQZB7dqVjIbdNIMWQTWij4nIhFG4YoxQC4/xem7WCqYETOZRYxhgaupOB3GXheGEtpGBiAe4hoFxFd7x0bYYyNebfHAYxEnUhE1xPRTiLaOTk52faFlU0bvekYNg9k2lrkxg1D5WzcUwnfzGpdsS6UomHhd//lMXzrseaKsk4VlQVuf/Ufz+Cv/3NvS69hWA66U+FBMcDCDAPgqoZaP39ew8CvjrcNd9U1ZLrlwPKC09yVxCebVbqSAL9iOagENvSlcNRTDF+8/yD+5kd7MTpTwjkjXRWupPqKIRNQDLppi+I2IOxKqlQMfL0zRVN0nx3urjYMXXVcSYnA+M7gc6RhWBhLma76GsbYMSIaBnAvEe0LPsgYY0QUmY7BGLsZwM0AsGPHjrZPkeNNxs4YzODQZPsMA89EKdTJSOk0fDObrXHFulDyZQsOgxjEslAOTuTRldRE/vxiqcxKmi2aTVUsBzEsR2zuQcPQKMYwV8Mw9KbjNX/+XE1y99O2kSy+8/jRyNGb7vP9v6W5knvFzWchc8OQizQM/r/+xr405ssW5oomnj8xj0s29+HWD1wm2mYQucHnZIOeQ9mEJgxM2bJDLp96ioG3JB+fK4M/a7ir+vfPDUNlAz3Ab71RrRikK2khLJliYIwd875PALgTwGUAThDRWgDwvjfXLKbNFL25vGt6kmIwSDvggTme0rcUcL/4TJtdSXyUIp/Ju1BuvO0J/PV/PteOJQGodiUVDbvlgTuG7SAVUxFXlbArqYkYQyqmVmXYuIqhliuJZzL5iqFo2Dhew+By100y5g6qmSkGXUnNKwYAODpTxPMn8jhnTReyCQ1EBCJCOhbuq1SLSldS0JDUMwxrhWEoCcM7FOFKysQ1DHclsLm/erNPBILP7ufTkImroqGepDWWxDAQUYaIuvhtAG8G8AyAHwC4znvadQD+YynWVzZtpGIa1nQnMV0wRHn/YhGKQV86xcA3yOk2u5L4Z+KjFxfKZF4X/X3agS4qn93PXTAsFM3Wfv68qVsypoQUQ7EJwxA1FrIvHcdMoZYrqVoxALWLLbnrZl1vCnNFEzNFI+RKKpsOJud14Zoaz/kzkzl8BvWTR2cxVzKxbTgbeo9URZV0LUJZSXWDz2ED05+JI64qGJsriwuxqBiDohAe/rM34jcu3Vj1mMhK8pTQxv4ULtjQIwYCSVpjqVxJIwDu9H5pGoBvMcbuJqLHAXyXiD4I4EUA7znVC2OMoWhYSMUV4c6YyLVngLtQDMsgxtBuVxJ/3cUoLMYYciVTzAJQI1wnraJXuJKKuh3ydzcDz0pKxtTQ52vGlcSv/IP0ZeI1FQN3DfGA7JlD7ib9wmQBbzin9vM39KVxeLIAo8KVBLhKYENfGs+N5XBizgs+x8NZSQBwv9fNdZuXDcVJVRTD1SKbUFEwLDgOQ9m0Q+4qNbBBVzbRIyKs6UlibK4s3ovPVKgkqr4BqFYMn3rH+XBkCdKCWRLDwBg7DKBqygljbArAm079inx4dWY6rmHEk7gT8+W2GAauGPJLqBiEK6lDimFyXgdjbEFXamXTC6Q6DMdnS235mXODoFs2DK9FRaMr/Up4U7fgVXZXUmsYfK6tGGLu2EnLrgro+llJ7r8mPz9XI5ON/9zX96bEMe5K4plUx2ZKeMPLhvHcWE64woKZPT2pGLoSGn528CQA4OyRsGLgvYeaUQyMAUXTRtl00J+pdiWl42pkrGRNTxLjc2Vkk5qrIBq8VyX851g5mU6yMJZbuuqSwzfOZEzFiJcZMT7XnjgD98OvxuAz32x1ywnlzbdCcPNrV9CfZyOVTUf8bkum3fRQG8B3JfHNNBN3p7FFxRjKph/DmCtZkYaBX9HPRhjnXMmEFijQUhVCOq7WVJn8IoNf9QOuqwrwjYrlMAxk4kjHVdgOE0VgHCLC+r6Um5ab1Kr8+8kWXEmAa6zKlh2KSWiq+35RPZAAYF1PEmO5EiZyeqQbqRE9qZjXwqP1cyXVSMNQQbCXzIiXGcEDdouFtyNYyhgD37TaXccQ/EyTC4wzBLNnXmii3XQzBIPP3CAz1lqLDN2rxOWulGxSQyqmRiqGG297Ar/3jccBeC23a8QYAES6k3ifpKDiChaOVT0/wjD0VCgGwFMFddI9uTrbNtJVpfZE8LnBVTh//bxuQTedUIxBofqGYU1PCuNzZUzMlyMDz40Y6krgxx+9Ete8fE3L50qqkYahAn7lm46r6E3HENeUthkGrhiW1JUkFEO76xj8TXKhmUlBpXF4sl2GwXUlWQ4LKZJG7iTGmFAVhufy4Vfx2YSGZEytijEcnszjvn0ToiCtpivJGyQTZZxz5Wpjkk1qwgBUEulKCgSfOd2pmIhbpBPVGzw3LJVuJCAQfG6QrspjBwXdcoPPoToG93ZlDQNnbU8Sps3w/In5yFTVZjhruKstcSmJNAxVBF1JRISR7kT7FIPhp/I1O8+33fgFbp1xJQELD0DzjTumUtsUg27Z4HvFdN7/zI1STe944hgu+7v/FnEJnpUEANmk20OoUjHc5k1cO5nXoVtus71aWUlAbVdSd0U30K6EVtuVVLagkF8LEHz9nlqKIVa9OW/scxXDWRWBZyCYldS4wI2vyTUM1VlJlRlJHL7+sulEFrdJTi3SMFRQ2ZZ4TXdStBFYCHfsGsXVn3vAa13gbySFJaplKBt+Pv9iGstVEmwjvdCUVe5KOndtNw63IcbAGEPZ9IvTTgau0Bsphh/uPo6TeQOzRUNkJfENsivhuZICr1EybHxv1ygSmgKH+YqnnmGo50oKkk3WdiXxGco8bgEgVMfA6Qkohqi21bxC+JyRasPg1zE0LnDjaypb0XUMvK1FJWsDhm0hMQZJe5GGoQK+YXC3wXB3clFFW88cn8OBiTwKhh3aPPNLFIAumW7XS6C97qSCbnsuFmXBPy+eennRhl4cnyu3XIhWCa9h4JvmdD5Yg1D75182bfzihSkAbvaWw+ApBt+V5CoGX/Xd89w45komfvtVmwH4zRejg8/usUjFUK5WDNl6isEzDJm4Ck0hEPmprjFVERc4QcUQddX+urOH8IX3XoxXnzlQ9ZjfortBuqr3+nMldyBQ8PmaMAzRr8EHAgHRVc+SU4s0DBXwzYj/M4x0JRflSuJVpzMFI3TVt1QB6JJpi/qMdrqTioY7LnK4a+HV4tyVdJE3FWyx7iTeUZX77IM+/XpGZ+eRGRGb4L17gllJPPgcVFyHJvJQCLj2AnfWcj3DkIy5w2Siqs9zpWrF0JWM1VYMZXeGMhGhNx0T2TmcnkC8oVvMM6h2JWmqgrdftC4ylbTZGANXDPz3H3QlKcKVFB1jGMjExQXLQoLPkvYiDUMFJa8qlm8Ca3oSKBh23Y6ou4/O4le+9LPIzYa7R+ZKJoqGLf5p59tU5Pbo4Sm856afNzVQyC08crDOuzprp2EoGDYyCbdlwcJdSRbiqoJz17rujMMnF+dO4plHPBg71cCVdOO3nsBn79mPBw/4jRmFYVCVUPC50pU0NlfGcFcS670g7v5xd+1RhgFw3UnTka6kiOBzQqtdx2D4M5S7UzHxWTlBw8CVRC0/fy1SIiup/nYxkImjK6Hh6dE5AIhsolcrK0lRSFywSFfS0iMNQwV+VpL7B8z/WE/UcY88cmgKT740KxqIBeGKYbZooqBbGPLyrNulGB5/YRqPvTDd1GbMXStre/0B7+2ixBVDd2JRiqErqYmxlEenF9eQjysG7roJKoaoWpIH90/iCz85iNsefVFsTie9gHXQldSd1KqCz2NzZazpSWIgk4CqkBiuU8sw9KZjVT9/y3ZQMOzq4LMXY2CsuvZivmyJq/DeVCwUawB8tcSL2IDodNV6NFs0piiECzb04LEj7pzqqLbbtRQD4McZZPB56ZGGoYIqV1J341oG/thUvvoKULiSigaKhi3+6NtlGGYDhqcRfCNb56U2trOWoaDbwpU0uYgYA0+r7E3HIg1tK9RTDJXqzrQdzOuuYikYtnAJhVxJcd+VlKxSDCWs7UlCVQjDXQm8NO2uvZ5iqFRsfjuMiuAzryiOUDl53Xc9feSqs/HRq7aFHufv3x2qY2it4QHvP9SouyoAXLihV/xdRWUl1VIMgBtnyCa0ltcnaT/yN1BBqSL43Ixh4FfrUa4ZoRhKrmI4f103gPbVMvD3bMYtJAxDD1cM7Y0x9KbjGOpKYF63UPI61LZCzpszDLjpk3x4zELhMYAeHnyu40riLr8/euNZmC2auO7Vm3HLI0fEKM9QgVsihpLhprFatgNVIYzNlfHas4cAuAkLY15PoqgCN8BVDJWznOcjZkQDflA3r1tVV9wF3Xclvc57/yBcKfCJbkDriuHN543g5LweqpWoxYUbesTtpBaVlVR7y7nu1Vvwqojgt+TUIw1DBSXTv6ZQDgAAHqRJREFUbbLGe7WIthh1DMO4twlEXYELw1BwFcOQl3HRLsPAJ6Y1c/XPjV5P2m2P0M5+SQXDxvo+VQQOJ+bLLffCd11J7ua1oS+F/eOL67LKA8g9geAzryKurEHgv6dN/Wl8+E3rwRiDppDvSlLDwWcx9N5y4DCGomELV8ia7gR2I+x+qqQ/E6/6nYk+SRGKAXANx0h3+HXy5WpjEeQ9OzbivLXuSfXSVesx0p3EH199dlPPDRmGWHVWUr21XrK5D5dsXpIx75IKpCupgqJhhxqM8b7uJ+drb7w8/lCZZaJbtticJuZ1WA4Tvut2uZK4UmjKlRRQQ33pOE7mdfzdXXvx6OGpRa+jZLjdNLcOusZgISNRXVcSb5ucxuhsqaWeRpVwxeBPCDPQm45Boep0Ve6S4+0kiNwrbN5m23Uluf8uvI4BcD83vzDgKZdcZdZyIwFuCm2u7KZ1crjbqr9ihkCw1UQQxhjyhiViB1FctrUfH3jN1tDrVHY3bSfre1NiBkLQMJw90oVf37ERl5/R37H3lrQPaRgqiHKBDGQTmC5E+80dhwlXUmWWSXBAynHPbdCXjiGuKm0b1jMbiGE0gl8lp2Juu4//3DOGmx88jI99d/eii90KXvD5/HXurOw9o7Mtv0aw6ndjXwqG5YQG47SKMAzeZs8YvJx/rcqVxJVXcDPvSmrClVSZrso3vbJpi98tVwzNGIa+dAyMhf9GeP1H5fQ6XhRWWctQNGz3MyWb2+i5YWhVMbQCEQnVEIxJJGMq/u+7LsSAbHK3IpCGoYKSWW0Y+jPxUOAyyEzREKMiKxVDsCkc9yenExoyCbV9wedi68HnVFxBXzoO22F43dlDODZbwq2PHFnUOoq6qxjScQ1nj3Rht5ey2Ao8Kwlw5wsAwOgiAtB+gZu/QafjqptRVGkYvN9VMN2zOxkLuZIu3tSHq84dwTlr/FnIJdNXDGtaMAz88wVrNbi7sjIrx68oDv+OuYKo554JcuZQFm85fwSv3NrZq/YLN7h1KI3mN0iWL9IwVFDpSgLc/OyojCMgHHuonIrGNxsi3zBk4po36WrxhsFxmAggN6UYAn2grj5vBO++ZAO+/juX4g3nDOGf7j8YMmyMMXz1ocNi06sH7yfEZwRfuKEHe0ZnI9Mr671G2XR8xdDvjZtcRMqqCD6nfNdMJqEhHVdFQ0MO/zkG0z27UxoMr6dVXHMHN331uh3oTsZEALdk2BibK4PIr9hd04Rh4FfVQWV1IldGfyZelRbKjWVl7Qu/8KgX0A2SjKm46f07Oj4H+W0XrsVrzhoU/ZckKw9pGCpwJ09VK4ZawV0u/3tSsSp3EzcM63pS4p86nXB7+dfqlnl8toTv7jwq7n/38aPCVVFJ3rDAXdTNBJLLAVfSda/egk+/+yKoCuFP3nIO5ssWfrjnuHju0ekS/uZHe/HvT442fF1ucNLeBnXhhl7MFM2WsormxYAadzNd38trGRauGMqeYuhOauCdpDNxDam4hlKNGEMw8NsV6OtTOTiGXw1zxTCYTVQlLNQzDCPdSYx0J7AnoKxO5MqRxV3BHkQc22H4v3fvg6oQXramu+qcpeTskS588/de2VGXlaSzLNXM541EdD8RPUdEzxLRR7zjnyKiY0T0lPd1bafX8sLJAn6y74S47471rDAMWdcwRF0Bc8Vw7tquqjm+3DDwBmWAuzFl6yiGbz9+FH/6vT2YKRiYLhj40zv24JuPvhj53NnA+0W1V6jEdyWFP9/563pw1nAWP9ozJo7xGoJaRikIn6GcDigGAKFNrxG5ihz+VFzFYDaxqJRVPtYzEVNF1W464bajqIoxlEx0JTRoqv8vERzLWVn1mwoYhrFcWaQAAxCT/+oZBgC4YH0vdocUgx7qksrJBhTDc8dz+PSP9+GPvvUE/nvvBP7328/DOWuqG99JJIthqRSDBeBjjLHzAFwO4EYiOs977B8YY9u9r7s6vZAv3HcAH779KXHfdSWFpflAJg7DdiJTTHl9w8vWdFepCh7QDBmGhFrXlcSH3BydKYqr5Vo9g2ZL7vt1JbW6rqRdL07j4MR8VY1GkGsvWIvHjkyLLJxRYRgau5J4p1huGF62phtxVcHu0Vnc/cx4Uz2PhGIIVP1u6EstqsiNK6RkzE8bzcS1aMNQrG5FEVxLXA3/zLhxLRs2xmZLoQ29K6Hhsi39uHhTb931XbShB4cnCyJNdTxXFsOhgsRUt+V3Xrfw+fsO4Iv3H8J/7z2B379yK95/+ea67yGRLIQlMQyMsTHG2BPe7XkAewGsX4q17D8xj7xuiU2/HBF8Hsi48j4qznAip2MgE8dwdwKlwFhHwB3tCACb+n2fLlcMteoYuGtqdKYkrpZrDa3h7qMzBjN1g88fvv0p/L+79wvFEBUUfNsFa8EYcPez4wB8335TisFzy/A0yLjm9jv66kOHccM3d+Evv/9Mw9fIlaqLuzb2L67ITbccELmBY97pkyuGyuDzbMkMBakBP++ff6YgqQpXUrA7KBHhuze8Cu/cXv9P+kKvWeAzo3OwbAcn87pwQ1WSTcQwX7Zw+GQeV507jAN/ey0++bbzFjRbWyJpxJLHGIhoC4CLAfzCO/RHRLSHiL5ORJHVLkR0PRHtJKKdk5OTUU9pCtthODjh5tvzIGvRsEX/eU5/1g1IRmUmnciVMdKdRH9Ej/1c2UQmrmIw6wc00wkVmYRa2zB4V+xHp4viavmFqUJkPj8PmG4dzCCvW5GN9EqGjWOzJYznyiibNoiim6GdPZLFGUMZ3OW5k7hiONbExiwUQ6A52xtfNoLBbAJXnDWAnx+ealiAx6+ag+0gNvSlcHy2FMr1b4WyaSOpuQOXeOpkxsuc4u4vzlyEYQi6kmoZhsl5HfO6FekCasSF612X2+7ROUzmdTDmu6Eq6UpqyJVMHJkq4oyh6ilrEkk7WVLDQERZAHcA+ChjLAfgywDOBLAdwBiAz0adxxi7mTG2gzG2Y2ioug1AsxybKYmUxgnPJRSVrsoLdqI2N9cwJNAX8Rw+2jGY6eIqhljNQT28JsJVDO7mbFhOVfsE/voAsMUrKuOupSDcjTPuzTdIe5PpKiEiXHP+GvzihSkUDUtcqc/rVu3OnrqF3UdnRUfaYI+bj1y1DY998ip84q3nwnYYfuwpkVrwDJugYtg6mIHlMBxa4NCesukPi+EqqVa66mzRqIoJBF1JlcY06f2NPPHSDABgc3/rGTh9mTg29aexZ3RWFElGuZIANwD9/Il5GJYjigglkk6xZIaBiGJwjcJtjLF/BwDG2AnGmM0YcwB8BcBlnVwD75kP+EHkqAK3frHp65gvm0JlAK5hWNOTDDwnbBi6U7HQlWgqpiKbUFEwqrtl2g4TefNujKEkesxE+el5sJtvFJXB7+B5J/M68np1YD3IKzb1wWHAs8dzODpTFNkwtdxJ/3Dv8/i1Lz+C8Tl3U8tEvPb567qxeSCNu54eq3osiOgTFFAMfGjMQwdOVj0/r1sNW2aUTVukfvJOn5mEhnRMrTLMrhGPrjgGaiuGn+ybQFxT8Jptg3XXUovtG3ux68WZqlqISrIJDQc9A3mGNAySDrNUWUkE4GsA9jLGPhc4vjbwtF8B0Ng5vQgOhDZ4HabtwHJYlStJxBgKBj5/3wG8458eRsmwYdoOTuYNDHclI8c1csPQl/ablykKIVOjW+Z0wRBuE64YLq4ztGa2ZKAroWHQqyaNCkDzEZkOc41NvaKjCze6ro3Hj0zjRE7Hji2uJy/KMDgOw4+eHoPlMHHVnI7IpyciXHvBWjxyaAr3PncCDzw/iQeen6yqj8iVTRCF2zVs6EvjjKEMHny+2l34v/5tN375iz+LdJ/NFU2Mz5VD4yV5Q7d03IsxmLZwzzHGariSeHsMv9cPJ6Yq0BSCaTO8dttQKB7RClecNYCJeR0PeTMgarWczibdvxkA2DokDYOksyxVE70rALwfwNNExFOC/hzAe4loOwAG4AiAP+jkIg5MzGNNdxIF3cKJXNkf6xmvzkBJxVRM5Q08PTqHomHjsSPTopBpfV8qUjHkSiY29qfFlSh3tWQCeenBqlXuRtrQl8LoTBGMAW982TD2jc9HzkCeLZrozcQCoyJru5IA4MjJYt3OmsNdSaztSeLuZ1y3zyu3DuCn+ycj4wxPjc6KDqK7XnQNQ5RiAIC3X7gOX/7pIfz+N3aKYxdv6sWdf3gFAPfK/qf7JzGUTVRNEHvttiF8+/GXQsPldx6Zxn95azw0mce5a8N5/H/yvd04OJHHWcNZcU4wK4m3kS5bbrV20bBh2qymKymuKpHut1RMxbxu4W0Xron83M1w5TbXFfqD3cehKoTBTLRh4P2QuhKamOkhkXSKJTEMjLGHAUSlU3Q8PTXIgRN5bBvJ4vhsCSdy5apZDEH6M3FM5XXsHc8BAB58fhI9qRiI3HbH/PZMhSvp5QFXEp+cxa9EZ4tmqC8ODzy/YlMffrDbLTbb2O9eNR+OUgxFA72puDBKUUVuh04WxFCY43MlXLC+p+o5QS7c0IMfP+vWdVy8qRcxlXAsImX1rj1jiKmEpKYK41PLTXXeum7c+8evFbUK337sJXz/qWPQLRtxVcHH79iDp4/N4ab3X1J17pXbBnHLI0ew88gMXrNtEIwx/N1de0XK6d6xXMgwzJVM/HT/BEzb7Y7KVQxXDjz4D3iJBnFNFLdVTj/jwedKNxInFVehWw7edO5I5OPNsK43hbOGszg4kcfanmTkaE3Ar2XYOpSRmUiSjrPkWUlLheNlJG0b7sKaniTGc2URZI1qMTCYjePpY3OYL1sgAh46MIm7nh7Djs19GOl2B7T0pmKhRno8+BxTldAAkq1eSwKuAgq6BdN2xICbYOvhjX1pbB3M4PBkAWZFLQVPsYxyYwGui+SFybzojcNY4/41vM8N4NZfrO1JVbmSGGP4r2fGceW2IbzcMzSaQoirtf+cto10ibbKb3zZMEybYe/YPH76/CS+/9RxfOzqs/GW86uvvC8/YwAxlYSrZeeLM3jipVl84tpzEdcU7B3LhZ5/394TonfVgYm8cCGFFEOgMyoQ3UAP8NNVa4207M/E8bpzhqomrrXKaz3VUNk8Lwj/m5TxBcmp4LQ1DMdmSyiZNraNZDHSlcRETsezx91K3bNHqitJ+zNxHPLqCa45fw2eP5HHvvF5vPXla0PP4QFg03ZQNOzQzF3uajlz2GtNPZEHYwxv+/xD+MyP9wtX0is2+YZhQ18KWwczOD5XwlWfewBv+uxPRTHYbNFEbzqOZExFMqZUVT9PFQzkyhYu3dIvgthRxW1BLvIMQ0wlDHclsa43WWUYdo/O4dhsCddesFZcrafj0dlOUfD8/adHZ3H/vgmkYiquf90Zkc/NJDRcsrkPDx90A9C8mvqa89fgnJEu7B0LB6DvenoMa3uSYiPlQWdex5BJqMJAc9chz+bqqaxjSGiiDiKKr163A59+14VNfeZ6XHm2G7iuVcMABBTDoExVlXSe09Yw8Jm824azGOlJ4kSujN1H55CMKdg2XP3P1+/5fomA37vS38TeesGawHPimPL6JfFUUm4Ygimt6biGjf0pHJjI4/hcGUemivjp/klMzOvoTmo4IxBc3NCXxjkjXWDMveI/kdNx0wOHAXBXkvv67qhIE5btB2O5i+fM4azowdPIMFzgtbNY35uCqhDW9VYrhruedt1IV587gnPXuka02Q6fgDtBbjAbx+7ROTz4/CRedeZA3XnCl2zuw77/397dR0dV3gkc//6SSYYkMySQSUJICElIBCIQwIgoL1uLpSBVK8uqKKtWz+52t27b9WxbPdKt69G1lm17Fk+3tV3fa7fuVj1yTml9wdauWi3IW0BepbEQQoCsvEdekmf/uM+9zB1mkhAzmXHn9zknJzfP3Jn88szk/u59nuc+z76jfHTaaTqKhIKUhIOMLw+zpe2IN7rryEen+d32g8yfUO7ddXz2isHtfA54/Szuus9HvKYk/6ikrCwhFAwkbEqqHJZ/zhrL/TG9ppghOVnejKvxuH0MtdrxrAZBxiaGcSOG8sC1Exg7IkxZOMiZbsNvt+1nwshC33w5rmJ7k1p1cQFTq4ooGxrkotHDfHe8Di/IpfVQpzfKBc4mhu9eN5lvXdXg7VtfGmZH+1E27nbmytnWfpQd7ccoHTqEgmCA4oJcIqEgebnZzL1wBE/dNo1X7pzNVY0j+Y83drH3UCeHO097I56G5efy5s6DTL7vFe5dsRk421RVGymg1DZT9DaxWWFeDrUlBd4MnBVFeew78hGnbcIxxrCyuY0ZdREK83O8K4bzmTDNmbO/iNe27qel4wSzehnqOamyiK5uw+a9R2yfgpOMxpcPpeP4KQ4cPcneQ53c+ewGTnV1s2DSCKZVO81n8Tuf/U1J7l3jsVcM4HRAJ0oMAyUvN5tffPEy/u5TYxLu455U1MU5aVFqoGXs0p4ji/K46RJnnhl37HhLxwk+PS5+R6LbwTu+PIyI8OgtF5/TF3H52FJe2tzO6pYPvekn3INN7E1J9aUh3thxkHW7z06i9s4fO5he64zdryrO93rns7PEW0/4658dy0ub9nHbE6vpNmfXM46Eg7zXdoSaSAFPvNVCeEiA17cfIDfgnImW2SuGvsyR/+83TfXO4CuK8ug28EHHCepKQzS3HmbPh518eY6z6Hx9WYhAlpz3qmCTKgt5bet+4OzInETc5q11f/qQHe3H+MKMagAvKT23tpUf/GYnZ7q7uWv+OKZWDePUGWP/XuegHrLrHrtTYkB0U1L8zmdw7mUIZCe/s3dCL4MCPtNQxmO3Np0zAkupZMjYxBCtNKrTr3FU/H9QLzHYKY7j/SNfPXkkD6zcwtNvf8DeQ52UhIPemWus+rIwp7q6+eXGNupLQ7R0HOd0l/HWTH7g8xPjPm/U8HyWL57MUjv/kHtX9tIF4zkwu5bptcX81VNrePi1nRTm5bBs0SSys8RLfn1ZCD56GufZF5SQJfDc2j18Y944VjbvI5AlzG1wEmgwkE1daajPawK4JkU1WY3ppXmkbKjTdPTCulZOdXV7B0f3vXjo11spLxzCf/3NpYyydyC7I6rc/oTFl1TROKqInOwsr+wX7+7m7uebiYRy7b7n1k1Rfk6/p+QYSMFAdsKTFqUGmiYG/KNBEg3ndMeO93TGlp8bYNFFlTz+ZgsA/3LtxIRt724/RuuhTm68pIrwkABr/3TI6wtoGJn498ybUM6lYyKsbG7zRvJcUBb2Os2XL57C82v3sGBiubeUovs39tbHEGtkUR5XjC/j2dW7uX1mDS+ub+Wyuoivbf3BhRO9zu2+ckc/zaqP9NppLSI0Vhby6hbnCsN9Dwrzc6goyqPj+El+cnOTlxTAuTL68V82McbOKxQJBb2rLjcBvLS5neKCXLbuO0pJOBg3jqULGtIiMSg1mDQxgHcwDg8JUJ1gdasZdRHuvaqBPxvbc7PHkumjefzNFupKQ1zXVJlwv+i24sbKwqjE0LfJ2Arzclg8rSruY6FggJsvrfaVlfWxjyGeJdNH8/J77Vz98BscOHqS5Yun+B6fUhV3rsMeRUJBvnddIxcnuKKKNamyiFe37Cc3O8vXAXv/tRPIy8mOewV3+bjSuK/lNntVDstjxR0z2bz3MGe64h/8e2viUer/I00MONMbREK5jB0RTniDUW4gi1tn1PT6WmNKQjy4cCITK+J3YrsKggEqivJoPdTJpMoiiguCPMKuhFMifFzuUMj+rMM7sy5CdXE+LR0neHDhxD4fzHuzcGrixBnLbXqqLwuRE1Wvl4+Nf/DvSWF+Dt/8XAOXjy1heEFur30cSmUaTQzW0gUNVAzL633HPkh0Jh+rvixEx/GT1Jc6U15/eU49n+rHga4v3CsGtzP2fGRlCcv+opFdB45x/cV9+9sGmtv0NFCdr7fP7D3JK5Wp5HwWbE9HTU1NZs2aNb3vmIbeev8gLQdPcOMlyT/YdnUbvv/KdpZMH92vtQPSwSOvv89lYyLevRZKqf4TkXeNMU1xH9PEoJRSmaenxJCxN7gppZSKTxODUkopH00MSimlfDQxKKWU8tHEoJRSykcTg1JKKR9NDEoppXw0MSillPL5xN/gJiIHgA/6+fQIcHAAw0kmjTU5NNbk0FiTYyBjHW2MiTtR2Cc+MXwcIrIm0Z1/6UZjTQ6NNTk01uQYrFi1KUkppZSPJgallFI+mZ4YfpzqAM6DxpocGmtyaKzJMSixZnQfg1JKqXNl+hWDUkqpGJoYlFJK+WRsYhCReSKyTUR2ishdqY4nmoiMEpHfiMh7IrJZRL5iy+8VkVYRWW+/rkx1rAAi0iIizTamNbZsuIi8IiI77PdhaRDn2Ki6Wy8iR0Tkq+lSryLymIjsF5FNUWVx61Ecy+3nd6OITE2DWJeJyFYbzwsiUmTLq0WkM6p+f5QGsSZ8z0Xkbluv20Tks2kQ67NRcbaIyHpbnrx6NcZk3BeQDbwP1AK5wAagIdVxRcVXDky122FgO9AA3Av8Y6rjixNvCxCJKfsOcJfdvgt4KNVxxvkM7ANGp0u9ArOBqcCm3uoRuBL4FSDAdOCdNIh1LhCw2w9FxVodvV+a1Gvc99z+n20AgkCNPU5kpzLWmMe/C/xTsus1U68YpgE7jTG7jDGngJ8D16Q4Jo8xps0Ys9ZuHwW2ABWpjeq8XQM8abefBD6fwljimQO8b4zp713zA84Y8zvgf2OKE9XjNcBTxvE2UCQi5YMTafxYjTEvG2PO2B/fBioHK56eJKjXRK4Bfm6MOWmM+SOwE+d4MSh6ilVEBLgO+M9kx5GpiaEC2B318x7S9MArItXAFOAdW3SHvVR/LB2aZywDvCwi74rIX9uyMmNMm93eB5SlJrSEbsD/D5aO9QqJ6zHdP8O34VzRuGpEZJ2IvC4is1IVVIx473k61+ssoN0YsyOqLCn1mqmJ4RNBRELAc8BXjTFHgB8CY4DJQBvOZWU6mGmMmQrMB74kIrOjHzTOdW/ajIsWkVzgauC/bVG61qtPutVjIiJyD3AGeMYWtQFVxpgpwJ3Az0RkaKrisz4R73mMxfhPZpJWr5maGFqBUVE/V9qytCEiOThJ4RljzPMAxph2Y0yXMaYb+AmDeInbE2NMq/2+H3gBJ652t2nDft+fugjPMR9Ya4xph/StVytRPablZ1hEbgU+B9xkExm2WabDbr+L025/QcqCpMf3PF3rNQAsBJ51y5JZr5maGFYD9SJSY88ebwBWpDgmj21LfBTYYoz5XlR5dBvytcCm2OcONhEpEJGwu43TAbkJpz5vsbvdAryYmgjj8p15pWO9RklUjyuAm+3opOnA4agmp5QQkXnA14GrjTEnospLRCTbbtcC9cCu1ETpxZToPV8B3CAiQRGpwYn1D4MdXxxXAFuNMXvcgqTW62D1tqfbF86oju04WfaeVMcTE9tMnCaDjcB6+3Ul8DTQbMtXAOVpEGstziiODcBmty6BYmAVsAN4FRie6lhtXAVAB1AYVZYW9YqTrNqA0zht27cnqkec0Ug/sJ/fZqApDWLdidM+735mf2T3/XP72VgPrAWuSoNYE77nwD22XrcB81Mdqy1/AvhizL5Jq1edEkMppZRPpjYlKaWUSkATg1JKKR9NDEoppXw0MSillPLRxKCUUspHE4NS/SAi94nIFQPwOscGIh6lBpIOV1UqhUTkmDEmlOo4lIqmVwxKWSKyRET+YOe2f0REskXkmIh8X5x1MVaJSInd9wkRWWS3vy3O2hkbReRfbVm1iLxmy1aJSJUtrxGR34uzfsX9Mb//ayKy2j7nn21ZgYj8UkQ2iMgmEbl+cGtFZSJNDEoBIjIeuB6YYYyZDHQBN+HcKb3GGHMh8DrwrZjnFeNMqXChMWYS4B7sHwaetGXPAMtt+b8BPzTGTMS5w9V9nbk4UxpMw5nY7SI7GeE8YK8xptEYMwH49YD/8UrF0MSglGMOcBGw2q6QNQdnuo9uzk5c9lOc6UqiHQY+Ah4VkYWAO0fQpcDP7PbTUc+bwdl5mp6Oep259msdzvQG43ASRTPwGRF5SERmGWMOf8y/U6leBVIdgFJpQnDO8O/2FYp8M2Y/X6ecMeaMiEzDSSSLgDuAT/fyu+J17AnwoDHmkXMecJbtvBK4X0RWGWPu6+X1lfpY9IpBKccqYJGIlIK31vJonP+RRXafG4E3op9k18woNMasBP4BaLQPvYUzay84TVL/Y7ffjCl3vQTcZl8PEakQkVIRGQmcMMb8FFiGs+yjUkmlVwxKAcaY90RkKc5KdFk4s1t+CTgOTLOP7cfph4gWBl4UkSE4Z/132vK/Bx4Xka8BB4Av2PKv4Cyo8g2ipiI3xrxs+zl+78y6zjFgCVAHLBORbhvT3w7sX67UuXS4qlI90OGkKhNpU5JSSikfvWJQSinlo1cMSimlfDQxKKWU8tHEoJRSykcTg1JKKR9NDEoppXz+DzXVYO1ZXqQ9AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Testing for 20 episodes ...\n",
            "Episode 1: reward: 200.000, steps: 200\n",
            "Episode 2: reward: 200.000, steps: 200\n",
            "Episode 3: reward: 200.000, steps: 200\n",
            "Episode 4: reward: 200.000, steps: 200\n",
            "Episode 5: reward: 200.000, steps: 200\n",
            "Episode 6: reward: 200.000, steps: 200\n",
            "Episode 7: reward: 200.000, steps: 200\n",
            "Episode 8: reward: 200.000, steps: 200\n",
            "Episode 9: reward: 200.000, steps: 200\n",
            "Episode 10: reward: 200.000, steps: 200\n",
            "Episode 11: reward: 200.000, steps: 200\n",
            "Episode 12: reward: 200.000, steps: 200\n",
            "Episode 13: reward: 200.000, steps: 200\n",
            "Episode 14: reward: 200.000, steps: 200\n",
            "Episode 15: reward: 200.000, steps: 200\n",
            "Episode 16: reward: 200.000, steps: 200\n",
            "Episode 17: reward: 200.000, steps: 200\n",
            "Episode 18: reward: 200.000, steps: 200\n",
            "Episode 19: reward: 200.000, steps: 200\n",
            "Episode 20: reward: 200.000, steps: 200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f52ae6cded0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    }
  ]
}